id,pattern,response
1,How can I run a web server on AWS?,"To run a web server on AWS, you can use Amazon EC2. EC2 provides scalable compute capacity, allowing you to launch virtual servers in the cloud."
2,What AWS service should I use to host a scalable application?,"For hosting scalable applications, consider using Amazon EC2. It provides resizable compute capacity and integrates well with other AWS services for building robust and scalable architectures."
3,How do I deploy a custom application on the cloud?,"To deploy a custom application, use Amazon EC2. It allows you to create virtual machines tailored to your application's requirements, providing full control over the environment."
4,I need a dedicated environment for my application. What service should I choose?,"If you need dedicated resources for your application, Amazon EC2 is the ideal choice. It provides dedicated virtual servers with various instance types to meet specific performance needs."
5,What's the best way to get started with virtual servers on AWS?,"Start with Amazon EC2. It offers a wide range of pre-configured Amazon Machine Images (AMIs), making it easy to launch instances with different operating systems and software stacks."
6,How can I ensure high availability for my application on AWS?,"To ensure high availability, use Amazon EC2 in combination with Auto Scaling. Auto Scaling automatically adjusts the number of EC2 instances to handle changes in demand, ensuring your application is always available."
7,I have a Windows-based application. What AWS service should I use?,"For Windows-based applications, Amazon EC2 provides Windows instances, allowing you to seamlessly run and manage Windows workloads in the cloud."
8,What's the recommended approach for running batch processing workloads?,Amazon EC2 is suitable for running batch processing workloads. You can launch instances with the required compute power and configure them to handle batch jobs efficiently.
9,How do I optimize costs when using virtual servers on AWS?,"To optimize costs, leverage Amazon EC2 Spot Instances for non-time-sensitive workloads. Spot Instances offer spare compute capacity at lower prices, providing significant cost savings."
10,What's the primary benefit of using EC2 instances for computational tasks?,The primary benefit of using Amazon EC2 for computational tasks is flexibility. You can choose instance types based on your workload requirements and have the ability to scale resources up or down as needed.
11,I need consistent performance for my application. What AWS EC2 option would you recommend?,"For consistent and predictable performance, consider using On-Demand Instances. They provide fixed pricing and are suitable for applications with steady-state or variable workloads."
12,How can I save costs for my application with flexible pricing options?,"To save costs, explore Spot Instances. Spot Instances offer significant cost savings compared to On-Demand Instances and are ideal for fault-tolerant and flexible applications."
13,I want to ensure dedicated hardware for my instances. Which EC2 option provides this?,"If you require dedicated hardware, consider using Dedicated Hosts. They allow you to run instances on physical servers dedicated to your use, providing additional control over the underlying infrastructure."
14,"What's the difference between Dedicated Hosts and Dedicated Instances, and when should I use each?","Dedicated Hosts provide physical servers for exclusive use, while Dedicated Instances run on hardware dedicated to a single AWS account. Choose Dedicated Hosts for regulatory or licensing requirements and Dedicated Instances for isolation at the instance level."
15,I have a workload with variable demand. What EC2 option would be cost-effective and scalable?,"For variable demand workloads, a combination of On-Demand and Spot Instances can be cost-effective. Use On-Demand for baseline capacity and Spot Instances to handle spikes in demand."
16,I need reserved capacity for my application. Which EC2 option is suitable for this requirement?,Reserved Instances provide capacity reservation for a specific instance type in a chosen Availability Zone. They offer significant cost savings compared to On-Demand pricing and are ideal for applications with steady-state workloads.
17,How can I ensure my instances are always available even during unexpected interruptions?,"Use a mix of On-Demand and Spot Instances with Auto Scaling. Auto Scaling can automatically replace Spot Instances that are interrupted, ensuring your application remains available."
18,My application requires dedicated hardware due to licensing restrictions. What option should I choose?,Opt for Dedicated Hosts if your application has licensing restrictions that require you to run on dedicated physical servers.
19,"What EC2 option is best suited for short-lived, stateless applications that can tolerate interruptions?","Spot Instances are well-suited for short-lived, stateless applications that can tolerate interruptions. They offer cost savings for workloads with flexible start and end times."
20,I have a stable workload with consistent resource requirements. What's the most cost-effective EC2 option?,"For a stable workload with consistent resource requirements, Reserved Instances offer the most cost-effective pricing compared to On-Demand Instances."
21,How can I achieve high availability with dedicated hardware for my instances?,"To achieve high availability with dedicated hardware, consider using Dedicated Hosts along with Auto Scaling. This combination provides dedicated infrastructure while ensuring availability during interruptions."
22,"I need to host a large-scale, parallel processing workload. Which EC2 option is recommended?","Spot Instances are recommended for large-scale, parallel processing workloads where cost efficiency is a priority. Their flexible pricing makes them suitable for distributed computing tasks."
23,I want to ensure my instances are always running on the same physical server. Which EC2 option provides this level of control?,Dedicated Instances run on hardware dedicated to a single AWS account and provide instance-level isolation. This option ensures that your instances are always running on the same physical server.
24,"How can I plan capacity for my applications with a predictable, steady-state workload?","Reserved Instances allow you to plan capacity for predictable, steady-state workloads with a fixed term commitment and cost savings compared to On-Demand Instances."
25,I have a workload that requires occasional bursts of compute power. Which EC2 option is suitable for this scenario?,On-Demand Instances are suitable for workloads that require occasional bursts of compute power. They provide flexibility and can be launched and terminated as needed.
26,What's the advantage of using Dedicated Hosts over other EC2 options?,Dedicated Hosts offer the advantage of maximum visibility and control over the underlying physical servers. They provide predictability and isolation for your instances.
27,How can I ensure capacity is reserved for my instances while maintaining flexibility?,"Reserved Instances provide capacity reservation while maintaining flexibility in terms of instance size, type, and OS. This option is suitable for applications with stable, consistent workloads."
28,I need to launch instances quickly without any upfront commitments. What EC2 option should I choose?,On-Demand Instances provide the flexibility to launch instances quickly without any upfront commitments. They are suitable for applications with variable workloads.
29,My application requires instances to be placed in a specific Availability Zone. Which EC2 option allows for this level of control?,Reserved Instances and On-Demand Instances allow you to launch instances in a specific Availability Zone. Reserved Instances provide cost savings for committed capacity.
30,How can I optimize costs for my application with fluctuating demand throughout the day?,Use a combination of Reserved Instances for baseline capacity and Spot Instances for handling fluctuations in demand. This hybrid approach optimizes costs for varying workloads.
31,How can I ensure capacity is reserved for my instances while maintaining flexibility?,"Reserved Instances provide capacity reservation while maintaining flexibility in terms of instance size, type, and OS. This option is suitable for applications with stable, consistent workloads."
32,I need to launch instances quickly without any upfront commitments. What EC2 option should I choose?,On-Demand Instances provide the flexibility to launch instances quickly without any upfront commitments. They are suitable for applications with variable workloads.
33,My application requires instances to be placed in a specific Availability Zone. Which EC2 option allows for this level of control?,Reserved Instances and On-Demand Instances allow you to launch instances in a specific Availability Zone. Reserved Instances provide cost savings for committed capacity.
34,How can I optimize costs for my application with fluctuating demand throughout the day?,Use a combination of Reserved Instances for baseline capacity and Spot Instances for handling fluctuations in demand. This hybrid approach optimizes costs for varying workloads.
35,Are there EC2 options suitable for workloads that require occasional bursts of compute power?,Spot Instances are suitable for workloads that require occasional bursts of compute power. They offer flexibility and cost savings for fluctuating workloads.
36,What's the advantage of using Dedicated Instances over other EC2 options?,Dedicated Instances provide instance-level isolation on hardware dedicated to a single AWS account. They offer additional isolation compared to instances on shared hardware.
37,How can I achieve high availability with dedicated hardware for my instances?,Amazon EC2 Dedicated Hosts offer the advantage of maximum visibility and control over the underlying physical servers. They provide predictability and isolation for your instances.
38,I want to ensure my instances are always running on the same physical server. Which EC2 option provides this level of control?,Reserved Instances provide capacity reservation for a specific instance type in a chosen Availability Zone. They offer significant cost savings compared to On-Demand pricing and are ideal for applications with steady-state workloads.
39,I have a stable workload with consistent resource requirements. What's the most cost-effective EC2 option?,"For a stable workload with consistent resource requirements, Reserved Instances offer the most cost-effective pricing compared to On-Demand Instances."
40,"I need to host a large-scale, parallel processing workload. Which EC2 option is recommended?","Spot Instances are recommended for large-scale, parallel processing workloads where cost efficiency is a priority. Their flexible pricing makes them suitable for distributed computing tasks."
41,"What EC2 option is best suited for short-lived, stateless applications that can tolerate interruptions?","Spot Instances are well-suited for short-lived, stateless applications that can tolerate interruptions. They offer cost savings for workloads with flexible start and end times."
42,"How can I plan capacity for my applications with a predictable, steady-state workload?","Reserved Instances allow you to plan capacity for predictable, steady-state workloads with a fixed term commitment and cost savings compared to On-Demand Instances."
43,What's the advantage of using Dedicated Hosts over other EC2 options?,Dedicated Hosts offer the advantage of maximum visibility and control over the underlying physical servers. They provide predictability and isolation for your instances.
44,I have a workload with variable demand. What EC2 option would be cost-effective and scalable?,"For variable demand workloads, a combination of On-Demand and Spot Instances can be cost-effective. Use On-Demand for baseline capacity and Spot Instances to handle spikes in demand."
45,How can I ensure my instances are always available even during unexpected interruptions?,Use a mix of On-Demand and Spot Instances with Auto Scaling to ensure availability during interruptions. Auto Scaling can automatically replace Spot Instances that are interrupted.
46,My application requires dedicated hardware due to licensing restrictions. What option should I choose?,"Amazon EC2 Dedicated Hosts provide physical servers for exclusive use, ensuring dedicated hardware for your instances. They are ideal for applications with licensing restrictions."
47,How can I ensure reserved capacity for my application? Which EC2 option provides this feature?,Reserved Instances provide capacity reservation for a specific instance type in a chosen Availability Zone. This ensures capacity is reserved for your instances while maintaining flexibility.
48,I need a cost-effective solution for a workload with predictable traffic. What should I use?,"For a cost-effective solution with predictable traffic, Reserved Instances are recommended. They provide capacity reservation for a fixed term."
49,I have a workload that requires occasional bursts of compute power. Which EC2 option is suitable for this scenario?,Spot Instances are suitable for workloads that require occasional bursts of compute power. They offer flexibility and cost savings for fluctuating workloads.
50,How can I ensure capacity is reserved for my instances while maintaining flexibility?,Use a combination of Reserved Instances for baseline capacity and Spot Instances for handling fluctuations in demand. This hybrid approach optimizes costs for varying workloads.
51,What is Amazon Elastic Compute Cloud (Amazon EC2)?,Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.
52,What can I do with Amazon EC2?,"Just as Amazon Simple Storage Service (Amazon S3) enables storage in the cloud, Amazon EC2 enables compute in the cloud. The Amazon EC2 simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use."
53,How can I get started with Amazon EC2?,"To sign up for Amazon EC2, select the Sign up for This Web Service button on the Amazon EC2 detail page. You must have an AWS account to access this service; if you do not already have one, you will be prompted to create one when you begin the Amazon EC2 signup process. After signing up, please refer to the Amazon EC2 documentation, which includes our Getting Started Guide."
54,Why am I asked to verify my phone number when signing up for Amazon EC2?,Amazon EC2 registration requires you to have a valid phone number and email address on file with AWS in case we ever need to contact you. Verifying your phone number takes only a couple of minutes and involves receiving a phone call during the registration process and entering a PIN using the phone key pad.
55,What can developers now do that they could not before?,"Until now, small developers did not have the capital to acquire massive compute resources and ensure they had the capacity they needed to handle unexpected spikes in load. Amazon EC2 helps developers use Amazons own benefits of massive scale with no upfront investment or performance compromises. Developers are now free to innovate knowing that no matter how successful their businesses become, it will be inexpensive and simple to ensure they have the compute capacity they need to meet their business requirements."
56,How do I run systems in the Amazon EC2 environment?,"Once you have set up your account and select or create your AMIs, you are ready to boot your instance. You can start your AMI on any number of On-Demand instances by using the RunInstances API call. You simply need to indicate how many instances you wish to launch. If you wish to run more than your On-Demand quota, complete the Amazon EC2 instance request form."
57,What is the difference between using the local instance store and Amazon Elastic Block Store (Amazon EBS) for the root device?,"When you launch your Amazon EC2 instances you have the ability to store your root device data on Amazon EBS or the local instance store. By using Amazon EBS, data on the root device will persist independently from the lifetime of the instance. This enables you to stop and restart the instance at a subsequent time, which is similar to shutting down your laptop and restarting it when you need it again."
58,How quickly will systems be running?,"It typically takes less than 10 minutes from the issue of the RunInstances call to the point where all requested instances begin their boot sequences. This time depends on a number of factors including: the size of your AMI, the number of instances you are launching, and how recently you have launched that AMI. Images launched for the first time may take slightly longer to boot."
59,How do I load and store my systems with Amazon EC2?,"Amazon EC2 allows you to set up and configure everything about your instances from your operating system up to your applications. An Amazon Machine Image (AMI) is simply a packaged-up environment that includes all the necessary bits to set up and boot your instance. Your AMIs are your unit of deployment. You might have just one AMI or you might compose your system out of several building block AMIs (e.g., webservers, appservers, and databases). Amazon EC2 provides a number of tools to make creating an AMI easy. Once you create a custom AMI, you will need to bundle it. If you are bundling an image with a root device backed by Amazon EBS, you can simply use the bundle command in the AWS Management Console. If you are bundling an image with a boot partition on the instance store, then you will need to use the AMI Tools to upload it to Amazon S3. Amazon EC2 uses Amazon EBS and Amazon S3 to provide reliable, scalable storage of your AMIs so that we can boot them when you ask us to do so."
60,How do I access my systems?,"The RunInstances call that initiates execution of your application stack will return a set of DNS names, one for each system that is being booted. This name can be used to access the system exactly as you would if it were in your own data center. You own that machine while your operating system stack is executing on it."
61,Is Amazon EC2 used in conjunction with Amazon S3?,"Yes, Amazon EC2 is used jointly with Amazon S3 for instances with root devices backed by local instance storage. By using Amazon S3, developers have access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. In order to execute systems in the Amazon EC2 environment, developers use the tools provided to load their AMIs into Amazon S3 and to move them between Amazon S3 and Amazon EC2. See How do I load and store my systems with Amazon EC2? for more information about AMIs."
62,How many instances can I run in Amazon EC2?,"You are limited to running On-Demand Instances per your vCPU-based On-Demand Instance limit, purchasing 20 Reserved Instances, and requesting Spot Instances per your dynamic Spot limit per region. New AWS accounts may start with limits that are lower than the limits described here."
63,Are there any limitations in sending email from Amazon EC2 instances?,"Yes. In order to maintain the quality of Amazon EC2 addresses for sending email, we enforce default limits on the amount of email that can be sent from EC2 accounts. If you wish to send larger amounts of email from EC2, you can apply to have these limits removed from your account by filling out this form."
64,How quickly can I scale my capacity both up and down?,"Amazon EC2 provides a truly elastic computing environment. Amazon EC2 enables you to increase or decrease capacity within minutes, not hours or days. You can commission one, hundreds or even thousands of server instances simultaneously. When you need more instances, you simply call RunInstances, and Amazon EC2 will typically set up your new instances in a matter of minutes. Of course, because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs."
65,What operating system environments are supported?,"Amazon EC2 currently supports a variety of operating systems including: Amazon Linux, Ubuntu, Windows Server, Red Hat Enterprise Linux, SUSE Linux Enterprise Server, openSUSE Leap, Fedora, Fedora CoreOS, Debian, CentOS, Gentoo Linux, Oracle Linux, and FreeBSD. We are looking for ways to expand it to other platforms."
66,Does Amazon EC2 use ECC memory?,"In our experience, ECC memory is necessary for server infrastructure, and all the hardware underlying Amazon EC2 uses ECC memory."
67,How is this service different than a plain hosting service?,"Traditional hosting services generally provide a pre-configured resource for a fixed amount of time and at a predetermined cost. Amazon EC2 differs fundamentally in the flexibility, control and significant cost savings it offers developers, allowing them to treat Amazon EC2 as their own personal data center with the benefit of Amazon.com's robust infrastructure."
68,What is changing?,Amazon EC2 is transitioning On-Demand Instance limits from the current instance count-based limits to the new vCPU-based limits to simplify the limit management experience for AWS customers. Usage toward the vCPU-based limit is measured in terms of number of vCPUs (virtual central processing units) for the Amazon EC2 Instance Types to launch any combination of instance types that meet your application needs.
69,What are vCPU-based limits?,"You are limited to running one or more On-Demand Instances in an AWS account, and Amazon EC2 measures usage towards each limit based on the total number of vCPUs (virtual central processing unit) that are assigned to the running On-Demand instances in your AWS account. The following table shows the number of vCPUs for each instance size. The vCPU mapping for some instance types may differ; see Amazon EC2 Instance Types for details."
70,How many On-Demand Instances can I run in Amazon EC2?,"There are five vCPU-based instance limits; each defines the amount of capacity you can use of a given instance family. All usage of instances in a given family, regardless of generation, size, or configuration variant (e.g. disk, processor type), will accrue towards the family's total vCPU limit, listed in the table below. New AWS accounts may start with limits that are lower than the limits described here."
71,Are these On-Demand Instance vCPU-based limits regional?,"Yes, the On-Demand Instance limits for an AWS account are set on a per-region basis."
72,Will these limits change over time?,"Yes, limits can change over time. Amazon EC2 is constantly monitoring your usage within each region and your limits are raised automatically based on your use of EC2."
73,How can I request a limit increase?,"Even though EC2 automatically increases your On-Demand Instance limits based on your usage, if needed you can request a limit increase from the Limits Page on Amazon EC2 console, the Amazon EC2 service page on the Service Quotas console, or the Service Quotas API/CLI."
74,How can I calculate my new vCPU limit?,You can find the vCPU mapping for each of the Amazon EC2 Instance Types or use the simplified vCPU Calculator to compute the total vCPU limit requirements for your AWS account.
75,Do vCPU limits apply when purchasing Reserved Instances or requesting Spot Instances?,"No, the vCPU-based limits only apply to running On-Demand instances and Spot Instances."
76,How can I view my current On-Demand Instance limits?,"You can find your current On-Demand Instance limits on the EC2 Service Limits page in the Amazon EC2 console, or from the Service Quotas console and APIs."
77,Will this affect running instances?,"No, opting into vCPU-based limits will not affect any running instances."
78,Can I still launch the same number of instances?,"Yes, the vCPU-based instance limits allow you to launch at least the same number of instances as count-based instance limits."
79,Will I be able to view instance usage against these limits?,"With the Amazon CloudWatch metrics integration, you can view EC2 usage against limits in the Service Quotas console. Service Quotas also enables customers to use CloudWatch for configuring alarms to warn customers of approaching limits. In addition, you can continue to track and inspect your instance usage in Trusted Advisor and Limit Monitor."
80,Will I still be able to use the DescribeAccountAttributes API?,"With the vCPU limits, we no longer have total instance limits governing the usage. Hence the DescribeAccountAttributes API will no longer return the max-instances value. Instead you can now use the Service Quotas APIs to retrieve information about EC2 limits. You can find more information about the Service Quotas APIs in the AWS documentation."
81,Will the vCPU limits have an impact on my monthly bill?,"No. EC2 usage is still calculated either by the hour or the second, depending on which AMI you're running and the instance type and size you've launched."
82,Will vCPU limits be available in all Regions?,vCPU-based instance limits are available in all commercial AWS Regions.
83,What is changing?,"As of Januaury 7, 2020, Amazon EC2 began rolling out a change to restrict email traffic over port 25 by default to protect customers and other recipients from spam and email abuse. Port 25 is typically used as the default SMTP port to send emails. AWS accounts that have requested and had Port 25 throttles removed in the past will not be impacted by this change."
84,I have a valid use case for sending emails to port 25 from EC2. How can I have these port 25 restrictions removed?,"If you have a valid use case for sending emails to port 25 (SMTP) from EC2, please submit a Request to Remove Email Sending Limitations to have these restrictions lifted. You can alternately send emails using a different port, or leverage an existing authenticated email relay service such as Amazon Simple Email Service (Amazon SES)."
85,What does your Amazon EC2 Service Level Agreement guarantee?,Our SLA guarantees a Monthly Uptime Percentage of at least 99.99% for Amazon EC2 and Amazon EBS within a Region.
86,How do I know if I qualify for an SLA Service Credit?,"You are eligible for an SLA credit for either Amazon EC2 or Amazon EBS (whichever was Unavailable, or both if both were Unavailable) if the Region that you are operating in has an Monthly Uptime Percentage of less than 99.99% during any monthly billing cycle. For full details on all of the terms and conditions of the SLA, as well as details on how to submit a claim, see the Amazon Compute Service Level Agreement."
87,What are Accelerated Computing instances?,"The Accelerated Computing instance category includes instance families that use hardware accelerators, or co-processors, to perform some functions, such as floating-point number calculation and graphics processing, more efficiently than is possible in software running on CPUs. Amazon EC2 provides three types of Accelerated Computing instances GPU compute instances for general-purpose computing, GPU graphics instances for graphics intensive applications, and FPGA programmable hardware compute instances for advanced scientific workloads."
88,When should I use GPU Graphics and Compute instances?,"GPU instances work best for applications with massive parallelism such as workloads using thousands of threads. Graphics processing is an example with huge computational requirements, where each of the tasks is relatively small, the set of operations performed form a pipeline, and the throughput of this pipeline is more important than the latency of the individual operations. To be able to build applications that exploit this level of parallelism, one needs GPU device specific knowledge by understanding how to program against various graphics APIs (DirectX, OpenGL) or GPU compute programming models (CUDA, OpenCL)."
89,What applications can benefit from P4d?,"Some of the applications that we expect customers to use P4d for are machine learning (ML) workloads like natural language understanding, perception model training for autonomous vehicles, image classification, object detection and recommendation engines. The increased GPU performance can significantly reduce the time to train and the additional GPU memory will help customers train larger, more complex models. HPC customers can use P4's increased processing performance and GPU memory for seismic analysis, drug discovery, DNA sequencing, and insurance risk modeling."
90,How do P4d instances compare to P3 instances?,"P4 instances feature NVIDIA's latest generation A100 Tensor Core GPUs to provide on average 2.5X increase in TFLOP performance over the previous generation V100 along with 2.5X the GPU memory. P4 instances feature Cascade Lake Intel CPU that has 24C per socket and an additional instruction set for vector neural network instructions. P4 instances will have 1.5X the total system memory and 4X the networking throughput of P3dn or 16x compared to P3.16xl. Another key difference is that the NVSwitch GPU interconnect throughput will double what was possible on P3 so each GPU can communicate with every other GPU at the same 600GB/s bidirectional throughput and with single-hop latency. This allows application development to consider multiple GPUs and memories as a single large GPU and a unified pool of memory. P4d instances are also deployed in tightly coupled hyperscale clusters, called EC2 UltraClusters, that enable you to run the most complex multi-node ML training and HPC applications."
91,What are EC2 UltraClusters and how can I get access?,"P4d instances are deployed in hyperscale clusters called EC2 UltraClusters. Each EC2 UltraCluster is comprised of more than 4,000 NVIDIA A100 Tensor Core GPUs, Petabit-scale networking, and scalable low latency storage with FSx for Lustre. Each EC2 UltraCluster is one of the world's top supercomputers. Anyone can easily spin up P4d instances in EC2 SuperClusters. For additional help, contact us."
92,Will AMIs that I used on P3 and P3dn work on P4?,"The P4 AMIs will need new NVIDIA drivers for the A100 GPUs and a newer version of the ENA driver installed. P4 instances are powered by Nitro System and they require AMIs with NVMe and ENA driver installed. P4 also comes with new Intel Cascade Lake CPUs, which come with an updated instruction set, so we recommend using the latest distributions of ML frameworks, which take advantage of these new instruction sets for data pre-processing."
93,How are P3 instances different from G3 instances?,"P3 instances are the next-generation of EC2 general-purpose GPU computing instances, powered by up to 8 of the latest-generation NVIDIA Tesla V100 GPUs. These new instances significantly improve performance and scalability, and add many new features, including new Streaming Multiprocessor (SM) architecture for machine learning (ML)/deep learning (DL) performance optimization, second-generation NVIDIA NVLink high-speed GPU interconnect, and highly tuned HBM2 memory for higher-efficiency."
94,What are the benefits of NVIDIA Volta GV100 GPUs?,"The new NVIDIA Tesla V100 accelerator incorporates the powerful new Volta GV100 GPU. GV100 not only builds upon the advances of its predecessor, the Pascal GP100 GPU, it significantly improves performance and scalability, and adds many new features that improve programmability. These advances will supercharge HPC, data center, supercomputer, and deep learning systems and applications."
95,Who will benefit from P3 instances?,"P3 instances with their high computational performance will benefit users in artificial intelligence (AI), machine learning (ML), deep learning (DL) and high performance computing (HPC) applications. Users include data scientists, data architects, data analysts, scientific researchers, ML engineers, IT managers and software developers. Key industries include transportation, energy/oil & gas, financial services (banking, insurance), healthcare, pharmaceutical, sciences, IT, retail, manufacturing, high-tech, transportation, government, and academia, among many others."
96,What are some key use cases of P3 instances?,"P3 instances use GPUs to accelerate numerous deep learning systems and applications including autonomous vehicle platforms, speech, image, and text recognition systems, intelligent video analytics, molecular simulations, drug discovery, disease diagnosis, weather forecasting, big data analytics, financial modeling, robotics, factory automation, real-time language translation, online search optimizations, and personalized user recommendations, to name just a few."
97,Why should customers use GPU-powered Amazon P3 instances for AI/ML and HPC?,"GPU-based compute instances provide greater throughput and performance because they are designed for massively parallel processing using thousands of specialized cores per GPU, versus CPUs offering sequential processing with a few cores. In addition, developers have built hundreds of GPU-optimized scientific HPC applications such as quantum chemistry, molecular dynamics, and meteorology, among many others. Research indicates that over 70% of the most popular HPC applications provide built-in support for GPUs."
98,How are G3 instances different from P2 instances?,"G3 instances use NVIDIA Tesla M60 GPUs and provide a high-performance platform for graphics applications using DirectX or OpenGL. NVIDIA Tesla M60 GPUs support NVIDIA GRID Virtual Workstation features, and H.265 (HEVC) hardware encoding. Each M60 GPU in G3 instances supports 4 monitors with resolutions up to 4096x2160, and is licensed to use NVIDIA GRID Virtual Workstation for one Concurrent Connected User. Example applications of G3 instances include 3D visualizations, graphics-intensive remote workstation, 3D rendering, application streaming, video encoding, and other server-side graphics workloads."
99,How are P3 instances different from P2 instances?,"P3 Instances are the next-generation of EC2 general-purpose GPU computing instances, powered by up to 8 of the latest-generation NVIDIA Volta GV100 GPUs. These new instances significantly improve performance and scalability and add many new features, including new Streaming Multiprocessor (SM) architecture, optimized for machine learning (ML)/deep learning (DL) performance, second-generation NVIDIA NVLink high-speed GPU interconnect, and highly tuned HBM2 memory for higher-efficiency."
100,What APIs and programming models are supported by GPU Graphics and Compute instances?,"P3 instances support CUDA 9 and OpenCL, P2 instances support CUDA 8 and OpenCL 1.2 and G3 instances support DirectX 12, OpenGL 4.5, CUDA 8, and OpenCL 1.2."
101,Where do I get NVIDIA drivers for P3 and G3 instances?,"There are two methods by which NVIDIA drivers may be obtained. There are listings on the AWS Marketplace that offer Amazon Linux AMIs and Windows Server AMIs with the NVIDIA drivers pre-installed. You may also launch 64-bit, HVM AMIs and install the drivers yourself. You must visit the NVIDIA driver website and search for the NVIDIA Tesla V100 for P3, NVIDIA Tesla K80 for P2, and NVIDIA Tesla M60 for G3 instances."
102,"Which AMIs can I use with P3, P2 and G3 instances?","You can currently use Windows Server, SUSE Enterprise Linux, Ubuntu, and Amazon Linux AMIs on P2 and G3 instances. P3 instances only support HVM AMIs. If you want to launch AMIs with operating systems not listed here, contact AWS Customer Support with your request or reach out through EC2 Forums."
103,Does the use of G2 and G3 instances require third-party licenses?,"Aside from the NVIDIA drivers and GRID SDK, the use of G2 and G3 instances does not necessarily require any third-party licenses. However, you are responsible for determining whether your content or technology used on G2 and G3 instances requires any additional licensing. For example, if you are streaming content you may need licenses for some or all of that content. If you are using third-party technology such as operating systems, audio and/or video encoders, and decoders from Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies, please consult these providers to determine if a license is required. For example, if you leverage the on-board h.264 video encoder on the NVIDIA GRID GPU you should reach out to MPEG-LA for guidance, and if you use mp3 technology you should contact Thomson for guidance."
104,Why am I not getting NVIDIA GRID features on G3 instances using the driver downloaded from the NVIDIA website?,"The NVIDIA Tesla M60 GPU used in G3 instances requires a special NVIDIA GRID driver to enable all advanced graphics features, and 4 monitors support with resolution up to 4096x2160. You need to use an AMI with NVIDIA GRID driver pre-installed, or download and install the NVIDIA GRID driver following the AWS documentation."
105,Why am I unable to see the GPU when using Microsoft Remote Desktop?,"When using Remote Desktop, GPUs using the WDDM driver model are replaced with a non-accelerated Remote Desktop display driver. In order to access your GPU hardware, you need to utilize a different remote access tool, such as VNC."
106,What is Amazon EC2 F1?,"Amazon EC2 F1 is a compute instance with programmable hardware you can use for application acceleration. The new F1 instance type provides a high performance, easy to access FPGA for developing and deploying custom hardware accelerations."
107,What are FPGAs and why do I need them?,"FPGAs are programmable integrated circuits that you can configure using software. By using FPGAs you can accelerate your applications up to 30x when compared with servers that use CPUs alone. And, FPGAs are reprogrammable, so you get the flexibility to update and optimize your hardware acceleration without having to redesign the hardware."
108,How does F1 compare with traditional FPGA solutions?,"F1 is an AWS instance with programmable hardware for application acceleration. With F1, you have access to FPGA hardware in a few simple clicks, reducing the time and cost of full-cycle FPGA development and scale deployment from months or years to days. While FPGA technology has been available for decades, adoption of application acceleration has struggled to be successful in both the development of accelerators and the business model of selling custom hardware for traditional enterprises, due to time and cost in development infrastructure, hardware design, and at-scale deployment. With this offering, customers avoid the undifferentiated heavy lifting associated with developing FPGAs in on-premises data centers."
109,What is an Amazon FPGA Image (AFI)?,"The design that you create to program your FPGA is called an Amazon FPGA Image (AFI). AWS provides a service to register, manage, copy, query, and delete AFIs. After an AFI is created, it can be loaded on a running F1 instance. You can load multiple AFIs to the same F1 instance, and can switch between AFIs in runtime without reboot. This lets you quickly test and run multiple hardware accelerations in rapid sequence. You can also offer to other customers on the AWS Marketplace a combination of your FPGA acceleration and an AMI with custom software or AFI drivers."
110,How do I list my hardware acceleration on the AWS Marketplace?,"You would develop your AFI and the software drivers/tools to use this AFI. You would then package these software tools/drivers into an Amazon Machine Image (AMI) in an encrypted format. AWS manages all AFIs in the encrypted format you provide to maintain the security of your code. To sell a product in the AWS Marketplace, you or your company must sign up to be an AWS Marketplace reseller, you would then submit your AMI ID and the AFI ID(s) intended to be packaged in a single product. AWS Marketplace will take care of cloning the AMI and AFI(s) to create a product, and associate a product code to these artifacts, such that any end-user subscribing to this product code would have access to this AMI and the AFI(s)."
111,What is available with F1 instances?,"For developers, AWS is providing a Hardware Development Kit (HDK) to help accelerate development cycles, a FPGA Developer AMI for development in the cloud, an SDK for AMIs running the F1 instance, and a set of APIs to register, manage, copy, query, and delete AFIs. Both developers and customers have access to the AWS Marketplace where AFIs can be listed and purchased for use in application accelerations."
112,Do I need to be an FPGA expert to use an F1 instance?,AWS customers subscribing to an F1-optimized AMI from AWS Marketplace do not need to know anything about FPGAs to take advantage of the accelerations provided by the F1 instance and the AWS Marketplace. Simply subscribe to an F1-optimized AMI from the AWS Marketplace with an acceleration that matches the workload. The AMI contains all the software necessary for using the FPGA acceleration. Customers need only write software to the specific API for that accelerator and start using the accelerator.
113,I'm an FPGA developer; how do I get started with F1 instances?,"Developers can get started on the F1 instance by creating an AWS account and downloading the AWS Hardware Development Kit (HDK). The HDK includes documentation on F1, internal FPGA interfaces, and compiler scripts for generating AFI. Developers can start writing their FPGA code to the documented interfaces included in the HDK to create their acceleration function. Developers can launch AWS instances with the FPGA Developer AMI. This AMI includes the development tools needed to compile and simulate the FPGA code. The Developer AMI is best run on the latest C5, M5, or R4 instances. Developers should have experience in the programming languages used for creating FPGA code (i.e. Verilog or VHDL) and an understanding of the operation they wish to accelerate."
114,I'm not an FPGA developer; how do I get started with F1 instances?,"Customers can get started with F1 instances by selecting an accelerator from the AWS Marketplace, provided by AWS Marketplace sellers, and launching an F1 instance with that AMI. The AMI includes all of the software and APIs for that accelerator. AWS manages programming the FPGA with the AFI for that accelerator. Customers do not need any FPGA experience or knowledge to use these accelerators. They can work completely at the software API level for that accelerator."
115,Does AWS provide a developer kit?,"Yes. The Hardware Development Kit (HDK) includes simulation tools and simulation models for developers to simulate, debug, build, and register their acceleration code. The HDK includes code samples, compile scripts, debug interfaces, and many other tools you will need to develop the FPGA code for your F1 instances. You can use the HDK either in an AWS provided AMI, or in your on-premises development environment. These models and scripts are available, publicly with an AWS account."
116,Can I use the HDK in my on-premises development environment?,"Yes. You can use the Hardware Development Kit HDK either in an AWS-provided AMI, or in your on-premises development environment."
117,Can I add an FPGA to any EC2 instance type?,"No. F1 instances comes in two instance sizes: f1.2xlarge, f1.4xlarge, and f1.16 xlarge."
118,How do I use the Inferentia chip in Inf1 instances?,"You can start your workflow by building and training your model in one of the popular ML frameworks such as TensorFlow, PyTorch, or MXNet using GPU instances such as P4, P3, or P3dn. Once the model is trained to your required accuracy, you can use the ML framework's API to invoke Neuron, a software development kit for Inferentia, to compile the model for execution on Inferentia chips, load it in to Inferentia's memory, and then execute inference calls. In order to get started quickly, you can use AWS Deep Learning AMIs that come pre-installed with ML frameworks and the Neuron SDK. For a fully managed experience you will be able to use Amazon SageMaker, which will enable you to seamlessly deploy your trained models on Inf1 instances."
119,When would I use Inf1 vs. C6i or C5 vs. G4 instances for inference?,"Customers running machine learning models that are sensitive to inference latency and throughput can use Inf1 instances for high-performance cost-effective inference. For those ML models that are less sensitive to inference latency and throughput, customers can use EC2 C6i or C5 instances and utilize the AVX-512/VNNI instruction set. For ML models that require access to NVIDIA's CUDA, CuDNN or TensorRT libraries, we recommend using G4 instances."
120,When should I choose Elastic Inference (EI) for inference vs Amazon EC2 Inf1 instances?,"There are two cases where developers would choose EI over Inf1 instances: (1) if you need different CPU and memory sizes than what Inf1 offers, then you can use EI to attach acceleration to the EC2 instance with the right mix of CPU and memory for your application (2) if your performance requirements are significantly lower than what the smallest Inf1 instance provides, then using EI could be a more cost effective choice. For example, if you only need 5 TOPS, enough for processing up to 6 concurrent video streams, then using the smallest slice of EI with a C5.large instance could be up to 50% cheaper than using the smallest size of an Inf1 instance."
121,What ML models types and operators are supported by EC2 Inf1 instances using the Inferentia chip?,Inferentia chips support the commonly used machine learning models such as single shot detector (SSD) and ResNet for image recognition/classification and Transformer and BERT for natural language processing and translation and many others. A list of supported operators can be found on GitHub.
122,How do I take advantage of AWS Inferentia's NeuronCore Pipeline capability to lower latency?,"Inf1 instances with multiple Inferentia chips, such as Inf1.6xlarge or Inf1.24xlarge, support a fast chip-to-chip interconnect. Using the Neuron Processing Pipeline capability, you can split your model and load it to local cache memory across multiple chips. The Neuron compiler uses ahead-of-time (AOT) compilation technique to analyze the input model and compile it to fit across the on-chip memory of single or multiple Inferentia chips. Doing so enables the Neuron Cores to have high-speed access to models and not require access to off-chip memory, keeping latency bounded while increasing the overall inference throughput."
123,What is the difference between AWS Neuron and Amazon SageMaker Neo?,"AWS Neuron is a specialized SDK for AWS Inferentia chips that optimizes the machine learning inference performance of Inferentia chips. It consists of a compiler, run-time, and profiling tools for AWS Inferentia and is required to run inference workloads on EC2 Inf1 instances. On the other hand, Amazon SageMaker Neo is a hardware agnostic service that consists of a compiler and run-time that enables developers to train machine learning models once, and run them on many different hardware platforms."
124,How do I use the Trainium chips in Trn1 instances?,"The Trainium software stack, AWS Neuron SDK, integrates with leading ML frameworks, such as PyTorch and TensorFlow, so you can get started with minimal code changes. To get started quickly, you can use AWS Deep Learning AMIs and AWS Deep Learning Containers, which come preconfigured with AWS Neuron. If you are using containerized applications, you can deploy AWS Neuron by using Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), or your preferred native container engine. AWS Neuron also supports Amazon SageMaker, which you can use to build, train, and deploy machine learning models."
125,Where can I deploy deep learning models trained on Trn1?,"You can deploy deep learning models trained on Trn1 instances on any other Amazon EC2 instance that supports deep learning use cases, including instances based on CPUs, GPUs, or other accelerators. You can also deploy models trained on Trn1 instances outside of AWS, such as on-premises data centers or in embedded devices at the edge. For example, you can train your models on Trn1 instances and deploy them on Inf1 instances, G5 instances, G4 instances, or compute devices at the edge."
126,When would I use Trn1 instances over GPU-based instances for training ML models?,"Trn1 instances are a good fit for your natural language processing (NLP), large language model (LLM), and computer vision (CV) model training use cases. Trn1 instances focus on accelerating model training to deliver high performance while also lowering your model training costs. If you have ML models that need third-party proprietary libraries or languages, for example NVIDIA CUDA, CUDA Deep Neural Network (cuDNN), or TensorRT libraries, we recommend using the NVIDIA GPU-based instances (P4, P3)."
127,How are Burstable Performance Instances different?,"Amazon EC2 allows you to choose between Fixed Performance Instances (e.g. C, M and R instance families) and Burstable Performance Instances (e.g. T2). Burstable Performance Instances provide a baseline level of CPU performance with the ability to burst above the baseline."
128,How do I choose the right Amazon Machine Image (AMI) for my T2 instances?,"You will want to verify that the minimum memory requirements of your operating system and applications are within the memory allocated for each T2 instance size (for example, 512 MiB for t2.nano). Operating systems with Graphical User Interfaces (GUI) that consume significant memory and CPU, for example Microsoft Windows, might need a t2.micro or larger instance size for many use cases. You can find AMIs suitable for the t2.nano instance types on AWS Marketplace. Windows customers who do not need the GUI can use the Microsoft Windows Server 2012 R2 Core AMI."
129,"When should I choose a Burstable Performance Instance, such as T2?","T2 instances provide a cost-effective platform for a broad range of general purpose production workloads. T2 Unlimited instances can sustain high CPU performance for as long as required. If your workloads consistently require CPU usage much higher than the baseline, consider a dedicated CPU instances such as the M or C."
130,How can I see the CPU Credit balance for each T2 instance?,"You can see the CPU Credit balance for each T2 instance in EC2 per-Instance metrics in Amazon CloudWatch. T2 instances have four metrics, CPUCreditUsage, CPUCreditBalance, CPUSurplusCreditBalance and CPUSurplusCreditsCharged. CPUCreditUsage indicates the amount of CPU Credits used. CPUCreditBalance indicates the balance of CPU Credits. CPUSurplusCredit Balance indicates credits used for bursting in the absence of earned credits. CPUSurplusCreditsCharged indicates credits that are charged when average usage exceeds the baseline."
131,What happens to CPU performance if my T2 instance is running low on credits (CPU Credit balance is near zero)?,"If your T2 instance has a zero CPU Credit balance, performance will remain at baseline CPU performance. For example, the t2.micro provides baseline CPU performance of 10% of a physical CPU core. If your instance's CPU Credit balance is approaching zero, CPU performance will be lowered to baseline performance over a 15-minute interval."
132,Does my T2 instance credit balance persist at stop / start?,"No, a stopped instance does not retain its previously earned credit balance."
133,Can T2 instances be purchased as Reserved Instances or Spot Instances?,"T2 instances can be purchased as On-Demand Instances, Reserved Instances or Spot Instances."
134,What are Amazon EC2 T4g instances?,"Amazon EC2 T4g instances are the next-generation of general purpose burstable instances powered by Arm-based AWS Graviton2 processors. T4g instances deliver up to 40% better price performance over T3 instances. They are built on the AWS Nitro System, a combination of dedicated hardware and Nitro hypervisor."
135,What are some of the ideal use cases for T4g instances?,"T4g instances deliver up to 40% better price performance over T3 instances for a wide variety of burstable general purpose workloads such as micro-services, low-latency interactive applications, small and medium databases, virtual desktops, development environments, code repositories, and business-critical applications. Customers deploying applications built on open source software across T instances will find the T4g instances an appealing option to realize the best price performance. Arm developers can also build their applications directly on native Arm hardware as opposed to cross-compilation or emulation."
136,How can customers get access to the T4g free trial?,"Until December 31, 2024, all AWS customers will be enrolled automatically in the T4g free trial as detailed in the AWS Free Tier. During the free-trial period, customers who run a t4g.small instance will automatically get 750 free hours per month deducted from their bill during each month. The 750 hours are calculated in aggregate across all Regions in which the t4g.small instances are used. Customers must pay for surplus CPU credits when they exceed the instances allocated credits during the 750 free hours of the T4g free trial program. For more information about how CPU credits work, see Key concepts and definitions for burstable performance instances in the Amazon EC2 User Guide for Linux Instances."
137,What is the regional availability of T4g free trial?,"As part of the free trial, customers can run t4g.small instances across one or multiple Regions from a single cumulative bucket of 750 free hours per month until December 31, 2024. For example, a customer can run t4g.small in Oregon for 300 hours for a month and run another t4g.small in Tokyo for 450 hours during the same month. This would add up to 750 hours per month of the free-trial limit."
138,When should I use Compute Optimized instances?,"Compute Optimized instances are designed for applications that benefit from high compute power. These applications include compute-intensive applications like high-performance web servers, high-performance computing (HPC), scientific modelling, distributed analytics and machine learning inference."
139,What are Amazon EC2 C6g instances?,"Amazon EC2 C6g instances are the next-generation of compute-optimized instances powered by Arm-based AWS Graviton2 Processors. C6g instances deliver up to 40% better price performance over C5 instances. They are built on the AWS Nitro System, a combination of dedicated hardware and Nitro hypervisor."
140,What are some of the ideal use cases for C6g instances?,"C6g instances deliver significant price performance benefits for compute-intensive workloads such as high performance computing (HPC), batch processing, ad serving, video encoding, gaming, scientific modelling, distributed analytics, and CPU-based machine learning inference. Customers deploying applications built on open source software across C instances family will find the C6g instances an appealing option to realize the best price performance. Arm developers can also build their applications directly on native Arm hardware as opposed to cross-compilation or emulation."
141,What are the various storage options available on C6g instances?,"C6g instances are EBS-optimized by default and offer up to 19,000 Mbps of dedicated EBS bandwidth to both encrypted and unencrypted EBS volumes. C6g instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. Additionally, options with local NVMe instance storage are also available through the C6gd instance types."
142,Which network interface is supported on C6g instances?,"C6g instances support ENA based Enhanced Networking. With ENA, C6g instances can deliver up to 25 Gbps of network bandwidth between instances when launched within a Placement Group."
143,Will customers need to modify their applications and workloads to be able to run on the C6g instances?,"The changes required are dependent on the application. Customers running applications built on open source software will find that the Arm ecosystem is well developed and already likely supports their applications. Most Linux distributions as well as containers (Docker, Kubernetes, Amazon ECS, Amazon EKS, Amazon ECR) support the Arm architecture. Customers will find Arm versions of commonly used software packages available for installation through the same mechanisms that they currently use. Applications that are based on interpreted languages (such as Java, Node, Python) not reliant on native CPU instruction sets should run with minimal to no changes. Applications developed using compiled languages (C, C++, GoLang) will need to be re-compiled to generate Arm binaries. The Arm architecture is well supported in these popular programming languages and modern code usually requires a simple Make' command. Refer to the Getting Started guide on GitHub for more details."
144,Will there be more compute choices offered with the C6 instance families?,"Yes, we plan to offer Intel and AMD CPU powered instances in the future as part of the C6 instance families."
145,Can I launch C4 instances as Amazon EBS-optimized instances?,"Each C4 instance type is EBS-optimized by default. C4 instances 500 Mbps to 4,000 Mbps to EBS above and beyond the general-purpose network throughput provided to the instance. Since this feature is always enabled on C4 instances, launching a C4 instance explicitly as EBS-optimized will not affect the instance's behavior."
146,How can I use the processor state control feature available on the c4.8xlarge instance?,"The c4.8xlarge instance type provides the ability for an operating system to control processor C-states and P-states. This feature is currently available only on Linux instances. You may want to change C-state or P-state settings to increase processor performance consistency, reduce latency, or tune your instance for a specific workload. By default, Amazon Linux provides the highest-performance configuration that is optimal for most customer workloads; however, if your application would benefit from lower latency at the cost of higher single- or dual-core frequencies, or from lower-frequency sustained performance as opposed to bursty Turbo Boost frequencies, then you should consider experimenting with the C-state or P-state configuration options that are available to these instances. For additional information on this feature, see the Amazon EC2 User Guide section on Processor State Control."
147,Why should customers choose C6i instances over C5 instances?,"C6i instances offer up to 15% better price performance over C5 instances, and always-on memory encryption using Intel Total Memory encryption (TME). C6i instances provide a new instance size (c6i.32xlarge) with 128 vCPUs and 256 GiB of memory, 33% more than the largest C5 instance. They also provide up to 9% higher memory bandwidth per vCPU compared to C5 instances. C6i also give customers up to 50 Gbps of networking speed and 40 Gbps of bandwidth to the Amazon Elastic Block Store, twice that of C5 instances."
148,Why should customers choose C5 instances over C4 instances?,"The generational improvement in CPU performance and lower price of C5 instances, which combined result in a 25% price/performance improvement relative to C4 instances, benefit a broad spectrum of workloads that currently run on C3 or C4 instances. For floating point intensive applications, Intel AVX-512 enables significant improvements in delivered TFLOPS by effectively extracting data level parallelism. Customers looking for absolute performance for graphics rendering and HPC workloads that can be accelerated with GPUs or FPGAs should also evaluate other instance families in the Amazon EC2 portfolio that include those resources to find the ideal instance for their workload."
149,Which storage interface is supported on C5 instances?,C5 instances will support only NVMe EBS device model. EBS volumes attached to C5 instances will appear as NVMe devices. NVMe is a modern storage interface that provides latency reduction and results in increased disk I/O and throughput.
150,Why does the total memory reported by the operating system not exactly match the advertised memory on instance types?,"Portions of the EC2 instance memory are reserved and used by the virtual BIOS for video RAM, DMI, and ACPI. In addition, for instances that are powered by the AWS Nitro Hypervisor, a small percentage of the instance memory is reserved by the Amazon EC2 Nitro Hypervisor to manage virtualization."
151,Which pricing models do Hpc7g instances support?,"Hpc7g instances are available for purchase through the 1- and 3-year Amazon EC2 Instance Savings Plans, Compute Savings Plans, EC2 On-Demand Instances, and EC2 Reserved Instances."
152,Which AMIs are supported on Hpc7g instances?,Hpc7g instances support Amazon EBS backed AMIs only.
153,How are Hpc7a instances different from other EC2 instances?,"HPC-optimized EC2 Hpc7a instances are ideal for applications that benefit from high-performance processors such as large, complex simulations including computational fluid dynamics (CFD), numerical weather prediction, and multiphysics simulations. Hpc7a instances are designed to help you run tightly coupled, x86-based HPC workloads with better performance. Hpc7a instances feature 4th Gen AMD EPYC processors with 2x higher core density (up to 192 cores), 2.1x higher memory bandwidth throughput (768 GB of memory), and 3x higher network bandwidth compared to Hpc6a instances. These instances offer 300 Gbpsof EFA network bandwidth, powered by the AWS Nitro System, for for fast and low latency internode communications."
154,Which pricing models do Hpc7a instances support?,"Hpc7a instances are available for purchase through the 1- and 3-year Amazon EC2 Instance Savings Plans, Compute Savings Plans, EC2 On-Demand Instances, and EC2 Reserved Instances."
155,Which AMIs are supported on Hpc7a instances?,"Hpc7a instances support Amazon Linux 2, Amazon Linux, Ubuntu 18.04 or later, Red Hat Enterprise Linux 7.6 or later, SUSE Linux Enterprise Server 12 SP3 or later, CentOS 7 or later, and FreeBSD 11.1 or later."
156,Which AMIs are supported on Hpc6a instances?,"Hpc6a instances support Amazon Linux 2, Amazon Linux, Ubuntu 18.04 or later, Red Hat Enterprise Linux 7.4 or later, SUSE Linux Enterprise Server 12 SP2 or later, CentOS 7 or later, and FreeBSD 11.1 or later. These instances also support Windows Server 2012, 2012 R2, 2016, and 2019."
157,Which pricing models do Hpc6a instances support?,"Hpc6a instances are available for purchase through 1-year and 3-year Standard Reserved Instances, Convertible Reserved Instances, Savings Plans, and On-Demand Instances."
158,What are Amazon EC2 M6g instances?,"Amazon EC2 M6g instances are the next-generation of general-purpose instances powered by Arm-based AWS Graviton2 Processors. M6g instances deliver up to 40% better price/performance over M5 instances. They are built on the AWS Nitro System, a combination of dedicated hardware and Nitro hypervisor."
159,What are the specifications of the new AWS Graviton2 Processors?,"The AWS Graviton2 processors deliver up to 7x performance, 4x the number of compute cores, 2x larger caches, 5x faster memory, and 50% faster per core encryption performance than first generation AWS Graviton processors. Each core of the AWS Graviton2 processor is a single-threaded vCPU. These processors also offer always-on fully encrypted DRAM memory, hardware acceleration for compression workloads, dedicated engines per vCPU that double the floating-point performance for workloads such as video encoding, and instructions for int8/fp16 CPU-based machine learning inference acceleration. The CPUs are built utilizing 64-bit Arm Neoverse cores and custom silicon designed by AWS on the advanced 7 nm manufacturing technology."
160,Is memory encryption supported by AWS Graviton2 processors?,"AWS Graviton2 processors support always-on 256-bit memory encryption to further enhance security. Encryption keys are securely generated within the host system, do not leave the host system, and are irrecoverably destroyed when the host is rebooted or powered down. Memory encryption does not support integration with AWS Key Management Service (AWS KMS) and customers cannot bring their own keys."
161,What are some of the ideal use cases for M6g instances?,"M6g instances deliver significant performance and price performance benefits for a broad spectrum of general-purpose workloads such as application servers, gaming servers, microservices, mid-size databases, and caching fleets. Customers deploying applications built on open source software across the M instances will find the M6g instances an appealing option to realize the best price performance. Arm developers can also build their applications directly on native Arm hardware as opposed to cross-compilation or emulation."
162,What are the various storage options available on M6g instances?,"M6g instances are EBS-optimized by default and offer up to 19,000 Mbps of dedicated EBS bandwidth to both encrypted and unencrypted EBS volumes. M6g instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. Additionally, options with local NVMe instance storage are also available through the M6gd instance types."
163,Which network interface is supported on M6g instances?,"M6g instances support ENA based Enhanced Networking. With ENA, M6g instances can deliver up to 25 Gbps of network bandwidth between instances when launched within a Placement Group."
164,Will customers need to modify their applications and workloads to be able to run on the M6g instances?,"The changes required are dependent on the application. Customers running applications built on open source software will find that the Arm ecosystem is well developed and already likely supports their applications. Most Linux distributions as well as containers (Docker, Kubernetes, Amazon ECS, Amazon EKS, Amazon ECR) support the Arm architecture. Customers will find Arm versions of commonly used software packages available for installation through the same mechanisms that they currently use. Applications that are based on interpreted languages (such as Java, Node, Python) not reliant on native CPU instruction sets should run with minimal to no changes. Applications developed using compiled languages (C, C++, GoLang) will need to be re-compiled to generate Arm binaries. The Arm architecture is well supported in these popular programming languages and modern code usually requires a simple Make' command. Refer to the Getting Started guide on GitHub for more details."
165,What are Amazon EC2 A1 instances?,Amazon EC2 A1 instances are general purpose instances powered by the first-generation AWS Graviton Processors that are custom designed by AWS.
166,What are the specifications of the first-generation AWS Graviton Processors?,AWS Graviton processors are custom designed by AWS utilizing Amazon's extensive expertise in building platform solutions for cloud applications running at scale. These processors are based on the 64-bit Arm instruction set and feature Arm Neoverse cores as well as custom silicon designed by AWS. The cores operate at a frequency of 2.3 GHz.
167,When should I use A1 instances?,"A1 instances deliver significant cost savings for scale-out workloads that can fit within the available memory footprint. A1 instances are ideal for scale-out applications such as web servers, containerized microservices, and data/log processing. These instances will also appeal to developers, enthusiasts, and educators across the Arm developer community."
168,Will customers have to modify applications and workloads to be able to run on the A1 instances?,"The changes required are dependent on the application. Applications based on interpreted or run-time compiled languages (e.g. Python, Java, PHP, Node.js) should run without modifications. Other applications may need to be recompiled and those that don't rely on x86 instructions will generally build with minimal to no changes."
169,Which operating systems/AMIs are supported on A1 Instances?,"The following AMIs are supported on A1 instances: Amazon Linux 2, Ubuntu 16.04.4 or newer, Red Hat Enterprise Linux (RHEL) 7.6 or newer, SUSE Linux Enterprise Server 15 or newer. Additional AMI support for Fedora, Debian, NGINX Plus are also available through community AMIs and the AWS Marketplace. EBS backed HVM AMIs launched on A1 instances require NVMe and ENA drivers installed at instance launch."
170,Are there specific AMI requirements to run on M6g and A1 instances?,You will need to use the arm64 AMIs with the M6g and A1 instances. x86 AMIs are not compatible with M6g and A1 instances.
171,When should customers use A1 instances versus the new M6g instances?,"A1 instances continue to offer significant cost benefits for scale-out workloads that can run on multiple smaller cores and fit within the available memory footprint. The new M6g instances are a good fit for a broad spectrum of applications that require more compute, memory, networking resources and/or can benefit from scaling up across platform capabilities. M6g instances will deliver the best price-performance within the instance family for these applications. M6g supports up to 16xlarge instance size (A1 supports up to 4xlarge), 4GB of memory per vCPU (A1 supports 2GB memory per vCPU), and up to 25 Gbps of networking bandwidth (A1 supports up to 10 Gbps)."
172,What are the various storage options available to A1 customers?,"A1 instances are EBS-optimized by default and offer up to 3,500 Mbps of dedicated EBS bandwidth to both encrypted and unencrypted EBS volumes. A1 instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. A1 instances will not support the blkfront interface."
173,Which network interface is supported on A1 instances?,"A1 instances support ENA based Enhanced Networking. With ENA, A1 instances can deliver up to 10 Gbps of network bandwidth between instances when launched within a Placement Group."
174,Do A1 instances support the AWS Nitro System?,"Yes, A1 instances are powered by the AWS Nitro System, a combination of dedicated hardware and Nitro hypervisor."
175,Why should customers choose EC2 M5 Instances over EC2 M4 Instances?,"Compared with EC2 M4 Instances, the new EC2 M5 Instances deliver customers greater compute and storage performance, larger instance sizes for less cost, consistency and security. The biggest benefit of EC2 M5 Instances is based on its usage of the latest generation of Intel Xeon Scalable processors (Skylake-SP or Cascade Lake), which deliver up to 20% improvement in price/performance compared to M4. With AVX-512 support in M5 vs. the older AVX2 in M4, customers will gain 2x higher performance in workloads requiring floating point operations. M5 instances offer up to 25 Gbps of network bandwidth and up to 10 Gbps of dedicated bandwidth to Amazon EBS. M5 instances also feature significantly higher networking and Amazon EBS performance on smaller instance sizes with EBS burst capability."
176,Why should customers choose M6i instances over M5 instances?,"Amazon M6i instances are powered by 3rd generation Intel Xeon Scalable processors (code named Ice Lake) with an all-core turbo frequency of 3.5 GHz, offer up to 15% better compute price performance over M5 instances, and always-on memory encryption using Intel Total Memory Encryption (TME). Amazon EC2 M6i instances are the first to use a lower-case i to indicate they are Intel-powered instances. M6i instances provide a new instance size (m6i.32xlarge) with 128 vCPUs and 512 GiB of memory, 33% more than the largest M5 instance. They also provide up to 20% higher memory bandwidth per vCPU compared to M5 instances, allowing customers to efficiently perform real-time analysis for data-intensive AI/ML, gaming, and High Performance Computing (HPC) applications. M6i also give customers up to 50 Gbps of networking speed and 40 Gbps of bandwidth to the Amazon Elastic Block Store, twice that of M5 instances. M6i also allows customers to use Elastic Fabric Adapter on the 32xlarge size, enabling low latency and high scale inter-node communication. For optimal networking performance on these new instances, Elastic Network Adapter (ENA) driver update may be required. For more information on optimal ENA driver for M6i, see this article."
177,How does support for Intel AVX-512 benefit customers who use the EC2 M5 family or the M6i family?,"Intel Advanced Vector Extensions 512 (AVX-512) is a set of new CPU instructions available on the latest Intel Xeon Scalable processors, that can accelerate performance for workloads and usages such as scientific simulations, financial analytics, artificial intelligence, machine learning/deep learning, 3D modeling and analysis, image and video processing, cryptography and data compression, among others. Intel AVX-512 offers exceptional processing of encryption algorithms, helping to reduce the performance overhead for cryptography, which means customers who use the EC2 M5 family or M6i family can deploy more secure data and services into distributed environments without compromising performance."
178,What are EC2 High Memory instances?,"Amazon EC2 High Memory instances offer 3, 6, 9, 12, 18, or 24 TiB of memory in a single instance. These instances are designed to run large in-memory databases, including production installations of SAP HANA, in the cloud."
179,Are High Memory instances certified by SAP to run SAP HANA workloads?,"High Memory instances are certified by SAP for running Business Suite on HANA, the next-generation Business Suite S/4HANA, Data Mart Solutions on HANA, Business Warehouse on HANA, and SAP BW/4HANA in production environments. For details, see SAP's Certified and Supported SAP HANA Hardware Directory."
180,What instance types are available for High Memory instances?,"High Memory instances are available as both bare metal and virtualized instances, giving customers the choice to have direct access to the underlying hardware resources, or to take advantage of the additional flexibility that virtualized instances offer including On-Demand and 1-year and 3-year Savings Plan purchase options. Please check out available options for High Memory instances in the Memory optimized section of EC2 Instance types page."
181,What are some of the benefits of using High Memory Virtualized instances over High Memory Bare Metal instances?,"Benefits of High Memory virtual instances over High Memory Metal instances include significantly better launch/reboot times, flexible purchase options (On-Demand, Savings Plan, Reserved Instances, Dedicated Hosts), choice of tenancy type, self-service options and support for a higher number of EBS volumes (27 vs 19)."
182,When should a High Memory Metal' instance be used vs using High Memory Virtualized' instance?,"Though High Memory Virtualized' instances are in-general recommended to be used, there are specific situations where only High Memory Metal instances can work. These situation include when using OS versions that are not supported on High Memory Virtual instances OR when using applications that need to run in non-virtualized mode to meet licensing / support requirements OR when using applications that require access to hardware feature set (such as Intel VT-x) OR when using custom hypervisor (e.g, ESXi)."
183,How do I migrate from High Memory metal instances to High Memory virtualized instances?,"You can migrate your High Memory metal instance to a virtualized instance in just few steps. 1/Stop your instance, 2/ Change the instance and tenancy type through EC2 API and 3/ Start your instance back up. If you are using Red Hat Enterprise Linux for SAP or SUSE Linux Enterprise Server for SAP, you need to ensure that your operating system and kernel versions are compatible with virtualized High Memory instances. For further details, see Migrating SAP HANA on AWS to an EC2 High Memory Instance documentation."
184,What are the storage options available with High Memory instances?,"High Memory instances support Amazon EBS volumes for storage. High Memory instances are EBS-optimized by default, and offer up to 38 Gbps of storage bandwidth.:"
185,Which storage interface is supported on High Memory instances?,"High Memory instances access EBS volumes via PCI attached NVM Express (NVMe) interfaces. EBS volumes attached to High Memory instances appear as NVMe devices. NVMe is an efficient and scalable storage interface, which is commonly used for flash based SSDs and provides latency reduction and results in increased disk I/O and throughput. The EBS volumes are attached and detached by PCI hotplug."
186,What network performance is supported on High Memory instances?,"High Memory instances use the Elastic Network Adapter (ENA) for networking and enable Enhanced Networking by default. With ENA, High Memory instances can utilize up to 100 Gbps of network bandwidth."
187,Can I run High Memory instances in my existing Amazon Virtual Private Cloud (Amazon VPC)?,You can run High Memory instances in your existing and new Amazon VPCs.
188,Do High Memory instances enable CPU power management state control?,"Yes. You can configure C-states and P-states on High Memory instances. You can use C-states to enable higher turbo frequencies (as much as 4.0 GHz). You can also use P-states to lower performance variability by pinning all cores at P1 or higher P states, which is similar to disabling Turbo, and running consistently at the base CPU clock speed."
189,What purchase options are available for High Memory instances?,"EC2 High Memory virtualized instances (e.g. u-6tb1.112xlarge) are available for purchase via On-Demand, 1-Yr and 3-Yr Savings Plan, and 1-Yr and 3-Yr Reserved Instance. EC2 High Memory metal instances (e.g. u-6tb1.metal) are only available for purchase as EC2 Dedicated Hosts on a 1-Yr and 3-Yr Reservation."
190,What is the lifecycle of a Dedicated Host?,"Once a Dedicated Host is allocated within your account, it will be standing by for your use. You can then launch an instance with a tenancy of ""host"" using the RunInstances API, and can also stop/start/terminate the instance through the API. You can use the AWS Management Console to manage the Dedicated Host and the instance."
191,"Can I launch, stop/start, and terminate High Memory instances using AWS CLI/SDK?","You can launch, stop/start, and terminate instances using AWS CLI/SDK."
192,Which AMIs are supported with High memory instances?,"EBS-backed HVM AMIs with support for ENA networking can be used with High Memory instances. The latest Amazon Linux, Red Hat Enterprise Linux, SUSE Enterprise Linux Server, and Windows Server AMIs are supported. Operating system support for SAP HANA workloads on High Memory instances include: SUSE Linux Enterprise Server 12 SP3 for SAP, Red Hat Enterprise Linux 7.4 for SAP, Red Hat Enterprise Linux 7.5 for SAP, SUSE Linux Enterprise Server 12 SP4 for SAP, SUSE Linux Enterprise Server 15 for SAP, Red Had Enterprise Linux 7.6 for SAP. Refer to SAP's Certified and Supported SAP HANA Hardware Directory for latest detail on supported operating systems."
193,Are there standard SAP HANA reference deployment frameworks available for the High Memory instance and the AWS Cloud?,"You can use the AWS Quick Start reference SAP HANA deployments to rapidly deploy all the necessary SAP HANA building blocks on High Memory instances following SAP's recommendations for high performance and reliability. AWS Quick Starts are modular and customizable, so you can layer additional functionality on top or modify them for your own implementations."
194,When should I use memory-optimized instances?,"Memory-optimized instances offer large memory size for memory intensive applications including in-memory applications, in-memory databases, in-memory analytics solutions, HPC, scientific computing, and other memory-intensive applications."
195,What are Amazon EC2 R6g instances?,"Amazon EC2 R6g instances are the next-generation of memory-optimized instances powered by Arm-based AWS Graviton2 Processors. R6g instances deliver up to 40% better price performance over R5 instances. They are built on the AWS Nitro System, a combination of dedicated hardware and Nitro hypervisor."
196,What are some of the ideal use cases for R6g instances?,"R6g instances deliver significant price performance benefits for memory-intensive workloads such as instances and are ideal for running memory-intensive workloads such as open-source databases, in-memory caches, and real time big data analytics. Customers deploying applications built on open source software across R instances will find the R6g instances an appealing option to realize the best price performance within the instance family. Arm developers can also build their applications directly on native Arm hardware as opposed to cross-compilation or emulation."
197,What are the various storage options available on R6g instances?,"R6g instances are EBS-optimized by default and offer up to 19,000 Mbps of dedicated EBS bandwidth to both encrypted and unencrypted EBS volumes. R6g instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. Additionally, options with local NVMe instance storage are also available through the R6gd instance types."
198,Which network interface is supported on R6g instances?,"R6g instances support ENA based Enhanced Networking. With ENA, R6g instances can deliver up to 25 Gbps of network bandwidth between instances when launched within a Placement Group."
199,Will customers need to modify their applications and workloads to be able to run on the R6g instances?,"The changes required are dependent on the application. Customers running applications built on open source software will find that the Arm ecosystem is well developed and already likely supports their applications. Most Linux distributions as well as containers (Docker, Kubernetes, Amazon ECS, Amazon EKS, Amazon ECR) support the Arm architecture. Customers will find Arm versions of commonly used software packages available for installation through the same mechanisms that they currently use. Applications that are based on interpreted languages (such as Java, Node, Python) not reliant on native CPU instruction sets should run with minimal to no changes. Applications developed using compiled languages (C, C++, GoLang) will need to be re-compiled to generate Arm binaries. The Arm architecture is well supported in these popular programming languages and modern code usually requires a simple Make' command. Refer to the Getting Started guide on GitHub for more details."
200,What are Amazon EC2 R5b instances?,"R5b instances are EBS-optimized variants of memory-optimized R5 instances that deliver up to 3x better EBS performance compared to same sized R5 instances. R5b instances deliver up to 60 Gbps bandwidth and 260K IOPS of EBS performance, the fastest block storage performance on EC2. They are built on the AWS Nitro System, which is a combination of dedicated hardware and Nitro hypervisor."
201,What are some of the ideal use cases for R5b instances?,"R5b instances are ideal for large relational database workloads, including Microsoft SQL Server, SAP HANA, IBM DB2, and Oracle that run performance intensive applications such as commerce platforms, ERP systems, and health record systems. Customers looking to migrate large on-premises workloads with large storage performance requirements to AWS will find R5b instances to be a good fit."
202,What are the various storage options available on R5b instances?,"R5b instances are EBS-optimized by default and offer up to 60,000 Mbps of dedicated EBS bandwidth and 260K IOPS for both encrypted and unencrypted EBS volumes. R5b instances only support Non-Volatile Memory Express (NVMe) interface to access EBS storage volumes. R5b is supported by all volume types, with the exception of io2 volumes."
203,When should I use R5b instances?,Customers running workloads such as large relational databases and data analytics that want to take advantage of the increased EBS storage network performance can use R5b instances to deliver higher performance and bandwidth. Customers can also lower costs by migrating their workloads to smaller size R5b instances or by consolidating workloads on fewer R5b instances.
204,What are the storage options available with High Memory instances?,"High Memory instances support Amazon EBS volumes for storage. High Memory instances are EBS-optimized by default, and offer up to 38Gbps of storage bandwidth to both encrypted and unencrypted EBS volumes."
205,What are Amazon EC2 X2gd instances?,"Amazon EC2 X2gd instances are the next generation of memory-optimized instances powered by AWS-designed Arm-based AWS Graviton2 processors. X2gd instances deliver up to 55% better price performance compared to x86-based X1 instances and offer the lowest cost per GiB of memory in Amazon EC2. They are the first of the X instances to be built on the AWS Nitro System, which is a combination of dedicated hardware and Nitro hypervisor."
206,What workloads are suited for X2gd instances?,"X2gd is ideal for customers with Arm-compatible memory bound scale-out workloads such as Redis and Memcached in-memory databases, that need low latency memory access and benefit from more memory per vCPU. X2gd is also well suited for relational databases such as PostgreSQL, MariaDB, MySQL, and RDS Aurora. Customers who run memory intensive workloads such as Apache Hadoop, real-time analytics, and real-time caching servers will benefit from 1:16 vCPU to memory ratio of X2gd. Single threaded workloads such as EDA backend verification jobs will benefit from physical core and more memory of X2gd instances, allowing them to consolidate more workloads on to a single instance. X2gd instance also feature local NVMe SSD block storage to improve response times by acting as a caching layer."
207,"When should I use X2gd instances compared to the X1, X2i, or R instances?","X2gd instances are suitable for Arm-compatible memory bound scale-out workloads such as in-memory databases, memory analytics applications, open-source relational database workloads, EDA workloads, and large caching servers. X2gd instances offer customers the lowest cost per gigabyte of memory within EC2, with sizes up to 1 TiB. X2iezn, X2idn, X2iedn, X1, and X1e instances use x86 processors and are suitable for memory-intensive enterprise-class, scale-up workloads such as Windows workloads, in-memory databases (e.g. SAP HANA), and relational databases (e.g. OracleDB). Customers can leverage the x86-based X instances for larger memory sizes up to 4 TiB. R6g and R6gd instances are suitable for workloads such as web applications, databases, and search indexing queries that need more vCPUs during times of heavy data processing. Customers running memory bound workloads that need less than 1 TiB memory and have dependency on x86 instruction set such as Windows applications, and applications like Oracle or SAP can leverage R5 instances and R6 instances."
208,When should I use X2idn and X2iedn instances?,"X2idn and X2iedn instances are powered by 3rd generation Intel Xeon Scalable processors with an all-core turbo frequency up to 3.5 GHz and deliver up to 50% higher compute price performance than comparable X1 instances. X2idn and X2iedn instances both include up to 3.8 TB of local NVMe SSD storage and up to 100 Gbps of networking bandwidth, while X2idn offers up to 2 TiB of memory and X2iedn offers up to 4 TiB of memory. X2idn and X2iedn instances are SAP-Certified and are a great fit for workloads such as small-to large-scale traditional and in-memory databases, and analytics."
209,When should I use X2iezn instances?,"X2iezn instances feature the fastest Intel Xeon Scalable processors in the cloud and are a great fit for workloads that need high single-threaded performance combined with a high memory-to-vCPU ratio and high speed networking. X2iezn instances have an all-core turbo frequency up to 4.5 GHz, feature a 32:1 ratio of memory to vCPU, and deliver up to 55% higher compute price performance compared to X1e instances. X2iezn instances are a great fit for electronic design automation (EDA) workloads like physical verification, static timing analysis, power signoff, and full chip gate-level simulation."
210,Which operating systems/AMIs are supported on X2gd instances?,"The following AMIs are supported: Amazon Linux 2, Ubuntu 18.04 or newer, Red Hat Enterprise Linux 8.2 or newer, and SUSE Enterprise Server 15 or newer. Customers will find additional AMIs such as Fedora, Debian, NetBSD, and CentOS available through community AMIs and the AWS Marketplace. For containerized applications, Amazon ECS and EKS optimized AMIs are available as well."
211,When should I use X1 instances?,"X1 instances are ideal for running in-memory databases like SAP HANA, big data processing engines like Apache Spark or Presto, and high performance computing (HPC) applications. X1 instances are certified by SAP to run production environments of the next-generation Business Suite S/4HANA, Business Suite on HANA (SoH), Business Warehouse on HANA (BW), and Data Mart Solutions on HANA on the AWS cloud."
212,Do X1 and X1e instances enable CPU power management state control?,"Yes. You can configure C-states and P-states on x1e.32xlarge, x1e.16xlarge, x1e.8xlarge, x1.32xlarge and x1.16xlarge instances. You can use C-states to enable higher turbo frequencies (as much as 3.1 GHz with one or two core turbo). You can also use P-states to lower performance variability by pinning all cores at P1 or higher P states, which is similar to disabling Turbo, and running consistently at the base CPU clock speed."
213,Are there standard SAP HANA reference deployment frameworks available for the High Memory instance and the AWS?,You can use AWS Launch Wizard for SAP or AWS Quick Start reference SAP HANA deployments to rapidly deploy all the necessary SAP HANA building blocks on High Memory instances following recommendations from AWS and SAP for high performance and reliability.
214,"Why don't I see M1, C1, CC2 and HS1 instances on the pricing pages any more?",These have been moved to the Previous Generation Instance page.
215,Are these Previous Generation instances still being supported?,Yes. Previous Generation instances are still fully supported.
216,Can I still use/add more Previous Generation instances?,"Yes. Previous Generation instances are still available as On-Demand, Reserved Instances, and Spot Instance, from our APIs, CLI and EC2 Management Console interface."
217,Are my Previous Generation instances going to be deleted?,"No. Your C1, C3, CC2, CR1, G2, HS1, M1, M2, M3, R3 and T1 instances are still fully functional and will not be deleted because of this change."
218,Are Previous Generation instances being discontinued soon?,"Currently, there are no plans to end of life Previous Generation instances. However, with any rapidly evolving technology the latest generation will typically provide the best performance for the price and we encourage our customers to take advantage of technological advancements."
219,Will my Previous Generation instances I purchased as a Reserved Instance be affected or changed?,"No. Your Reserved Instances will not change, and the Previous Generation instances are not going away."
220,What is a Dense-storage Instance?,"Dense-storage instances are designed for workloads that require high sequential read and write access to very large data sets, such as Hadoop distributed computing, massively parallel processing data warehousing, and log processing applications. The Dense-storage instances offer the best price/GB-storage and price/disk-throughput across other EC2 instances."
221,How do dense-storage instances compare to High I/O instances?,"High I/O instances (Im4gn, Is4gen, I4i, I3, I3en) are targeted at workloads that demand low latency and high random I/O in addition to moderate storage density and provide the best price/IOPS across other EC2 instance types. Dense-storage instances (D3, D3en, D2) and HDD-storage instances (H1) are optimized for applications that require high sequential read/write access and low cost storage for very large data sets and provide the best price/GB-storage and price/disk-throughput across other EC2 instances."
222,How much disk throughput can Dense-storage and HDD-storage instances deliver?,"The largest current generation of Dense HDD-storage instances, d3en.12xlarge, can deliver up to 6.2 GiB/s read and 6.2 GiB/s write disk throughput with a 128k block size. Please see the product detail page for additional performance information. To ensure the best disk throughput performance from your D2, D3 and D3en instances on Linux, we recommend that you use the most recent version of the Amazon Linux AMI, or another Linux AMI with a kernel version of 3.8 or later that supports persistent grants an extension to the Xen block ring protocol that significantly improves disk throughput and scalability."
223,Do Dense-storage and HDD-storage instances provide any failover mechanisms or redundancy?,"D2 and H1 instances provide notifications for hardware failures. Like all instance storage, Dense HDD-storage volumes persist only for the life of the instance. Hence, we recommend that you build a degree of redundancy (e.g. RAID 1/5/6) or use file systems (e.g. HDFS and MapR-FS) that support redundancy and fault tolerance. You can also back up data periodically to more data storage solutions such as Amazon EBS or Amazon S3."
224,How do dense HDD-storage instances differ from Amazon EBS?,"Amazon EBS offers simple, elastic, reliable (replicated), and persistent block level storage for Amazon EC2 while abstracting the details of the underlying storage media in use. Amazon EC2 instance instances with local HDD or NVMe storage provide directly attached, high performance storage building blocks that can be used for a variety of storage applications. Dense-storage instances are specifically targeted at customers who want high sequential read/write access to large data sets on local storage, e.g. for Hadoop distributed computing and massively parallel processing data warehousing."
225,Can I launch dense HDD-storage instances as Amazon EBS optimized instances?,"Each HDD-storage instance type (H1, D2, D3, and D3en) is EBS optimized by default. Since this feature is always enabled, launching one of these instances explicitly as EBS optimized will not affect the instance's behavior. For more information, see Amazon EBoptimized instances."
226,Can I launch D2 instances as Amazon EBS optimized instances?,"Each D2 instance type is EBS optimized by default. D2 instances 500 Mbps to 4,000 Mbps to EBS above and beyond the general-purpose network throughput provided to the instance. Since this feature is always enabled on D2 instances, launching a D2 instance explicitly as EBS optimized will not affect the instance's behavior."
227,What is a High I/O instance?,"High I/O instances use NVMe based local instance storage to deliver very high, low latency, I/O capacity to applications, and are optimized for applications that require millions of IOPS. Like Cluster instances, High I/O instances can be clustered via cluster placement groups for low latency networking."
228,Are all features of Amazon EC2 available for High I/O instances?,"High I/O instances support all Amazon EC2 features. Im4gn, Is4gen, I4i, I3 and I3en instances offer NVMe only storage, while previous generation I2 instances allow legacy blkfront storage access."
229,AWS has other database and Big Data offerings. When or why should I use High I/O instances?,"High I/O instances are ideal for applications that require access to millions of low latency IOPS, and can leverage data stores and architectures that manage data redundancy and availability. Example applications are:"
230,Do High I/O instances provide any failover mechanisms or redundancy?,"Like other Amazon EC2 instance types, instance storage on Im4gn, Is4gen, I4i, I3 and I3en instances persists during the life of the instance. Customers are expected to build resilience into their applications. We recommend using databases and file systems that support redundancy and fault tolerance. Customers should back up data periodically to Amazon S3 for improved data durability."
231,Do High I/O instances support TRIM?,"The TRIM command allows the operating system to inform SSDs which blocks of data are no longer considered in use and can be wiped internally. In the absence of TRIM, future write operations to the involved blocks can slow down significantly. Im4gn, Is4gen, I4i, I3 and I3en instances support TRIM."
232,How do D3 and D3en instances compare to D2 instances?,"D3 and D3en instances offer improved specifications over D2 on the following compute, storage and network attributes:"
233,Do D3 and D3en instances encrypt storage volumes and network traffic?,Yes; data written onto the storage volumes will be encrypted at rest using AES-256-XTS. Network traffic between D3 and D3en instances in the same VPC or a peered VPC are encrypted by default using a 256-bit key.
234,What happens to my data when a system terminates?,"The data stored on a local instance store will persist only as long as that instance is alive. However, data that is stored on an Amazon EBS volume will persist independently of the life of the instance. Therefore, we recommend that you use the local instance store for temporary data and, for data requiring a higher level of durability, we recommend using Amazon EBS volumes or backing up the data to Amazon S3. If you are using an Amazon EBS volume as a root partition, you will need to set the Delete On Terminate flag to ""N"" if you want your Amazon EBS volume to persist outside the life of the instance."
235,What kind of performance can I expect from Amazon EBS volumes?,"Amazon EBS provides four current generation volume types that are divided into two major categories: SSD-backed storage for transactional workloads and HDD-backed storage for throughput intensive workloads. These volume types differ in performance characteristics and price, allowing you to tailor your storage performance and cost to the needs of your applications. For more information, see the Amazon EBS overview. For additional information on performance, see the Amazon EC2 User Guide's EBS Performance section."
236,What are Throughput Optimized HDD (st1) and Cold HDD (sc1) volume types?,"ST1 volumes are backed by hard disk drives (HDDs) and are ideal for frequently accessed, throughput intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. These volumes deliver performance in terms of throughput, measured in MB/s, and include the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume. ST1 is designed to deliver the expected throughput performance 99% of the time and has enough I/O credits to support a full-volume scan at the burst rate."
237,Which volume type should I choose?,"Amazon EBS includes two major categories of storage: SSD-backed storage for transactional workloads (performance depends primarily on IOPS) and HDD-backed storage for throughput workloads (performance depends primarily on throughput, measured in MB/s). SSD-backed volumes are designed for transactional, IOPS-intensive database workloads, boot volumes, and workloads that require high IOPS. SSD-backed volumes include Provisioned IOPS SSD (io1 and io2) and General Purpose SSD (gp2 and gp3). HDD-backed volumes are designed for throughput-intensive and big-data workloads, large I/O sizes, and sequential I/O pattern. HDD-backed volumes include Throughput Optimized HDD (st1) and Cold HDD (sc1). For more information, see the Amazon EBS overview."
238,Do you support multiple instances accessing a single volume?,"Yes, you can enable Multi-Attach on an EBS Provisioned IOPS io1 volume to allow a volume to be concurrently attached to up to sixteen Nitro-based EC2 instances within the same Availability Zone. For more information on Amazon EBS Multi-Attach, see the EBS product page."
239,Will I be able to access my EBS snapshots using the regular Amazon S3 APIs?,"No, EBS snapshots are only available through the Amazon EC2 APIs."
240,Do volumes need to be un-mounted in order to take a snapshot? Does the snapshot need to complete before the volume can be used again?,"No, snapshots can be done in real time while the volume is attached and in use. However, snapshots only capture data that has been written to your Amazon EBS volume, which might exclude any data that has been locally cached by your application or OS. In order to ensure consistent snapshots on volumes attached to an instance, we recommend cleanly detaching the volume, issuing the snapshot command, and then reattaching the volume. For Amazon EBS volumes that serve as root devices, we recommend shutting down the machine to take a clean snapshot."
241,Are snapshots versioned? Can I read an older snapshot to do a point-in-time recovery?,"Each snapshot is given a unique identifier, and customers can create volumes based on any of their existing snapshots."
242,What charges apply when using Amazon EBS shared snapshots?,"If you share a snapshot, you won't be charged when other users make a copy of your snapshot. If you make a copy of another user's shared volume, you will be charged normal EBS rates."
243,Can users of my Amazon EBS shared snapshots change any of my data?,"Users who have permission to create volumes based on your shared snapshots will first make a copy of the snapshot into their account. Users can modify their own copies of the data, but the data on your original snapshot and any other volumes created by other users from your original snapshot will remain unmodified."
244,How can I discover Amazon EBS snapshots that have been shared with me?,You can find snapshots that have been shared with you by selecting Private Snapshots from the viewing dropdown in the Snapshots section of the AWS Management Console. This section will list both snapshots you own and snapshots that have been shared with you.
245,How can I find what Amazon EBS snapshots are shared globally?,You can find snapshots that have been shared globally by selecting Public Snapshots from the viewing dropdown in the Snapshots section of the AWS Management Console.
246,Do you offer encryption on Amazon EBS volumes and snapshots?,Yes. EBS offers seamless encryption of data volumes and snapshots. EBS encryption better enables you to meet security and encryption compliance requirements.
247,How can I find a list of Amazon Public Data Sets?,All information on Public Data Sets is available in our Public Data Sets Resource Center. You can also obtain a listing of Public Data Sets within the AWS Management Console by choosing Amazon Snapshots from the viewing dropdown in the Snapshots section.
248,Where can I learn more about EBS?,See Amazon EBS FAQ.
249,How do I access a file system from an Amazon EC2 instance?,"To access your file system, you mount the file system on an Amazon EC2 Linux-based instance using the standard Linux mount command and the file system's DNS name. Once you've mounted, you can work with the files and directories in your file system just like you would with a local file system."
250,What Amazon EC2 instance types and AMIs work with Amazon EFS?,"Amazon EFS is compatible with all Amazon EC2 instance types and is accessible from Linux-based AMIs. You can mix and match the instance types connected to a single file system. For a step-by-step example of how to access a file system from an Amazon EC2 instance, please see the Amazon EFS Getting Started guide."
251,How do I load data into a file system?,You can load data into an Amazon EFS file system from your Amazon EC2 instances or from your on-premises datacenter servers.
252,How do I access my file system from outside my VPC?,Amazon EC2 instances within your VPC can access your file system directly. On-premises servers can mount your file systems via an AWS Direct Connect connection to your VPC.
253,How many Amazon EC2 instances can connect to a file system?,Amazon EFS supports one to thousands of Amazon EC2 instances connecting to a file system concurrently.
254,Where can I learn more about EFS?,You can visit the Amazon EFS FAQ page.
255,Is data stored on Amazon EC2 NVMe instance storage encrypted?,"Yes, all data is encrypted in an AWS Nitro hardware module prior to being written on the locally attached SSDs offered via NVMe instance storage."
256,What encryption algorithm is used to encrypt Amazon EC2 NVMe instance storage?,Amazon EC2 NVMe instance storage is encrypted using an XTS-AES-256 block cipher.
257,Are encryption keys unique to an instance or a particular device for NVMe instance storage?,"Encryption keys are securely generated within the Nitro hardware module, and are unique to each NVMe instance storage device that is provided with an EC2 instance."
258,What is the lifetime of encryption keys on NVMe instance storage?,"All keys are irrecoverably destroyed on any de-allocation of the storage, including instance stop and instance terminate actions."
259,Can I disable NVMe instance storage encryption?,"No, NVMe instance storage encryption is always on, and cannot be disabled."
260,Do the published IOPS performance numbers on I3 and I3en include data encryption?,"Yes, the documented IOPS numbers for Im4gn, Is4gen, I4i, I3 and I3en NVMe instance storage include encryption."
261,Does Amazon EC2 NVMe instance storage support AWS Key Management Service (KMS)?,"No, disk encryption on NVMe instance storage does not support integration with AWS KMS system. Customers cannot bring in their own keys to use with NVMe instance storage."
262,When should I use ENA Express?,"ENA Express works best for applications requiring high, single-flow throughput, like distributed storage systems and live media encoding. These workloads require high single flow bandwidth and low tail latency."
263,What protocols are supported by ENA Express?,ENA Express supports TCP by default. UDP can optionally be enabled through an API argument or within the management console.
264,What instances are supported?,"ENA Express is supported on Graviton-, Intel-, and AMD-based EC2 instances. It is supported on compute-optimized, memory-optimized, general purpose and storage-optimized based instances. For a complete list of supported instances, please see the ENA Express user guide."
265,What is the difference between Elastic Fabric Adapter (EFA) and ENA Express?,"EFA is a network interface built for HPC and ML applications, and it also leverages the SRD protocol. EFA requires a different network programming model, which uses the LibFabric interface to pass communication to the ENI. Unlike EFA, ENA Express helps you run your application transparently on TCP and UDP. Additionally, ENA Express allows for intra-Availability Zone (AZ) communication, while EFA is currently limited to communication within the same subnet."
266,"What happens if I'm running ENA Express on one instance, and it is communicating with another instance that doesn't support ENA Express or hasn't enabled it on the ENI?","ENA Express will detect if ENA Express has been enabled on another instance. If that instance doesn't support or hasn't enabled ENA Express, your instance will fallback to normal ENA operation. You will not be able to achieve any of the SRD performance benefits in this case, but there are no adverse effects either."
267,What operating systems are supported?,"The SRD functionality will be supported on all operating systems, but please note that ENA Express monitoring metrics will be available on only the EthTool in the latest Amazon Linux AMI or by installing the ENA driver version 2.8.0 or later from GitHub, with all operating systems supporting the metrics in the future."
268,Are there any additional costs to running ENA Express?,"No, ENA Express is free to use."
269,Why should I use EFA?,"EFA brings the scalability, flexibility, and elasticity of cloud to tightly coupled HPC applications. With EFA, tightly coupled HPC applications have access to lower and more consistent latency and higher throughput than traditional TCP channels, enabling them to scale better. EFA support can be enabled dynamically, on-demand on any supported EC2 instance without pre-reservation, giving you the flexibility to respond to changing business/workload priorities."
270,What types of applications can benefit from using EFA?,"HPC applications distribute computational workloads across a cluster of instances for parallel processing. Examples of HPC applications include computational fluid dynamics (CFD), crash simulations, and weather simulations. HPC applications are generally written using the Message Passing Interface (MPI) and impose stringent requirements for inter-instance communication in terms of both latency and bandwidth. Applications using MPI and other HPC middleware that supports the libfabric communication stack can benefit from EFA."
271,How does EFA communication work?,"EFA devices provide all ENA devices' functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality. Most applications will use existing middleware, such as the MPI, to interface with EFA. AWS has worked with a number of middleware providers to ensure support for the OS bypass functionality of EFA. Please note that communication using the OS bypass functionality is limited to instances within a single subnet of a virtual private cloud (VPC)."
272,Which instance types support EFA?,"For a full list of supported EC2 instances, refer to this page in our documentation."
273,What are the differences between an EFA ENI and an ENA ENI?,"An ENA ENI provides traditional IP networking features necessary to support VPC networking. An EFA ENI provides all the functionality of an ENA ENI, plus hardware support for applications to communicate directly with the EFA ENI without involving the instance kernel (OS-bypass communication) using an extended programming interface. Due to the advanced capabilities of the EFA ENI, EFA ENIs can only be attached at launch or to stopped instances."
274,What are the pre-requisites to enabling EFA on an instance?,EFA support can be enabled either at the launch of the instance or added to a stopped instance. EFA devices cannot be attached to a running instance.
275,What networking capabilities are included in this feature?,"We currently support enhanced networking capabilities using SR-IOV (Single Root I/O Virtualization). SR-IOV is a method of device virtualization that provides higher I/O performance and lower CPU utilization compared to traditional implementations. For supported Amazon EC2 instances, this feature provides higher packet per second (PPS) performance, lower inter-instance latencies, and very low network jitter."
276,Why should I use Enhanced Networking?,"If your applications benefit from high packet-per-second performance and/or low latency networking, Enhanced Networking will provide significantly improved performance, consistence of performance and scalability."
277,How can I enable Enhanced Networking on supported instances?,"In order to enable this feature, you must launch an HVM AMI with the appropriate drivers. The instances listed as current generation use ENA for enhanced networking. Amazon Linux AMI includes both of these drivers by default. For AMIs that do not contain these drivers, you will need to download and install the appropriate drivers based on the instance types you plan to use. You can use Linux or Windows instructions to enable Enhanced Networking in AMIs that do not include the SR-IOV driver by default. Enhanced Networking is only supported in Amazon VPC."
278,Do I need to pay an additional fee to use Enhanced Networking?,"No, there is no additional fee for Enhanced Networking. To take advantage of Enhanced Networking you need to launch the appropriate AMI on a supported instance type in a VPC."
279,Which instance types support Enhanced Networking?,"Depending on your instance type, you can enable enhanced networking by using one of the following mechanisms:"
280,What load balancing options does the Elastic Load Balancing service offer?,"Elastic Load Balancing offers two types of load balancers that both feature high availability, automatic scaling, and robust security. These include the Classic Load Balancer that routes traffic based on either application or network level information, and the Application Load Balancer that routes traffic based on advanced application level information that includes the content of the request."
281,When should I use the Classic Load Balancer and when should I use the Application Load Balancer?,"The Classic Load Balancer is ideal for simple load balancing of traffic across multiple EC2 instances, while the Application Load Balancer is ideal for applications needing advanced routing capabilities, microservices, and container-based architectures. Please visit Elastic Load Balancing for more information."
282,Why am I limited to 5 Elastic IP addresses per region?,"Public (IPV4) internet addresses are a scarce resource. There is only a limited amount of public IP space available, and Amazon EC2 is committed to helping use that space efficiently."
283,Why am I charged when my Elastic IP address is not associated with a running instance?,"In order to help ensure our customers are efficiently using the Elastic IP addresses, we impose a small hourly charge for each address when it is not associated with a running instance."
284,Do I need one Elastic IP address for every instance that I have running?,"No. You do not need an Elastic IP address for all your instances. By default, every instance comes with a private IP address and an internet routable public IP address. The private IP address remains associated with the network interface when the instance is stopped and restarted, and is released when the instance is terminated. The public address is associated exclusively with the instance until it is stopped, terminated or replaced with an Elastic IP address. These IP addresses should be adequate for many applications where you do not need a long lived internet routable end point. Compute clusters, web crawling, and backend services are all examples of applications that typically do not require Elastic IP addresses."
285,How long does it take to remap an Elastic IP address?,The remap process currently takes several minutes from when you instruct us to remap the Elastic IP until it fully propagates through our system.
286,Can I configure the reverse DNS record for my Elastic IP address?,"All Elastic IP addresses come with reverse DNS, in a standard template of the form ec2-1-2-3-4.region.compute.amazonaws.com. For customers requiring custom reverse DNS settings for internet-facing applications that use IP-based mutual authentication (such as sending email from EC2 instances), you can configure the reverse DNS record of your Elastic IP address by filling out this form. Alternatively, please contact AWS Customer Support if you want AWS to delegate the management of the reverse DNS for your Elastic IPs to your authoritative DNS name servers (such as Amazon Route 53), so that you can manage your own reverse DNS PTR records to support these use-cases. Note that a corresponding forward DNS record pointing to that Elastic IP address must exist before we can create the reverse DNS record."
287,How do I prevent other people from viewing my systems?,"You have complete control over the visibility of your systems. The Amazon EC2 security systems allow you to place your running instances into arbitrary groups of your choice. Using the web services interface, you can then specify which groups may communicate with which other groups, and also which IP subnets on the Internet may talk to which groups. This allows you to control access to your instances in our highly dynamic environment. Of course, you should also secure your instance as you would any other server."
288,Can I get a history of all EC2 API calls made on my account for security analysis and operational troubleshooting purposes?,"Yes. To receive a history of all EC2 API calls (including VPC and EBS) made on your account, you simply turn on CloudTrail in the AWS Management Console. For more information, visit the CloudTrail home page."
289,Where can I find more information about security on AWS?,For more information on security on AWS please refer to our Amazon Web Services: Overview of Security Processes white paper and to our Amazon EC2 running Windows Security Guide.
290,What is the minimum time interval granularity for the data that Amazon CloudWatch receives and aggregates?,Metrics are received and aggregated at 1 minute intervals.
291,Which operating systems does Amazon CloudWatch support?,Amazon CloudWatch receives and provides metrics for all Amazon EC2 instances and should work with any operating system currently supported by the Amazon EC2 service.
292,Will I lose the metrics data if I disable monitoring for an Amazon EC2 instance?,"You can retrieve metrics data for any Amazon EC2 instance up to 2 weeks from the time you started to monitor it. After 2 weeks, metrics data for an Amazon EC2 instance will not be available if monitoring was disabled for that Amazon EC2 instance. If you want to archive metrics beyond 2 weeks you can do so by calling mon-get-stats command from the command line and storing the results in Amazon S3 or Amazon SimpleDB."
293,Can I access the metrics data for a terminated Amazon EC2 instance or a deleted Elastic Load Balancer?,Yes. Amazon CloudWatch stores metrics for terminated Amazon EC2 instances or deleted Elastic Load Balancers for 2 weeks.
294,Does the Amazon CloudWatch monitoring charge change depending on which type of Amazon EC2 instance I monitor?,"No, the Amazon CloudWatch monitoring charge does not vary by Amazon EC2 instance type."
295,Why does the graphing of the same time window look different when I view in 5 minute and 1 minute periods?,"If you view the same time window in a 5 minute period versus a 1 minute period, you may see that data points are displayed in different places on the graph. For the period you specify in your graph, Amazon CloudWatch will find all the available data points and calculates a single, aggregate point to represent the entire period. In the case of a 5 minute period, the single data point is placed at the beginning of the 5 minute time window. In the case of a 1 minute period, the single data point is placed at the 1 minute mark. We recommend using a 1 minute period for troubleshooting and other activities that require the most precise graphing of time periods."
296,Can I automatically scale Amazon EC2 Auto Scaling Groups?,"Yes. Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application. EC2 Auto Scaling helps you maintain application availability through fleet management for EC2 instances, which detects and replaces unhealthy instances, and by scaling your Amazon EC2 capacity up or down automatically according to conditions you define. You can use EC2 Auto Scaling to automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs."
297,Why should I hibernate an instance?,"You can hibernate an instance to get your instance and applications up and running quickly, if they take a long time to bootstrap (e.g. load memory caches). You can start instances, bring them to a desired state and hibernate them. These pre-warmed instances can then be resumed to reduce the time it takes for an instance to return to service. Hibernation retains memory state across Stop/Start cycles."
298,What happens when I hibernate my instance?,"When you hibernate an instance, data from your EBS root volume and any attached EBS data volumes is persisted. Additionally, contents from the instance's memory (RAM) are persisted to EBS root volume. When the instance is restarted, it returns to its previous state and reloads the RAM contents."
299,What is the difference between hibernate and stop?,"In the case of hibernate, your instance gets hibernated and the RAM data persisted. In the case of Stop, your instance gets shut down and RAM is cleared."
300,How much does it cost to hibernate an instance?,"Hibernating instances are charged at standard EBS rates for storage. As with a stopped instance, you do not incur instance usage fees while an instance is hibernating."
301,How can I hibernate an instance?,"Hibernation needs to be enabled when you launch the instance. Once enabled, you can use the StopInstances API with an additional Hibernate' parameter to trigger hibernation. You can also do this through the console by selecting your instance, then clicking Actions> Instance State > Stop - Hibernate. For more information on using hibernation, refer to the user guide."
302,How can I resume a hibernating instance?,"You can resume by calling the StartInstances API as you would for a regular stopped instance. You can also do this through the console by selecting your instance, then clicking Actions > Instance State > Start."
303,Can I enable hibernation on an existing instance?,"No, you cannot enable hibernation on an existing instance (running or stopped). This needs to be enabled during instance launch."
304,How can I tell that an instance is hibernated?,You can tell that an instance is hibernated by looking at the state reason. It should be Client.UserInitiatedHibernate'. This is visible on the console under Instances - Details view or in the DescribeInstances API response as the reason field.
305,What is the state of an instance when it is hibernating?,Hibernated instances are in Stopped' state.
306,What data is saved when I hibernate an instance?,"EBS volume storage (boot volume and attached data volumes) and memory (RAM) are saved. Your private IP address remains the same (for VPC), as does your elastic IP address (if applicable). The network layer behavior will be similar to that of EC2 Stop-Start workflow."
307,Where is my data stored when I hibernate an instance?,"As with the Stop feature, root device and attached device data are stored on the corresponding EBS volumes. Memory (RAM) contents are stored on the EBS root volume."
308,Is my memory (RAM) data encrypted when it is moved to EBS?,"Yes, RAM data is always encrypted when it is moved to the EBS root volume. Encryption on the EBS root volume is enforced at instance launch time. This is to ensure protection for any sensitive content that is in memory at the time of hibernation."
309,How long can I keep my instance hibernated?,"We do not support keeping an instance hibernated for more than 60 days. You need to resume the instance and go through Stop and Start (without hibernation) if you wish to keep the instance around for a longer duration. We are constantly working to keep our platform up-to-date with upgrades and security patches, some of which can conflict with the old hibernated instances. We will notify you for critical updates that require you to resume the hibernated instance to perform a shutdown or a reboot."
310,What are the prerequisites to hibernate an instance?,"To use hibernation, the root volume must be an encrypted EBS volume. The instance needs to be configured to receive the ACPID signal for hibernation (or use the Amazon published AMIs that are configured for hibernation). Additionally, your instance should have sufficient space available on your EBS root volume to write data from memory."
311,Which instances and operating systems support hibernation?,"For instances running Amazon Linux, Amazon Linux 2, Ubuntu, and Windows, Hibernation is supported across C3, C4, C5, C5d, I3, M3, M4, M5, M5a, M5ad, M5d, R3, R4, R5, R5a, R5ad, R5d, T2, T3, and T3a instances."
312,Should I use specific Amazon Machine Image (AMIs) if I want to hibernate my instance?,"You can use any AMI that is configured to support hibernation. You can use AWS published AMIs that support hibernation by default. Alternatively, you can create a custom image from an instance after following the hibernation pre-requisite checklist and configuring your instance appropriately."
313,What if my EBS root volume is not large enough to store memory state (RAM) for hibernation?,"To enable hibernation, space is allocated on the root volume to store the instance memory (RAM). Make sure that the root volume is large enough to store the RAM contents and accommodate your expected usage, e.g. OS, applications. If the EBS root volume does not have enough space, hibernation will fail and the instance will get shut down instead."
314,What is VM Import/Export?,VM Import/Export enables customers to import Virtual Machine (VM) images in order to create Amazon EC2 instances. Customers can also export previously imported EC2 instances to create VMs. Customers can use VM Import/Export to leverage their previous investments in building VMs by migrating their VMs to Amazon EC2.
315,What operating systems are supported?,"VM Import/Export currently supports Windows and Linux VMs, including multiple editions of Windows Server, Red Hat Enterprise Linux (RHEL), CentOS, Ubuntu, Debian and others. For more details on VM Import, including supported file formats, architectures, and operating system configurations, please see the VM Import/Export section of the VM Import/Export."
316,What VM file formats are supported?,"You can import VMware ESX VMDK images, Citrix Xen VHD images, Microsoft Hyper-V VHD images and RAW images as Amazon EC2 instances. You can export EC2 instances to VMware ESX VMDK, VMware ESX OVA, Microsoft Hyper-V VHD or Citrix Xen VHD images. For a full list of supported operating systems, please see What operating systems are supported?"
317,What is VMDK?,"VMDK is a file format that specifies a virtual machine hard disk encapsulated within a single file. It is typically used by virtual IT infrastructures such as those sold by VMware, Inc."
318,How do I prepare a VMDK file for import using the VMware vSphere client?,The VMDK file can be prepared by calling File-Export-Export to OVF template in VMware vSphere Client. The resulting VMDK file is compressed to reduce the image size and is compatible with VM Import/Export. No special preparation is required if you are using the Amazon EC2 VM Import Connector vApp for VMware vCenter.
319,What is VHD?,VHD (Virtual Hard Disk) is a file format that specifies a virtual machine hard disk encapsulated within a single file. The VHD image format is used by virtualization platforms such as Microsoft Hyper-V and Citrix Xen.
320,How do I prepare a VHD file for import from Citrix Xen?,"Open Citrix XenCenter and select the virtual machine you want to export. Under the Tools menu, choose ""Virtual Appliance Tools"" and select ""Export Appliance"" to initiate the export task. When the export completes, you can locate the VHD image file in the destination directory you specified in the export dialog."
321,How do I prepare a VHD file for import from Microsoft Hyper-V?,"Open the Hyper-V Manager and select the virtual machine you want to export. In the Actions pane for the virtual machine, select ""Export"" to initiate the export task. Once the export completes, you can locate the VHD image file in the destination directory you specified in the export dialog."
322,Are there any other requirements when importing a VM into Amazon EC2?,"The virtual machine must be in a stopped state before generating the VMDK or VHD image. The VM cannot be in a paused or suspended state. We suggest that you export the virtual machine with only the boot volume attached. You can import additional disks using the ImportVolume command and attach them to the virtual machine using AttachVolume. Additionally, encrypted disks (e.g. Bit Locker) and encrypted image files are not supported. You are also responsible for ensuring that you have all necessary rights and licenses to import into AWS and run any software included in your VM image."
323,Does the virtual machine need to be configured in any particular manner to enable import to Amazon EC2?,"Ensure Remote Desktop (RDP) or Secure Shell (SSH) is enabled for remote access and verify that your host firewall (Windows firewall, iptables, or similar), if configured, allows access to RDP or SSH. Otherwise, you will not be able to access your instance after the import is complete. Please also ensure that Windows VMs are configured to use strong passwords for all users including the administrator and that Linux VMs are configured with a public key for SSH access."
324,How do I import a virtual machine to an Amazon EC2 instance?,You can import your VM images using the Amazon EC2 API tools:
325,How do I export an Amazon EC2 instance back to my on-premise virtualization environment?,You can export your Amazon EC2 instance using the Amazon EC2 CLI tools:
326,Are there any other requirements when exporting an EC2 instance using VM Import/Export?,"You can export running or stopped EC2 instances that you previously imported using VM Import/Export. If the instance is running, it will be momentarily stopped to snapshot the boot volume. EBS data volumes cannot be exported. EC2 instances with more than one network interface cannot be exported."
327,Can I export Amazon EC2 instances that have one or more EBS data volumes attached?,"Yes, but VM Import/Export will only export the boot volume of the EC2 instance."
328,What does it cost to import a virtual machine?,"You will be charged standard Amazon S3 data transfer and storage fees for uploading and storing your VM image file. Once your VM is imported, standard Amazon EC2 instance hour and EBS service fees apply. If you no longer wish to store your VM image file in S3 after the import process completes, use the ec2-delete-disk-image command line tool to delete your disk image from Amazon S3."
329,What does it cost to export a VM?,"You will be charged standard Amazon S3 storage fees for storing your exported VM image file. You will also be charged standard S3 data transfer charges when you download the exported VM file to your on-premise virtualization environment. Finally, you will be charged standard EBS charges for storing a temporary snapshot of your EC2 instance. To minimize storage charges, delete the VM image file in S3 after downloading it to your virtualization environment."
330,"When I import a VM of Windows Server 2003 or 2008, who is responsible for supplying the operating system license?","When you launch an imported VM using Microsoft Windows Server 2003 or 2008, you will be charged standard instance hour rates for Amazon EC2 running the appropriate Windows Server version, which includes the right to utilize that operating system within Amazon EC2. You are responsible for ensuring that all other installed software is properly licensed."
331,Can I continue to use the AWS provided Microsoft Windows license key after exporting an EC2 instance back to my on-premises virtualization environment?,"No. After an EC2 instance has been exported, the license key utilized in the EC2 instance is no longer available. You will need to reactivate and specify a new license key for the exported VM after it is launched in your on-premises virtualization platform."
332,"When I import a VM with Red Hat Enterprise Linux (RHEL), who is responsible for supplying the operating system license?","When you import Red Hat Enterprise Linux (RHEL) VM images, you can use license portability for your RHEL instances. With license portability, you are responsible for maintaining the RHEL licenses for imported instances, which you can do using Cloud Access subscriptions for Red Hat Enterprise Linux. Please contact Red Hat to learn more about Cloud Access and to verify your eligibility."
333,How long does it take to import a virtual machine?,"The length of time to import a virtual machine depends on the size of the disk image and your network connection speed. As an example, a 10 GB Windows Server 2008 SP2 VMDK image takes approximately 2 hours to import when it's transferred over a 10 Mbps network connection. If you have a slower network connection or a large disk to upload, your import may take significantly longer."
334,In which Amazon EC2 Regions can I use VM Import/Export?,Visit the Region Table page to see product service availability by Region.
335,How many simultaneous import or export tasks can I have?,Each account can have up to five active import tasks and five export tasks per region.
336,Can I run imported virtual machines in Amazon Virtual Private Cloud (Amazon VPC)?,"Yes, you can launch imported virtual machines within Amazon VPC."
337,Can I use the AWS Management Console with VM Import/Export?,"No. VM Import/Export commands are available via EC2 CLI and API. You can also use the AWS Management Portal for vCenter to import VMs into Amazon EC2. Once imported, the resulting instances are available for use via the AWS Management Console."
338,How will I be charged and billed for my use of Amazon EC2?,"You pay only for what you use. Displayed pricing is an hourly rate but depending on which instances you choose, you pay by the hour or second (minimum of 60 seconds) for each instance type. Partial instance-hours consumed are billed based on instance usage. Data transferred between AWS services in different regions is charged at standard inter-region data transfer rates. Usage for other Amazon Web Services is billed separately from Amazon EC2."
339,When does billing of my Amazon EC2 systems begin and end?,"Billing commences when Amazon EC2 initiates the boot sequence of an AMI instance. Billing ends when the instance terminates, which could occur through a web services command, by running ""shutdown -h"", or through instance failure. When you stop an instance, we shut it down but don't charge hourly usage for a stopped instance, or data transfer fees, but we do charge for the storage for any Amazon EBS volumes. To learn more, visit the AWS Documentation."
340,What defines billable EC2 instance usage?,"Instance usages are billed for any time your instances are in a ""running"" state. If you no longer wish to be charged for your instance, you must ""stop"" or ""terminate"" the instance to avoid being billed for additional instance usage. Billing starts when an instance transitions into the running state."
341,"If I have two instances in different availability zones, how will I be charged for regional data transfer?","Each instance is charged for its data in and data out at corresponding Data Transfer rates. Therefore, if data is transferred between these two instances, it is charged at ""Data Transfer Out from EC2 to Another AWS Region"" for the first instance and at ""Data Transfer In from Another AWS Region"" for the second instance. Please refer to this page for detailed data transfer pricing."
342,"If I have two instances in different Regions, how will I be charged for data transfer?","Each instance is charged for its data in and data out at Inter-Region Data Transfer rates. Therefore, if data is transferred between these two instances, it is charged at Inter-Region Data Transfer Out for the first instance and at Inter-Region Data Transfer In for the second instance."
343,How will my monthly bill show per-second versus per-hour?,"Although EC2 charges in your monthly bill will now be calculated based on a per second basis, for consistency, the monthly EC2 bill will show cumulative usage for each instance that ran in a given month in decimal hours. For example, an instance running for 1 hour 10 minutes and 4 seconds would look like 1.1677. Read this blog for an example of the detailed billing report."
344,Do your prices include taxes?,"Except as otherwise noted, our prices are exclusive of applicable taxes and duties, including VAT and applicable sales tax. For customers with a Japanese billing address, use of AWS services is subject to Japanese Consumption Tax. Learn more."
345,Will I incur any data transfer out to the internet charges when I move my data out of AWS?,"AWS offers eligible customers free data transfer out to the internet when they move all of their data off of AWS, in accordance with the process below."
346,I want to move my data out of AWS. How do I request free data transfer out to the internet?,Complete the following steps:
347,Why do I have to request AWS' pre-approval for free data transfer out to the internet before moving my data out of AWS?,"AWS customers make hundreds of millions of data transfers each day, and we generally don't know the reason for any given data transfer. For example, customers may be transferring data to an end user of their application, to a visitor of their website, or to another cloud or on-premises environment for backup purposes. Accordingly, the only way we know that your data transfer is to support your move off of AWS is if you tell us beforehand."
348,What is a Convertible RI?,A Convertible RI is a type of Reserved Instance with attributes that can be changed during the term.
349,When should I purchase a Convertible RI instead of a Standard RI?,"The Convertible RI is useful for customers who can commit to using EC2 instances for a three-year term in exchange for a significant discount on their EC2 usage, are uncertain about their instance needs in the future, or want to benefit from changes in price."
350,What term length options are available on Convertible RIs?,"Like Standard RIs, Convertible RIs are available for purchase for a one-year or three-year term."
351,"Can I exchange my Convertible RI to benefit from a Convertible RI matching a different instance type, operating system, tenancy, or payment option?","Yes, you can select a new instance type, operating system, tenancy, or payment option when you exchange your Convertible RIs. You also have the flexibility to exchange a portion of your Convertible RI or merge the value of multiple Convertible RIs in a single exchange."
352,Can I transfer a Convertible or Standard RI from one region to another?,"No, an RI is associated with a specific region, which is fixed for the duration of the reservation's term."
353,How do I change the configuration of a Convertible RI?,You can change the configuration of your Convertible RI using the EC2 Management Console or the GetReservedInstancesExchangeQuote API. You also have the flexibility to exchange a portion of your Convertible RI or merge the value of multiple Convertible RIs in a single exchange. Click here to learn more about exchanging Convertible RIs.
354,Do I need to pay a fee when I exchange my Convertible RIs?,"No, you do not pay a fee when you exchange your RIs. However you may need to pay a one-time true-up charge that accounts for differences in pricing between the Convertible RIs that you have and the Convertible RIs that you want."
355,How do Convertible RI exchanges work?,"When you exchange one Convertible RI for another, EC2 ensures that the total value of the Convertible RIs is maintained through a conversion. So, if you are converting your RI with a total value of $1000 for another RI, you will receive a quantity of Convertible RIs with a value that's equal to or greater than $1000. You cannot convert your Convertible RI for Convertible RI(s) of a lesser total value."
356,Can you define total value?,The total value is the sum of all expected payments that you'd make during the term for the RI.
357,Can you walk me through how the true-up cost is calculated for a conversion between two All Upfront Convertible RIs?,"Sure, let's say you purchased an All Upfront Convertible RI for $1000 upfront, and halfway through the term you decide to change the attributes of the RI. Since you're halfway through the RI term, you have $500 left of prorated value remaining on the RI. The All Upfront Convertible RI that you want to convert into costs $1,200 upfront today. Since you only have half of the term left on your existing Convertible RI, there is $600 of value remaining on the desired new Convertible RI. The true-up charge that you'll pay will be the difference in upfront value between original and desired Convertible RIs, or $100 ($600 - $500)."
358,Can you walk me through a conversion between No Upfront Convertible RIs?,"Unlike conversions between Convertible RIs with an upfront value, since you're converting between RIs without an upfront cost, there will not be a true-up charge. However, the amount you pay on an hourly basis before the exchange will need to be greater than or equal to the amount you pay on a total hourly basis after the exchange."
359,Can I customize the number of instances that I receive as a result of a Convertible RI exchange?,"No, EC2 uses the value of the Convertible RIs you're trading in to calculate the minimal number of Convertible RIs you'll receive while ensuring the result of the exchange gives you Convertible RIs of equal or greater value."
360,Are there exchange limits for Convertible RIs?,"No, there are no exchange limits for Convertible RIs."
361,Do I have the freedom to choose any instance type when I exchange my Convertible RIs?,"No, you can only exchange into Convertible RIs that are currently offered by AWS."
362,Can I upgrade the payment option associated with my Convertible RI?,"Yes, you can upgrade the payment option associated with your RI. For example, you can exchange your No Upfront RIs for Partial or All Upfront RIs to benefit from better pricing. You cannot change the payment option from All Upfront to No Upfront, and cannot change from Partial Upfront to No Upfront."
363,Do Convertible RIs allow me to benefit from price reductions when they happen?,"Yes, you can exchange your RIs to benefit from lower pricing. For example, if the price of new Convertible RIs reduces by 10%, you can exchange your Convertible RIs and benefit from the 10% reduction in price."
364,What is Amazon EC2 Fleet?,"With a single API call, EC2 Fleet lets you provision compute capacity across different instance types, Availability Zones and across On-Demand, Reserved Instances (RI) and Spot Instances purchase models to help optimize scale, performance and cost."
365,If I currently use Amazon EC2 Spot Fleet should I migrate to Amazon EC2 Fleet?,"If you are leveraging Amazon EC2 Spot Instances with Spot Fleet, you can continue to use that. Spot Fleet and EC2 Fleet offer the same functionality. There is no requirement to migrate."
366,Can I use Reserved Instance (RI) discounts with Amazon EC2 Fleet?,"Yes. Similar to other EC2 APIs or other AWS services that launch EC2 instances, if the On-Demand instance launched by EC2 Fleet matches an existing RI, that instance will receive the RI discount. For example, if you own Regional RIs for M4 instances and you have specified only M4 instances in your EC2 Fleet, RI discounts will be automatically applied to this usage of M4."
367,Will Amazon EC2 Fleet failover to On-Demand if EC2 Spot capacity is not fully fulfilled?,"No, EC2 Fleet will continue to attempt to meet your desired Spot capacity based on the number of Spot instances you requested in your Fleet launch specification."
368,What is the pricing for Amazon EC2 Fleet?,EC2 Fleet comes at no additional charge; you only pay for the underlying resources that EC2 Fleet launches.
369,Can you provide a real world example of how I can use Amazon EC2 Fleet?,"There are a number of ways to take advantage of Amazon EC2 Fleet, such as in big data workloads, containerized application, grid processing workloads, etc. In this example of a genomic sequencing workload, you can launch a grid of worker nodes with a single API call: select your favorite instances, assign weights for these instances, specify target capacity for On-Demand and Spot Instances, and build a fleet within seconds to crunch through genomic data quickly."
370,How can I allocate resources in an Amazon EC2 Fleet?,"By default, EC2 Fleet will launch the On-Demand option that is the lowest price. For Spot Instances, EC2 Fleet provides three allocation strategies: capacity-optimized, lowest price and diversified. The capacity-optimized allocation strategy attempts to provision Spot Instances from the most available Spot Instance pools by analyzing capacity metrics. This strategy is a good choice for workloads that have a higher cost of interruption such as big data and analytics, image and media rendering, machine learning, and high performance computing."
371,Can I submit a multi-region Amazon EC2 Fleet request?,"No, we do not support multi-region EC2 Fleet requests."
372,Can I tag an Amazon EC2 Fleet?,"Yes. You can tag an EC2 Fleet request to create business-relevant tag groupings to organize resources along technical, business, and security dimensions."
373,Can I modify my Amazon EC2 Fleet?,"Yes, you can modify the total target capacity of your EC2 Fleet when in maintain mode. You may need to cancel the request and submit a new one to change other request configuration parameters."
374,Can I specify a different AMI for each instance type that I want to use?,"Yes, simply specify the AMI you'd like to use in each launch specification you provide in your EC2 Fleet."
375,What are Amazon EC2 Capacity Blocks for ML?,"Amazon EC2 Capacity Blocks for ML allow you to reserve GPU instances in an Amazon EC2 UltraClusters to run your machine learning (ML) workloads. With Amazon EC2 Capacity Blocks, you can reserve GPU capacity starting on a future date for duration up to 14 days and in cluster sizes of one to 64 instances. When your EC2 Capacity Block reservation date and time arrives, you will be able to launch your instances and use them until your reservation time ends."
376,Why should I use EC2 Capacity Blocks?,"EC2 Capacity Blocks make it easy to access the highest-performing GPU instances in Amazon EC2 for ML, even in the face of industry-wide GPU shortages. Use EC2 Capacity Blocks to ensure capacity availability for GPU instances to plan your ML development with confidence. EC2 Capacity Blocks are delivered in EC2 UltraClusters so you can leverage the best network latency and throughput performance available in EC2."
377,When should I use Amazon EC2 Capacity Blocks instead of On-Demand Capacity Reservations?,"You should use EC2 Capacity Blocks when you need short-term capacity assurance to train or fine-tune ML models, run experiments, build prototypes, or handle surges in demand for ML applications. With EC2 Capacity Blocks, you can have peace of mind knowing you'll have access to GPU resources on a specific date to run your ML workloads. You should use On-Demand Capacity Reservations for all other workload types that need assurance, such as business-critical applications, regulatory requirements, or disaster recovery."
378,How do I get started with EC2 Capacity Blocks?,"You can search for available EC2 Capacity Blocks based on your capacity needs in the AWS Management Console, AWS Command Line Interface (AWS CLI), and AWS SDKs. Once you purchase an EC2 Capacity Block, a reservation is created in your account. When the EC2 Capacity Block start time arrives, EC2 will emit an event through Amazon EventBridge to indicate that the reservation is now active and available for use. To use an active EC2 Capacity Block, select the Capacity Block purchase option and target the capacity reservation ID for your EC2 Capacity Block while launching EC2 instances. As your EC2 Capacity Block end time approaches, EC2 will emit event through EventBridge letting you know your reservation is ending soon so you can checkpoint your workload. Around 30 minutes before your EC2 Capacity Block expires, AWS will begin terminating any running instances. The amount you are charged for your EC2 Capacity Block does not include the last 30 minutes of the reservation."
379,"Which instance types do EC2 Capacity Blocks support, and which AWS Regions are they available in?",EC2 Capacity Blocks support EC2 p5.48xlarge instances in the AWS US East (N. Virginia) and US East (Ohio) regions and EC2 p4d.24xlarge instances in US East (Ohio) and US West (Oregon) regions.
380,What size options are available with EC2 Capacity Blocks?,"EC2 Capacity Blocks are available in cluster sizes of 1, 2, 4, 8, 16, 32, and 64 instances, and they can be reserved for up to 14 days in one-day multiples."
381,How far in advance can I reserve an EC2 Capacity Block?,You can purchase an EC2 Capacity Block as far out as eight weeks into the future. All EC2 Capacity Blocks reservations start at 11:30 AM Coordinated Universal Time (UTC).
382,What happens if there are no EC2 Capacity Blocks available that meet my specifications?,"If there are no EC2 Capacity Blocks that match your requirements, you can retry your request with different input parameters. We recommend that you use the widest date range possible in your search requests for the best chance at finding an EC2 Capacity Block."
383,Can I modify or cancel my EC2 Capacity Block?,"No, an EC2 Capacity Block cannot be modified or canceled once it is reserved."
384,How much do EC2 Capacity Blocks cost?,"When you search for an EC2 Capacity Block across dates, AWS returns the lowest-priced offering available that meets your specifications in the date range you provide. The price for an EC2 Capacity Block depends on total available supply and demand at the time you purchase the reservation. You can view the price of an EC2 Capacity Block offering before you reserve it, and the price of an EC2 Capacity Block is charged up front at the time the reservation is made. The price of an EC2 Capacity Block does not change after it is reserved. When you launch instances in an active EC2 Capacity Block, you will only be charged for the usage of any premium operating system on a pay-as-you-go basis."
385,Do Savings Plans and Reserved Instances (RI) discounts apply to EC2 Capacity Blocks?,"No, EC2 Capacity Blocks are not covered by Savings Plans or RI discounts."
386,Can I use EC2 Capacity Blocks with Amazon SageMaker?,"At this time, EC2 Capacity Blocks only support EC2 instances."
387,How much do On-Demand Capacity Reservations cost?,"When the On-Demand Capacity Reservation is active, you will pay equivalent instance charges whether you run the instances or not. If you do not use the reservation, the charge will show up as an unused reservation on your EC2 bill. When you run an instance that matches the attributes of a reservation, you just pay for the instance and nothing for the reservation. There are no upfront or additional charges."
388,Can I get a discount for On-Demand Capacity Reservation usage?,"Yes. Savings Plans or Regional RI (RI scoped to a Region) discounts apply to On-Demand Capacity Reservations. When you are running an instance within your reservation, you are not charged for the reservation. Savings Plans or Regional RIs will apply to this usage as if it were On-Demand usage. When the reservation is not used, AWS Billing will automatically apply your discount when the attributes of the unused On-Demand Capacity Reservation match the attributes of an active Savings Plan or Regional RI."
389,"When should I use Savings Plans, EC2 RIs, and On-Demand Capacity Reservations?","Use Savings Plans or Regional RIs to reduce your bill while committing to a one- or three-year term. Savings Plans offer significant savings over On-Demand, just like EC2 RIs, but automatically reduce customers' bills on compute usage across any AWS Region, even as usage changes. Use On-Demand Capacity Reservations if you need the additional confidence in your ability to launch instances. On-Demand Capacity Reservations can be created for any duration and can be managed independently of your Savings Plans or RIs. If you have Savings Plans or Regional RIs, they will automatically apply to matching On-Demand Capacity Reservations. This gives you the flexibility to selectively add On-Demand Capacity Reservations to a portion of your instance footprint and still reduce your bill for that usage."
390,I have a Zonal RI (RI scoped to an AZ) that also provides a capacity reservation. How does this compare with an On-Demand Capacity Reservation?,A Zonal RI provides both a discount and a capacity reservation in a specific AZ in return for a one- to three-year commitment. An On-Demand Capacity Reservation allows you to create and manage reserved capacity independently of your RI commitment and term length.
391,I created an On-Demand Capacity Reservation. How can I use it?,"An On-Demand Capacity Reservation is tied to a specific AZ and is, by default, automatically used by running instances in that AZ. When you launch new instances that match the reservation attributes, they will automatically match to the reservation."
392,How many instances am I allowed to reserve?,"The number of instances that you are allowed to reserve is based on your account's On-Demand instance limit. You can reserve as many instances as that limit allows, minus the number of instances that are already running."
393,Can I modify an On-Demand Capacity Reservation after it has started?,Yes. You can reduce the number of instances that you reserved at any time. You can also increase the number of instances (subject to availability). You can also modify the end time of your reservation. You cannot modify an On-Demand Capacity Reservation that has ended or has been deleted.
394,Can I end an On-Demand Capacity Reservation after it has started?,"Yes. You can end an On-Demand Capacity Reservation by canceling it using the console or API/SDK, or by modifying your reservation to specify an end time that makes it expire automatically. Running instances are unaffected by changes to your On-Demand Capacity Reservation, including deletion or expiration of a reservation."
395,Where can I find more information about using On-Demand Capacity Reservations?,Refer to Linux or Windows technical documentation to learn about creating and using an On-Demand Capacity Reservation.
396,Can I share an On-Demand Capacity Reservation with another AWS account?,"Yes, you can share On-Demand Capacity Reservations with other AWS accounts or within your AWS Organization through AWS Resource Access Manager (AWS RAM). You can share EC2 On-Demand Capacity Reservations in three easy steps: Create a Resource Share using AWS RAM, add resources (On-Demand Capacity Reservations) to the Resource Share, and specify the target accounts that you wish to share the resources with."
397,What happens when I share an On-Demand Capacity Reservation with another AWS account?,"When an On-Demand Capacity Reservation is shared with other accounts, those accounts can consume the reserved capacity to run their EC2 instances. The exact behavior depends on the preferences set on the On-Demand Capacity Reservation. By default, On-Demand Capacity Reservations automatically match existing and new instances from other accounts that have shared access to the reservation. You can also target an On-Demand Capacity Reservation for specific workloads/instances. Individual accounts can control which of their instances consume On-Demand Capacity Reservations. Refer to Linux or Windows technical documentation to learn more about the instance matching options."
398,Is there an additional charge for sharing a reservation?,"No, there is no additional charge for sharing a reservation."
399,Who gets charged when an On-Demand Capacity Reservation is shared across multiple accounts?,"If multiple accounts are consuming an On-Demand Capacity Reservation, each account gets charged for its own instance usage. Unused reserved capacity, if any, gets charged to the account that owns the On-Demand Capacity Reservation. If there is a consolidated billing arrangement among the accounts that share an On-Demand Capacity Reservation, the primary account gets billed for instance usage across all the linked accounts."
400,Can I prioritize access to an On-Demand Capacity Reservation among the AWS accounts that have shared access?,"No. Instance spots in an On-Demand Capacity Reservation are available on a first-come, first-served basis to any account that has shared access."
401,"How can I communicate the AZ of an On-Demand Capacity Reservation with another account, given AZ name mappings could be different across AWS accounts?",You can now use an Availability Zone ID (AZ ID) instead of an AZ name. An AZ ID is a static reference and provides a consistent way of identifying the location of a resource across all your accounts. This makes it easier for you to provision resources centrally in a single account and share them across multiple accounts.
402,Can I stop sharing my On-Demand Capacity Reservation once I have shared it?,"Yes, you can stop sharing a reservation after you have shared it. When you stop sharing an On-Demand Capacity Reservation with specific accounts or stop sharing entirely, other accounts lose the ability to launch new instances into the On-Demand Capacity Reservation. Any capacity occupied by instances running from other accounts will be restored to the On-Demand Capacity Reservation for your use (subject to availability)."
403,Where can I find more information about sharing On-Demand Capacity Reservations?,Refer to Linux or Windows technical documentation to learn about sharing On-Demand Capacity Reservations.
404,Can I get a discount for On-Demand Capacity Reservation usage?,"Yes. Savings Plans or Regional RI discounts apply to On-Demand Capacity Reservations. AWS Billing automatically applies the discount when the attributes of an On-Demand Capacity Reservation match the attributes of a Savings Plan or Regional RI. When an On-Demand Capacity Reservation is used by an instance, you are only charged for the instance (with Savings Plan or RI discounts applied). Discounts are preferentially applied to instance usage before covering unused On-Demand Capacity Reservations."
405,What is a Reserved Instance?,A Reserved Instance (RI) is an EC2 offering that provides you with a significant discount on EC2 usage when you commit to a one-year or three-year term.
406,What are the differences between Standard RIs and Convertible RIs?,"Standard RIs offer a significant discount on EC2 instance usage when you commit to a particular instance family. Convertible RIs offer you the option to change your instance configuration during the term, and still receive a discount on your EC2 usage. For more information on Convertible RIs, please click here."
407,Do RIs provide a capacity reservation?,"Yes, when a Standard or Convertible RI is scoped to a specific Availability Zone (AZ), instance capacity matching the exact RI configuration is reserved for your use (these are referred to as zonal RIs). Zonal RIs give you additional confidence in your ability to launch instances when you need them."
408,When should I purchase a zonal RI?,"If you want to take advantage of the capacity reservation, then you should buy an RI in a specific AZ."
409,When should I purchase a regional RI?,"If you do not require the capacity reservation, then you should buy a regional RI. Regional RIs provide AZ and instance size flexibility, which offer broader applicability of the RI's discounted rate."
410,What are AZ and instance size flexibility?,"AZ and instance size flexibility make it easier for you to take advantage of your regional RI's discounted rate. AZ flexibility applies your RI's discounted rate to usage in any AZ in a Region, while instance size flexibility applies your RI's discounted rate to usage of any size within an instance family. Let's say you own an m5.2xlarge Linux/Unix regional RI with default tenancy in US East (N. Virginia). Then this RI's discounted rate can automatically apply to two m5.xlarge instances in us-east-1a or four m5.large instances in us-east-1b."
411,What types of RIs provide instance size flexibility?,"Linux/Unix regional RIs with the default tenancy provide instance size flexibility. Instance size flexibility is not available on RIs of other platforms such as Windows, Windows with SQL Standard, Windows with SQL Server Enterprise, Windows with SQL Server Web, RHEL, and SLES or G4 instances."
412,Do I need to take any action to take advantage of AZ and instance size flexibility?,Regional RIs do not require any action to take advantage of AZ and instance size flexibility.
413,I own zonal RIs. How do I assign them to a region?,You can assign your Standard zonal RIs to a region by modifying the scope of the RI from a specific AZ to a Region from the EC2 console or by using the ModifyReservedInstances API.
414,How do I purchase an RI?,"To get started, you can purchase an RI from the EC2 console or by using the AWS CLI. Simply specify the instance type, platform, tenancy, term, payment option, and region or AZ."
415,Can I purchase an RI for a running instance?,"Yes, AWS will automatically apply an RI's discounted rate to any applicable instance usage from the time of purchase. Visit the Getting Started page to learn more."
416,Can I control which instances are billed at the discounted rate?,"No. AWS automatically optimizes which instances are charged at the discounted rate to ensure you always pay the lowest amount. For information about billing, and how it applies to RIs, see Billing Benefits and Payment Options."
417,How does instance size flexibility work?,"EC2 uses the scale shown below to compare different sizes within an instance family. In the case of instance size flexibility on RIs, this scale is used to apply the discounted rate of RIs to the normalized usage of the instance family. For example, if you have an m5.2xlarge RI that is scoped to a region, then your discounted rate could apply towards the usage of 1 m5.2xlarge or 2 m5.xlarge instances."
418,Normalization Factor,nano
419,Can I change my RI during its term?,"Yes, you can modify the AZ of the RI, change the scope of the RI from AZ to Region (and the other way around), or modify instance sizes within the same instance family (on the Linux/Unix platform)."
420,Can I change the instance type of my RI during its term?,"Yes. Convertible RIs offer you the option to change the instance type, operating system, tenancy or payment option of your RI during its term. Please refer to the Convertible RI section of the FAQ for additional information."
421,What are the different payment options for RIs?,"You can choose from three payment options when you purchase an RI. With the All Upfront option, you pay for the entire RI term with one upfront payment. With the Partial Upfront option, you make a low upfront payment and are then charged a discounted hourly rate for the instance for the duration of the RI term. The No Upfront option does not require any upfront payment and provides a discounted hourly rate for the duration of the term."
422,When are RIs activated?,"The billing discount and capacity reservation (if applicable) is activated once your payment has successfully been authorized. You can view the status (pending | active | retired) of your RIs on the ""Reserved Instances"" page of the Amazon EC2 console."
423,Do RIs apply to Spot instances or instances running on a Dedicated Host?,"No, RIs do not apply to Spot instances or instances running on Dedicated Hosts. To lower the cost of using Dedicated Hosts, purchase Dedicated Host Reservations."
424,How do RIs work with Consolidated Billing?,"Our system automatically optimizes which instances are charged at the discounted rate to ensure that the consolidated accounts always pay the lowest amount. If you own RIs that apply to an AZ, then only the account which owns the RI will receive the capacity reservation. However, the discount will automatically apply to usage in any account across your consolidated billing family."
425,Can I get a discount on RI purchases?,"Yes, EC2 provides tiered discounts on RI purchases. These discounts are determined based on the total list value (non-discounted price) for the active RIs you have per Region. Your total list value is the sum of all expected payments for an RI within the term, including both the upfront and recurring hourly payments. The tier ranges and corresponding discounts are shown below."
426,Less than $500k,0%
427,$500k-$4M,5%
428,Can you help me understand how volume discounts are applied to my RI purchases?,"Sure. Let's assume that you currently have $400,000 worth of active RIs in the US-east-1 region. Now, if you purchase RIs worth $150,000 in the same region, then the first $100,000 of this purchase would not receive a discount. However, the remaining $50,000 of this purchase would be discounted by 5 percent, so you would only be charged $47,500 for this portion of the purchase over the term based on your payment option."
429,How do I calculate the list value of an RI?,Here is a sample list value calculation for three-year Partial Upfront Reserved Instances:
430,How are volume discounts calculated if I use Consolidated Billing?,"If you leverage Consolidated Billing, AWS will use the aggregate total list price of active RIs across all of your consolidated accounts to determine which volume discount tier to apply. Volume discount tiers are determined at the time of purchase, so you should activate Consolidated Billing prior to purchasing RIs to ensure that you benefit from the largest possible volume discount that your consolidated accounts are eligible to receive."
431,Do Convertible RIs qualify for Volume Discounts?,"No, but the value of each Convertible RI that you purchase contributes to your volume discount tier standing."
432,How do I determine which volume discount tier applies to me?,"To determine your current volume discount tier, please consult the Understanding Reserved Instance Discount Pricing Tiers portion of the Amazon EC2 User Guide."
433,"Will the cost of my RIs change, if my future volume qualifies me for other discount tiers?","No. Volume discounts are determined at the time of purchase, therefore the cost of your RIs will continue to remain the same as you qualify for other discount tiers. Any new purchase will be discounted according to your eligible volume discount tier at the time of purchase."
434,Do I need to take any action at the time of purchase to receive volume discounts?,"No, you will automatically receive volume discounts when you use the existing PurchaseReservedInstance API or EC2 Management Console interface to purchase RIs. If you purchase more than $10M worth of RIs contact us about receiving discounts beyond those that are automatically provided."
435,What is the Reserved Instance (RI) Marketplace?,The RI Marketplace is an online marketplace that provides AWS customers the flexibility to sell their Amazon EC2 RIs to other businesses and organizations. Customers can also browse the RI Marketplace to find an even wider selection of RI term lengths and pricing options sold by other AWS customers.
436,When can I list an RI on the RI Marketplace?,You can list an RI when:
437,Can RIs be transferred?,EC2 Reserved Instances are only transferrable in accordance with the requirements of the RI Marketplace provided in AWS Service Terms and cannot otherwise be transferred.
438,Can I sell any RI on the EC2 RI Marketplace?,"No, AWS prohibits the resale of RIs purchased as part of a discount program per AWS Service Terms. Any All Upfront, Partial Upfront, or No Unfront RIs that were purchased directly from AWS or from EC2 RI Marketplace that received a discount from AWS (for example, RI Volume Discount or other discount programs) are not eligible for sale on the EC2 RI Marketplace."
439,How will I register as a seller for the RI Marketplace?,"To register for the RI Marketplace, you can enter the registration workflow by selling an RI from the EC2 Management Console or setting up your profile from the ""Account Settings"" page on the AWS portal. No matter the route, you will need to complete the following steps:"
440,How will I know when I can start selling on the RI Marketplace?,"You can start selling on the RI Marketplace after you have added a bank account through the registration pipeline. Once activation is complete, you will receive a confirmation email. However, it is important to note that you will not be able to receive disbursements until we are able to receive verification from your bank, which may take up to two weeks, depending on the bank you use."
441,How do I list an RI for sale?,"To list an RI, simply complete these steps in the Amazon EC2 console:"
442,How are listed RIs displayed to buyers?,"RIs (both third-party and those offered by AWS) that have been listed on the RI Marketplace can be viewed in the ""Reserved Instances"" section of the Amazon EC2 console. You can also use the DescribeReservedInstancesListings API call."
443,How much of my RI term can I list?,"You can sell an RI for the term remaining, rounded down to the nearest month. For example, if you had 9 months and 13 days remaining, you will list it for sale as a 9-month-term RI."
444,Can I remove my RI after I've listed it for sale?,"Yes, you can remove your RI listings at any point until a sale is pending (meaning a buyer has bought your RI and confirmation of payment is pending)."
445,Which pricing dimensions can I set for the RIs that I want to list?,"Using the RI Marketplace, you can set an upfront price you'd be willing to accept. You cannot set the hourly price (which will remain the same as was set on the original RI), and you will not receive any funds collected from payments associated with the hourly prices."
446,Can I still use my reservation while it is listed on the RI Marketplace?,"Yes, you will continue to receive the capacity and billing benefit of your reservation until it is sold. Once sold, any running instance that was being charged at the discounted rate will be charged at the On-Demand rate until and unless you purchase a new reservation, or terminate the instance."
447,Can I resell an RI that I purchased from the RI Marketplace?,"Yes, you can resell RIs purchased from the RI Marketplace just like any other RI."
448,Are there any restrictions when selling RIs?,"Yes, you must have a US bank account to sell RIs in the RI Marketplace. Support for non-US bank accounts will be coming soon. Also, you may not sell RIs in the US GovCloud Region."
449,Can I sell RIs purchased from the public volume pricing tiers?,"No, this capability is not yet available."
450,Is there a charge for selling RIs in the RI Marketplace?,"Yes, AWS charges a service fee of 12% of the total upfront price of each RI that you sell in the RI Marketplace."
451,Can AWS sell subsets of my listed RIs?,"Yes, AWS may potentially sell a subset of the quantity of RIs that you have listed. For example, if you list 100 RIs, we may only have a buyer interested in purchasing 50 of them. We will sell those 50 instances and continue to list your remaining 50 RIs until and unless you decide not to list them any longer."
452,How do buyers pay for RIs that they've purchased?,Payment for completed RI sales are done via ACH wire transfers to a US bank account.
453,When will I receive my money?,"Once AWS has received funds from the customer that has bought your reservation, we will disburse funds via wire transfer to the bank account you specified when you registered for the RI Marketplace."
454,"If I sell my RI in the RI Marketplace, will I get refunded for the Premium Support I was charged, too?","No, you will not receive a prorated refund for the upfront portion of the AWS Premium Support Fee."
455,Will I be notified about RI Marketplace activities?,"Yes, you will receive a single email once a day that details your RI Marketplace activity whenever you create or cancel RI listings, buyers purchase your listings, or AWS disburses funds to your bank account."
456,What information is exchanged between the buyer and seller to help with the transaction tax calculation?,"The buyer's city, state, zip+4, and country information will be provided to the seller via a disbursement report. This information will enable sellers to calculate any necessary transaction taxes they need to remit to the government (e.g., sales tax, value-added tax, etc.). The legal entity name of the seller will also be provided on the purchase invoice."
457,Are there any restrictions on the customers when purchasing third-party RIs?,"Yes, you cannot purchase your own listed RIs, including those in any of your linked accounts (via Consolidated Billing)."
458,Do I have to pay for Premium Support when purchasing RIs from the RI Marketplace?,"Yes, if you are a Premium Support customer, you will be charged for Premium Support when you purchase an RI through the RI Marketplace."
459,What is Savings Plans?,"Savings Plans is a flexible pricing model that offers low prices on EC2, Lambda and Fargate usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a one- or three-year term. When you sign up for Savings Plans, you will be charged the discounted Savings Plans price for your usage up to your commitment. For example, if you commit to $10 of compute usage an hour, you will get the Savings Plans prices on that usage up to $10 and any usage beyond the commitment will be charged On Demand rates."
460,What types of Savings Plans does AWS offer?,AWS offers two types of Savings Plans:
461,How do Savings Plans compare to EC2 RIs?,"Savings Plans offers significant savings over On Demand, just like EC2 RIs, but automatically reduce your bills on compute usage across any AWS region, even as usage changes. This provides you the flexibility to use the compute option that best suits your needs and continue to save money, all without having to perform exchanges or modifications."
462,Do Savings Plans provide capacity reservations for EC2 instances?,"No, Savings Plans do not provide a capacity reservation. You can however reserve capacity with On-Demand Capacity Reservations and pay lower prices on them with Savings Plans."
463,How do I get started with Savings Plans?,"You can get started with Savings Plans from AWS Cost Explorer in the AwS Management Console or by using the API/CLI. You can easily make a commitment to a Savings Plan by using the recommendations provided in AWS Cost Explorer, to realize the biggest savings. The recommended hourly commitment is based on your historical On Demand usage and your choice of plan type, term length, and payment option. Once you sign up for a Savings Plan, your compute usage will automatically be charged at the discounted Savings Plan prices and any usage beyond your commitment will be charged at regular On Demand rates."
464,Can I continue to purchase EC2 RIs?,"Yes. You can continue purchasing RIs to maintain compatibility with your existing cost management processes, and your RIs will work alongside Savings Plans to reduce your overall bill. However as your RIs expire we encourage you to sign up for Savings Plans as they offer the same savings as RIs, but with additional flexibility."
465,What is a Spot Instance?,"Spot Instances are spare EC2 capacity that can save you up to 90% off of On-Demand prices that AWS can interrupt with a 2-minute notification. Spot uses the same underlying EC2 instances as On-Demand and Reserved Instances, and is best suited for fault-tolerant, flexible workloads. Spot Instances provides an additional option for obtaining compute capacity and can be used along with On-Demand and Reserved Instances."
466,How is a Spot Instance different than an On-Demand instance or Reserved Instance?,"While running, Spot Instances are exactly the same as On-Demand or Reserved instances. The main differences are that Spot Instances typically offer a significant discount off the On-Demand prices, your instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification, and Spot prices adjust gradually based on long term supply and demand for spare EC2 capacity."
467,How do I purchase and start up a Spot instance?,"Spot instances can be launched using the same tools you use to launch instances today, including AWS Management Console, Auto-Scaling Groups, Run Instances and Spot Fleet. In addition many AWS services support launching Spot instances such as EMR, ECS, Datapipeline, CloudFormation and Batch."
468,How many Spot Instances can I request?,"You can request Spot Instances up to your Spot limit for each region. Note that customers new to AWS might start with a lower limit. To learn more about Spot Instance limits, please refer to the Amazon EC2 User Guide."
469,What price will I pay for a Spot Instance?,"You pay the Spot price that's in effect at the beginning of each instance-hour for your running instance. If Spot price changes after you launch the instance, the new price is charged against the instance usage for the subsequent hour."
470,What is a Spot capacity pool?,"A Spot capacity pool is a set of unused EC2 instances with the same instance type, operating system, and Availability Zone. Each spot capacity pool can have a different price based on supply and demand."
471,What are the best practices to use Spot Instances?,"We highly recommend using multiple Spot capacity pools to maximize the amount of Spot capacity available to you. EC2 provides built-in automation to find the most cost-effective capacity across multiple Spot capacity pools using EC2 Auto Scaling, EC2 Fleet or Spot Fleet. For more information, please see Spot Best Practices."
472,How can I determine the status of my Spot request?,"You can determine the status of your Spot request via Spot Request Status code and message. You can access Spot Request Status information on the Spot Instance page of the EC2 console of the AWS Management Console, API and CLI. For more information, please visit the Amazon EC2 Developer guide."
473,Are Spot Instances available for all instance families and sizes and in all regions?,"Spot Instances are available in all public AWS regions. Spot is available for nearly all EC2 instance families and sizes, including the newest compute-optimized instances, accelerated graphics, and FPGA instance types. A full list of instance types supported in each region is listed here."
474,Which operating systems are available as Spot Instances?,"Linux/UNIX, Windows Server and Red Hat Enterprise Linux (RHEL) are available. Windows Server with SQL Server is not currently available."
475,Can I use a Spot Instance with a paid AMI for third-party software (such as IBM's software packages)?,Not at this time.
476,How will I be charged if my Spot instance is stopped or interrupted?,"If your Spot instance is terminated or stopped by Amazon EC2 in the first instance hour, you will not be charged for that usage. However, if you stop or terminate the Spot instance yourself, you will be charged to the nearest second. If the Spot instance is terminated or stopped by Amazon EC2 in any subsequent hour, you will be charged for your usage to the nearest second. If you are running on Windows or Red Hat Enterprise Linux (RHEL) and you stop or terminate the Spot instance yourself, you will be charged for an entire hour."
477,When would my Spot Instance get interrupted?,"Over the last 3 months, 92% of Spot Instance interruptions were from a customer manually terminating the instance because the application had completed its work."
478,What happens to my Spot instance when it gets interrupted?,"You can choose to have your Spot instances terminated, stopped or hibernated upon interruption. Stop and hibernate options are available for persistent Spot requests and Spot Fleets with the maintain option enabled. By default, your instances are terminated."
479,What is the difference between Stop and Hibernate interruption behaviors?,"In the case of Hibernate, your instance gets hibernated and the RAM data persisted. In the case of Stop, your instance gets shut down and RAM is cleared."
480,What if my EBS root volume is not large enough to store memory state (RAM) for Hibernate?,"You should have sufficient space available on your EBS root volume to write data from memory. If the EBS root volume does not have enough space, hibernation will fail and the instance will get shut down instead. Ensure that your EBS volume is large enough to persist memory data before choosing the hibernate option."
481,What is the benefit if Spot hibernates my instance on interruption?,"With hibernate, Spot instances will pause and resume around any interruptions so your workloads can pick up from exactly where they left off. You can use hibernation when your instance(s) need to retain instance state across shutdown-startup cycles, i.e. when your applications running on Spot depend on contextual, business, or session data stored in RAM."
482,What do I need to do to enable hibernation for my Spot instances?,Refer to Spot Hibernation to learn about enabling hibernation for your Spot instances.
483,Do I have to pay for hibernating my Spot instance?,There is no additional charge for hibernating your instance beyond the EBS storage costs and any other EC2 resources you may be using. You are not charged instance usage fees once your instance is hibernated.
484,Can I resume a hibernated instance?,"No, you will not be able to resume a hibernated instance directly. Hibernate-resume cycle is controlled by Amazon EC2. If an instance is hibernated by Spot, it will be resumed by Amazon EC2 when the capacity becomes available."
485,Which instances and operating systems support hibernation?,"Spot Hibernation is currently supported for Amazon Linux AMIs, Ubuntu and Microsoft Windows operating systems running on any instance type across C3, C4, C5, M4, M5, R3, R4 instances with memory (RAM) size less than 100 GiB."
486,How am I charged if Spot price changes while my instance is running?,"You will pay the price per instance-hour set at the beginning of each instance-hour for the entire hour, billed to the nearest second."
487,Where can I see my usage history for Spot instances and see how much I was billed?,The AWS Management Console makes a detailed billing report available which shows Spot instance start and termination/stop times for all instances. Customers can check the billing report against historical Spot prices via the API to verify that the Spot price they were billed is correct.
488,Are Spot blocks (Fixed Duration Spot instances) ever interrupted?,"Spot blocks are designed not to be interrupted and will run continuously for the duration you select, independent of Spot market price. In rare situations, Spot blocks may be interrupted due to AWS capacity needs. In these cases, we will provide a two-minute warning before we terminate your instance (termination notice), and you will not be charged for the affected instance(s)."
489,What is a Spot fleet?,"A Spot Fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job. You can include the instance types that your application can use. You define a target capacity based on your application needs (in units including instances, vCPUs, memory, storage, or network throughput) and update the target capacity after the fleet is launched. Spot fleets enable you to launch and maintain the target capacity, and to automatically request resources to replace any that are disrupted or manually terminated. Learn more about Spot fleets."
490,Is there any additional charge for making Spot Fleet requests?,"No, there is no additional charge for Spot Fleet requests."
491,What limits apply to a Spot Fleet request?,Visit the Spot Fleet Limits section of the Amazon EC2 User Guide to learn about the limits that apply to your Spot Fleet request.
492,What happens if my Spot Fleet request tries to launch Spot instances but exceeds my regional Spot request limit?,"If your Spot Fleet request exceeds your regional Spot instance request limit, individual Spot instance requests will fail with a ""Spot request limit exceeded request status"". Your Spot Fleet request's history will show any Spot request limit errors that the Fleet request received. Visit the Monitoring Your Spot Fleet section of the Amazon EC2 User Guide to learn how to describe your Spot Fleet request's history."
493,Are Spot fleet requests guaranteed to be fulfilled?,"No. Spot fleet requests allow you to place multiple Spot Instance requests simultaneously, and are subject to the same availability and prices as a single Spot Instance request. For example, if no resources are available for the instance types listed in your Spot Fleet request, we may be unable to fulfill your request partially or in full. We recommend that you to include all the possible instance types and availability zones that are suitable for your workloads in the Spot Fleet."
494,Can I submit a multi-Availability Zone Spot Fleet request?,"Yes, visit the Spot Fleet Examples section of the Amazon EC2 User Guide to learn how to submit a multi-Availability Zone Spot Fleet request."
495,Can I submit a multi-region Spot Fleet request?,"No, we do not support multi-region Fleet requests."
496,How does Spot Fleet allocate resources across the various Spot Instance pools specified in the launch specifications?,"The RequestSpotFleet API provides three allocation strategies: capacity-optimized, lowestPrice and diversified. The capacity-optimized allocation strategy attempts to provision Spot Instances from the most available Spot Instance pools by analyzing capacity metrics. This strategy is a good choice for workloads that have a higher cost of interruption such as big data and analytics, image and media rendering, machine learning, and high performance computing."
497,Can I tag a Spot Fleet request?,You can request to launch Spot Instances with tags via Spot Fleet. The Fleet by itself cannot be tagged.
498,How can I see which Spot fleet owns my Spot Instances?,You can identify the Spot Instances associated with your Spot Fleet by describing your fleet request. Fleet requests are available for 48 hours after all its Spot Instances have been terminated. See the Amazon EC2 User Guide to learn how to describe your Spot Fleet request.
499,Can I modify my Spot Fleet request?,"Yes, you can modify the target capacity of your Spot Fleet request. You may need to cancel the request and submit a new one to change other request configuration parameters."
500,Can I specify a different AMI for each instance type that I want to use?,"Yes, simply specify the AMI you'd like to use in each launch specification you provide in your Spot Fleet request."
501,"Can I use Spot Fleet with Elastic Load Balancing, Auto Scaling, or Elastic MapReduce?","You can use Auto Scaling features with Spot Fleet such as target tracking, health checks, CloudWatch metrics, etc., and can attach instances to your Elastic load balancers (both classic and application load balancers). Elastic MapReduce has a feature named Instance fleets that provides capabilities similar to Spot Fleet."
502,Does a Spot Fleet request terminate Spot Instances when they are no longer running in the lowest priced or capacity-optimized Spot pools and relaunch them?,"No, Spot Fleet requests do not automatically terminate and relaunch instances while they are running. However, if you terminate a Spot Instance, Spot Fleet will replenish it with a new Spot Instance in the new lowest priced pool or capacity-optimized pool based on your allocation strategy."
503,Can I use stop or Hibernation interruption behaviors with Spot Fleet?,"Yes, stop-start and hibernate-resume are supported with Spot Fleet with maintain fleet option enabled."
504,How do I use this service?,The service provides an NTP endpoint at a link-local IP address (169.254.169.123) accessible from any instance running in a VPC. Instructions for configuring NTP clients are available for Linux and Windows.
505,What are the key benefits of using this service?,A consistent and accurate reference time source is crucial for many applications and services. The Amazon Time Sync Service provides a time reference that can be securely accessed from an instance without requiring VPC configuration changes and updates. It is built on Amazon's proven network infrastructure and uses redundant reference time sources to ensure high accuracy and availability.
506,Which instance types are supported for this service?,All instances running in a VPC can access the service.
507,How isolated are Availability Zones from one another?,"Each Availability Zone runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Common points of failures like generators and cooling equipment are not shared across Availability Zones. Additionally, they are physically separate, such that even extremely uncommon disasters such as fires, tornados or flooding would only affect a single Availability Zone."
508,Is Amazon EC2 running in more than one AWS Region?,Yes. Please refer to Regional Products and Services for more details of our product and service availability by Region.
509,How can I make sure that I am in the same Availability Zone as another developer?,"We do not currently support the ability to coordinate launches into the same Availability Zone across AWS developer accounts. One Availability Zone name (for example, us-east-1a) in two AWS customer accounts may relate to different physical Availability Zones."
510,"If I transfer data between Availability Zones using public IP addresses, will I be charged twice for Regional Data Transfer (once because it's across zones, and a second time because I'm using public IP addresses)?","No. Regional Data Transfer rates apply if at least one of the following is true, but you are only charged once for a given instance even if both are true:"
511,What is a Cluster Compute Instance?,Cluster Compute Instances combine high compute resources with high performance networking for HPC applications and other demanding network-bound applications. Cluster Compute Instances provide similar functionality to other Amazon EC2 instances but have been specifically engineered to provide high performance networking.
512,What kind of network performance can I expect when I launch instances in a cluster placement group?,"The bandwidth an EC2 instance can utilize in a cluster placement group depends on the instance type and its networking performance specification. Inter-instance traffic within the same region can utilize 5 Gbps for single-flow and up to 25 Gbps for multiflow traffic. When launched in a placement group, select EC2 instances can utilize up to 10 Gbps for single-flow traffic."
513,What is a Cluster GPU Instance?,"Cluster GPU Instances provide general-purpose graphics processing units (GPUs) with proportionally high CPU and increased network performance for applications benefiting from highly parallelized processing that can be accelerated by GPUs using the CUDA and OpenCL programming models. Common applications include modeling and simulation, rendering and media processing."
514,What is a High Memory Cluster Instance?,"High Memory Cluster Instances provide customers with large amounts of memory and CPU capabilities per instance in addition to high network capabilities. These instance types are ideal for memory intensive workloads including in-memory analytics systems, graph analysis and many science and engineering applications."
515,Does use of Cluster Compute and Cluster GPU Instances differ from other Amazon EC2 instance types?,Cluster Compute and Cluster GPU Instances use differs from other Amazon EC2 instance types in two ways.
516,What is a cluster placement group?,A cluster placement group is a logical entity that enables creating a cluster of instances by launching instances as part of a group. The cluster of instances then provides low latency connectivity between instances in the group. Cluster placement groups are created through the Amazon EC2 API or AWS Management Console.
517,Are all features of Amazon EC2 available for Cluster Compute and Cluster GPU Instances?,"Currently, Amazon DevPay is not available for Cluster Compute or Cluster GPU Instances."
518,Is there a limit on the number of Cluster Compute or Cluster GPU Instances I can use and/or the size of cluster I can create by launching Cluster Compute Instances or Cluster GPU into a cluster placement group?,"There is no limit specific for Cluster Compute Instances. For Cluster GPU Instances, you can launch 2 Instances on your own. If you need more capacity, please complete the Amazon EC2 instance request form (selecting the appropriate primary instance type)."
519,Are there any ways to optimize the likelihood that I receive the full number of instances I request for my cluster via a cluster placement group?,"We recommend that you launch the minimum number of instances required to participate in a cluster in a single launch. For very large clusters, you should launch multiple placement groups, e.g. two placement groups of 128 instances, and combine them to create a larger, 256 instance cluster."
520,Can Cluster GPU and Cluster Compute Instances be launched into a single cluster placement group?,"While it may be possible to launch different cluster instance types into a single placement group, at this time we only support homogenous placement groups."
521,"If an instance in a cluster placement group is stopped then started again, will it maintain its presence in the cluster placement group?","Yes. A stopped instance will be started as part of the cluster placement group it was in when it stopped. If capacity is not available for it to start within its cluster placement group, the start will fail."
522,What CPU options are available on EC2 instances?,"EC2 instances offer a variety of CPU options to help customers balance performance and cost requirements.  Depending on the instance type, EC2 offers a choice in CPU including AWS Graviton/Graviton2 processors (Arm), AMD processors (x86), and Intel processors (x86)."
523,What kind of hardware will my application stack run on?,Visit Amazon EC2 Instance Type for a list of EC2 instances available by region.
524,How does EC2 perform maintenance?,"AWS regularly performs routine hardware, software, power, and network maintenance with minimal disruption across all EC2 instance types. This is achieved by a combination of technologies and methods across the entire AWS Global infrastructure, such as live update and live migration as well as redundant and concurrently maintainable systems. Non-intrusive maintenance technologies such as live update and live migration do not require instances to be stopped or rebooted. Customers are not required to take any action prior to, during or after live migration or live update. These technologies help improve application uptime and reduce your operational effort. Amazon EC2 uses live update to deploy software to servers quickly with minimal impact to customer instances. Live update ensures that customers' workloads run on servers with software that is up-to-date with security patches, new instance features and performance improvements. Amazon EC2 uses live migration when running instances need to be moved from one server to another for hardware maintenance or to optimize placement of instances or to dynamically manage CPU resources. Amazon EC2 has been expanding the scope and coverage of non-intrusive maintenance technologies over the years so that scheduled maintenance events are a fallback option rather than the primary means of enabling routine maintenance."
525,How do I select the right instance type?,"Amazon EC2 instances are grouped into 5 families: General Purpose, Compute Optimized, Memory Optimized, Storage Optimized and Accelerated Computing instances. General Purpose Instances have memory to CPU ratios suitable for most general purpose applications and come with fixed performance or burstable performance; Compute Optimized instances have proportionally more CPU resources than memory (RAM) and are well suited for scale out compute-intensive applications and High Performance Computing (HPC) workloads; Memory Optimized Instances offer larger memory sizes for memory-intensive applications, including database and memory caching applications; Accelerated Computing instances use hardware accelerators, or co-processors, to perform functions such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs; Storage Optimized Instances provide low latency, I/O capacity using SSD-based local instance storage for I/O-intensive applications, as well as dense HDD-storage instances, which provide local high storage density and sequential I/O performance for data warehousing, Hadoop and other data-intensive applications. When choosing instance types, you should consider the characteristics of your application with regards to resource utilization (i.e. CPU, Memory, Storage) and select the optimal instance family and instance size."
526,What is an EC2 Compute Unit and why did you introduce it?,"Transitioning to a utility computing model fundamentally changes how developers have been trained to think about CPU resources. Instead of purchasing or leasing a particular processor to use for several months or years, you are renting capacity by the hour. Because Amazon EC2 is built on commodity hardware, over time there may be several different types of physical hardware underlying EC2 instances. Our goal is to provide a consistent amount of CPU capacity no matter what the actual underlying hardware."
527,How does EC2 ensure consistent performance of instance types over time?,"AWS conducts yearly performance benchmarking of Linux and Windows compute performance on EC2 instance types. Benchmarking results, a test suite that customers can use to conduct independent testing, and guidance on expected performance variance is available under NDA for M,C,R, T and z1d instances; please contact your sales representative to request them."
528,What is the regional availability of Amazon EC2 instance types?,"For a list of all instances and regional availability, visit Amazon EC2 Pricing."
529,How much compute power do Micro instances provide?,"Micro instances provide a small amount of consistent CPU resources and allow you to burst CPU capacity up to 2 ECUs when additional cycles are available. They are well suited for lower throughput applications and web sites that consume significant compute cycles periodically but very little CPU at other times for background processes, daemons, etc. Learn more about using this instance type."
530,How does a Micro instance compare in compute power to a Standard Small instance?,"At steady state, Micro instances receive a fraction of the compute resources that Small instances do. Therefore, if your application has compute-intensive or steady state needs we recommend using a Small instance (or larger, depending on your needs). However, Micro instances can periodically burst up to 2 ECUs (for short periods of time). This is double the number of ECUs available from a Standard Small instance. Therefore, if you have a relatively low throughput application or web site with an occasional need to consume significant compute cycles, we recommend using Micro instances."
531,How can I tell if an application needs more CPU resources than a Micro instance is providing?,The CloudWatch metric for CPU utilization will report 100% utilization if the instance bursts so much that it exceeds its available CPU resources during that CloudWatch monitored minute. CloudWatch reporting 100% CPU utilization is your signal that you should consider scaling manually or via Auto Scaling up to a larger instance type or scale out to multiple Micro instances.
532,Are all features of Amazon EC2 available for Micro instances?,Currently Amazon DevPay is not available for Micro instances.
533,What is the Nitro Hypervisor?,"The launch of C5 instances introduced a new hypervisor for Amazon EC2, the Nitro Hypervisor. As a component of the Nitro system, the Nitro Hypervisor primarily provides CPU and memory isolation for EC2 instances. VPC networking and EBS storage resources are implemented by dedicated hardware components, Nitro Cards that are part of all current generation EC2 instance families. The Nitro Hypervisor is built on core Linux Kernel-based Virtual Machine (KVM) technology, but does not include general-purpose operating system components."
534,How does the Nitro Hypervisor benefit customers?,"The Nitro Hypervisor provides consistent performance and increased compute and memory resources for EC2 virtualized instances by removing host system software components. It allows AWS to offer larger instance sizes (like c5.18xlarge) that provide practically all of the resources from the server to customers. Previously, C3 and C4 instances each eliminated software components by moving VPC and EBS functionality to hardware designed and built by AWS. This hardware enables the Nitro Hypervisor to be very small and uninvolved in data processing tasks for networking and storage."
535,Will all EC2 instances use the Nitro Hypervisor?,"Eventually all new instance types will use the Nitro Hypervisor, but in the near term, some new instance types will use Xen depending on the requirements of the platform."
536,Will AWS continue to invest in its Xen-based hypervisor?,"Yes. As AWS expands its global cloud infrastructure, EC2's use of its Xen-based hypervisor will also continue to grow. Xen will remain a core component of EC2 instances for the foreseeable future. AWS is a founding member of the Xen Project since its establishment as a Linux Foundation Collaborative Project and remains an active participant on its Advisory Board. As AWS expands its global cloud infrastructure, EC2's Xen-based hypervisor also continues to grow. Therefore EC2's investment in Xen continues to grow, not shrink."
537,How many EBS volumes and Elastic Network Interfaces (ENIs) can be attached to instances running on the Nitro Hypervisor?,"Instances running on the Nitro Hypervisor support a maximum of 27 additional PCI devices for EBS volumes and VPC ENIs. Each EBS volume or VPC ENI uses a PCI device. For example, if you attach 3 additional network interfaces to an instance that uses the Nitro Hypervisor, you can attach up to 24 EBS volumes to that instance."
538,Will the Nitro Hypervisor change the APIs used to interact with EC2 instances?,"No, all the public facing APIs for interacting with EC2 instances that run using the Nitro Hypervisor will remain the same. For example, the hypervisor field of the DescribeInstances response will continue to report xen for all EC2 instances, even those running under the Nitro Hypervisor. This field may be removed in a future revision of the EC2 API."
539,Which AMIs are supported on instances that use the Nitro Hypervisor?,"EBS backed HVM AMIs with support for ENA networking and booting from NVMe storage can be used with instances that run under the Nitro Hypervisor. The latest Amazon Linux AMI and Windows AMIs provided by Amazon are supported, as are the latest AMI of Ubuntu, Debian, Red Hat Enterprise Linux, SUSE Enterprise Linux, CentOS, and FreeBSD."
540,Will I notice any difference between instances using Xen hypervisor and those using the Nitro Hypervisor?,"Yes. For example, instances running under the Nitro Hypervisor boot from EBS volumes using an NVMe interface. Instances running under Xen boot from an emulated IDE hard drive, and switch to the Xen paravirtualized block device drivers."
541,How are instance reboot and termination EC2 API requests implemented by the Nitro Hypervisor?,"The Nitro Hypervisor signals the operating system running in the instance that it should shut down cleanly by industry standard ACPI methods. For Linux instances, this requires that acpid be installed and functioning correctly. If acpid is not functioning in the instance, termination events will be delayed by multiple minutes and will then execute as a hard reset or power off."
542,How do EBS volumes behave when accessed by NVMe interfaces?,There are some important differences in how operating system NVMe drivers behave compared to Xen paravirtual (PV) block drivers.
543,What is Optimize CPUs?,"Optimize CPUs gives you greater control of your EC2 instances on two fronts. First, you can specify a custom number of vCPUs when launching new instances to save on vCPU-based licensing costs. Second, you can disable Intel Hyper-Threading Technology (Intel HT Technology) for workloads that perform well with single-threaded CPUs, such as certain HPC applications."
544,Why should I use Optimize CPUs feature?,You should use Optimize CPUs if:
545,How will the CPU optimized instances be priced?,CPU optimized instances will be priced the same as equivalent full-sized instances.
546,How will my application performance change when using Optimize CPUs on EC2?,Your application performance change with Optimize CPUs will be largely dependent on the workloads you are running on EC2. We encourage you to benchmark your application performance with Optimize CPUs to arrive at the right number of vCPUs and optimal hyper-threading behavior for your application.
547,Can I use Optimize CPUs on EC2 Bare Metal instance types (such as i3.metal)?,No. You can use Optimize CPUs with only virtualized EC2 instances.
548,How can I get started with using Optimize CPUs for EC2 Instances?,"For more information on how to get started with Optimize CPUs and supported instance types, please visit the Optimize CPUs documentation page here."
549,How am I billed for my use of Amazon EC2 running IBM?,"You pay only for what you use and there is no minimum fee. Pricing is per instance-hour consumed for each instance type. Partial instance-hours consumed are billed as full hours. Data transfer for Amazon EC2 running IBM is billed and tiered separately from Amazon EC2. There is no Data Transfer charge between two Amazon Web Services within the same Region (for example, between Amazon EC2 US West and another AWS service in the US West). Data transferred between AWS services in different regions will be charged as Internet Data Transfer on both sides of the transfer."
550,Can I use Amazon DevPay with Amazon EC2 running IBM?,"No, you cannot use DevPay to bundle products on top of Amazon EC2 running IBM at this time."
551,Can I use my existing Windows Server license with EC2?,"Yes you can. After you've imported your own Windows Server machine images using the ImportImage tool, you can launch instances from these machine images on EC2 Dedicated Hosts and effectively manage instances and report usage. Microsoft typically requires that you track usage of your licenses against physical resources such as sockets and cores and Dedicated Hosts helps you to do this. Visit the Dedicated Hosts detail page for more information on how to use your own Windows Server licenses on Amazon EC2 Dedicated Hosts."
552,What software licenses can I bring to the Windows environment?,"Specific software license terms vary from vendor to vendor. Therefore, we recommend that you check the licensing terms of your software vendor to determine if your existing licenses are authorized for use in Amazon EC2."
553,What workloads should you run on EC2 Mac instances?,"Amazon EC2 Mac instances are designed to build, test, sign, and publish applications for Apple platforms such as iOS, iPadOS, watchOS, tvOS, macOS, and Safari. Customers such as Pinterest, Intuit, FlipBoard, Twitch, and Goldman Sachs have seen up to 75% better build performance, up to 80% lower build failure rates, and up to 5x the number of parallel builds compared to running macOS on premises."
554,What are EC2 x86 Mac instances?,"x86-based EC2 Mac instances are built on Apple Mac mini computers featuring Intel Core i7 processors and are powered by the AWS Nitro System. They offer customers a choice of macOS Mojave (10.14), macOS Catalina (10.15), macOS Big Sur (11), and macOS Monterey (12) as Amazon Machine Images (AMIs). x86-based EC2 Instances are available in 12 Regions: US East (Ohio, N. Virginia), US West (Oregon), Europe (Stockholm, Frankfurt, Ireland, London), and Asia Pacific (Mumbai, Seoul, Singapore, Sydney, Tokyo). Learn more and get started with x86-based EC2 Mac instances here."
555,How do you release a Dedicated Host?,"The minimum allocation period for an EC2 Mac instance Dedicated Host is 24 hours. After the allocation period has exceeded 24 hours, first stop or terminate the instance running on the host, then release the host using the aws ec2 release-hosts CLI command or the AWS Management Console."
556,Can you share EC2 Mac Dedicated Hosts with other AWS accounts in your organization?,"Yes. You can share EC2 Mac Dedicated Hosts with AWS accounts inside your AWS organization, an organizational unit inside your AWS organization, or your entire AWS organization via AWS Resource Access Manager. For more information, please refer to the AWS Resource Access Manager documentation."
557,How many EC2 Mac instances can you run on an EC2 Mac Dedicated Host?,EC2 Mac instances leverage the full power of the underlying Mac mini hardware. You can run 1 EC2 Mac instance on each EC2 Mac Dedicated Host.
558,Can you update the EFI NVRAM variables on an EC2 Mac instance?,"Yes, you can update certain EFI NVRAM variables on an EC2 Mac instance that will persist across reboots. However, EFI NVRAM variables will be reset if the instance is stopped or terminated. Please see the EC2 Mac instances documentation for more information."
559,Can you use FileVault to encrypt the Amazon Elastic Block Store (Amazon EBS) boot volume on EC2 Mac instances?,"FileVault requires a login before booting into macOS and before remote access can be enabled. If FileVault is enabled, you will lose access to your data on the boot volume at instance reboot, stop, or terminate. We strongly recommend you do not enable FileVault. Instead, we recommend using Amazon EBS encryption for both boot and data EBS volumes on EC2 Mac instances."
560,Can you access to the microphone input or audio output on an EC2 Mac instance?,"There is no access to the microphone input on an EC2 Mac instance. The built-in Apple Remote Desktop VNC server does not support audio output. Third party remote desktop software, such as Teradici CAS, supports remote audio on macOS."
561,What macOS-based Amazon Machine Images (AMIs) are available for EC2 Mac instances?,"EC2 Mac instances use physical Mac mini hardware to run macOS. Apple hardware only supports the macOS version shipped with the hardware (or later). x86-based EC2 Mac instances use the 2018 Intel Core i7 Mac mini, which means macOS Mojave (10.14.x) is as 'far back' as you can go, since the 2018 Mac mini shipped with Mojave. EC2 M1 Mac instances use 2020 M1 Mac mini, which shipped with macOS Big Sur (11.x). EC2 M2 and M2 Pro Mac instances use the 2023 M2 and M2 Pro Mac Minis respectively, which shipped with macOS Ventura (13.2). To see which latest versions of macOS are available as EC2 Mac AMIs, please visit the documentation."
562,How can you run older versions of macOS on EC2 Mac instances?,"EC2 Mac instances are bare metal instances and do not use the Nitro hypervisor. You can install and run a type-2 virtualization layer on x86-based EC2 Mac instances to get access to macOS High Sierra, Sierra, or older macOS versions. On EC2 M1 Mac instances, as macOS Big Sur is the first macOS version to support Apple Silicon, older macOS versions will not run even under virtualization."
563,How can I run beta or preview versions of macOS on EC2 Mac instances?,Installation of beta or preview macOS versions is only available on Apple Silicon-based EC2 Mac Instances . Amazon EC2 doesn't qualify beta or preview macOS versions and doesn't ensure instances will remain functional after an update to a pre-production macOS version.
564,How can you use EC2 user data with EC2 Mac instances?,"As with EC2 Linux and Windows instances, you can pass custom user data to EC2 Mac instances. Instead of using cloud-init, EC2 Mac instances use an open-source launch daemon: ec2-macos-init. You can pass this data into the EC2 Launch Wizard as plain-text, as a file, or as base64-encoded-text."
565,What is the release cadence of macOS AMIs?,We make new macOS AMIs available on a best effort basis. You can subscribe to SNS notifications for updates. We are targeting 30-60 days after a macOS minor version update and 90-120 days after a macOS major version update to release official macOS AMIs.
566,What agents and packages are included in EC2 macOS AMIs?,The following agents and packages are included by default in EC2 macOS AMIs:
567,Can you update the agents and packages included in macOS AMIs?,There is a public GitHub repository of the Homebrew tap for all agents and packages added to the base macOS image. You can use Homebrew to install the latest versions of agents and packages on macOS instances.
568,Can you apply OS and software updates to your Mac instances directly from Apple Update Servers?,"Automatic macOS software updates are disabled on EC2 Mac instances. We recommend using our officially vended macOS AMIs to launch the version of macOS you need. On x86-based and all Apple Silicon EC2 Mac instances, you can update the version of macOS via the Software Update preferences pane, or via the software update CLI command. On both EC2 Mac instances, you can install and update applications and any other user-space software."
569,How do you connect to an EC2 Mac instance over SSH?,"After launching your instance and receiving an instance id, you can use the following command to poll the instance and determine when it is ready for SSH access. Connecting over SSH to EC2 Mac instances follows the same process as connecting to other EC2 instances, such as those running Linux or Windows. To support connecting to your instance using SSH, launch the instance using a key pair and a security group that allows SSH access. Provide the .pem file for the key pair when you connect to the instance. For more information, please see the documentation."
570,How do you connect to an EC2 Mac instance over VNC?,"macOS has built-in Screen Sharing functionality that is disabled by default, but can be enabled and used to connect to a Graphical (Desktop) session of your EC2 Mac instance. For more information on how to enable the built-in Screen Sharing, please see the documentation."
571,How do you connect to an EC2 Mac instance using AWS Systems Manager Session Manager?,"You can connect to your EC2 Mac instances with AWS Systems Manager Session Manager (SSM). Session Manager is a fully managed AWS Systems Manager feature that provides secure and auditable instance management. It removes the need to keep open inbound ports, maintain bastion hosts, or manage SSH keys. The SSM Agent is pre-installed by default on all EC2 macOS AMIs. For more information, please see this blog."
572,How many Amazon EBS volumes and Elastic Network Interfaces (ENIs) are supported by EC2 Mac instances?,"x86-based EC2 Mac instances support 16 EBS volumes and 8 ENI attachments, and EC2 M1 Mac instances support up to 10 EBS volumes and 8 ENI attachments."
573,Do EC2 Mac instances support EBS?,EC2 Mac instances are EBS optimized by default and offer up to 8 Gbps of dedicated EBS bandwidth to both encrypted and unencrypted EBS volumes.
574,Do EC2 Mac instances support booting from local storage?,"EC2 Mac instances can only boot from EBS-backed macOS AMIs. The internal SSD of the Mac mini is present in Disk Utility, but is not bootable."
575,Do EC2 Mac instances support Amazon FSx?,"Yes. EC2 Mac instances support FSx using the SMB protocol. You will need to enroll the EC2 Mac instance into a supported directory service (such as Active Directory or the AWS Directory Service) to enable FSx on EC2 Mac instances. For more information on FSx, visit the product page."
576,Do EC2 Mac instances support Amazon Elastic File System (Amazon EFS)?,"Yes, EC2 Mac instances support EFS over the NFSv4 protocol. For more information on EFS, visit the product page."
577,What is Nitro System support for Older Generation instances?,"The AWS Nitro System now will provide its modern hardware and software components for previous generation EC2 instances to extend the length of service beyond the typical lifetime of the underlying hardware. With the Nitro System support, customers can continue running their workloads and applications on the instance families they were built on."
578,Which previous-generation instances will receive Nitro System support and within what time frame?,"We have enabled Nitro system support for Amazon EC2 C1, M1, M2, C3, M3, R3, C4, and M4 instances. Customers of these instances will receive a maintenance notification of  migration to Nitro System. We will add support for additional instance types in 2023."
579,What actions do I need to take to migrate my existing previous generation instances?,"Customers do not need to take any action for migrating active previous generation instances running on older generation hardware. For instances that are on older generation hardware, each customer account ID mapped to instance(s) will receive an email notification 2 weeks prior to the scheduled maintenance."
580,What will happen if instance is stopped and started before or during the scheduled maintenance window?,Stop/Start of an instance during the scheduled maintenance window will migrate the instance to a new host and the instance will not have to undergo the scheduled maintenance. The stop/start may result in migration of the customer instance to be supported by the AWS Nitro System. Please note that the data on any local instance-store volumes will not be preserved when you stop and start your instance. Click here for more information about stop/start.
581,What will happen to my instance during this maintenance event?,"We will work in conjunction with the customer as a part of our standard AWS maintenance process. Several AWS teams have already migrated and are running previous generation instances on Nitro hardware. During maintenance, the instance will be rebooted which can take up to 30 minutes depending upon the instance size and attributes. For example: Instances with local disk take longer to migrate than instances without local disk. After the reboot, your instance retains its IP address, DNS name, and any data on local instance-store volumes."
582,Do I need to rebuild/recertify workloads to run on previous generation instances migrated to AWS Nitro System?,"No, customers don't need to rebuild/recertify workloads on previous generation instances migrated to AWS Nitro System."
583,Will there be any changes to my instance specifications once migrated to AWS Nitro System?,There will be no change to instance specification of previous generation instances when instances are migrated to AWS Nitro System.
584,Will all features and AMIs on my previous generation instances be supported as a part of this migration?,"Yes, all existing features and AMIs supported on previous generation instances will be supported as we migrate these instances to AWS Nitro System."
585,Will there be changes to pricing and billing when previous generation instances are migrated to AWS Nitro System?,"There will be no change to billing and pricing. We will continue to support the same pricing models we support today for the previous generation instances (On-Demand, 1yr/3yr Reserved Instance, Savings Plan, Spot)."
586,What is Amazon Elastic Kubernetes Service (Amazon EKS)?,Amazon EKS is a managed service that makes it easy for you to run Kubernetes on AWS without installing and operating your own Kubernetes control plane or worker nodes.
587,What is Kubernetes?,"Kubernetes is an open-source container orchestration system allowing you to deploy and manage containerized applications at scale. Kubernetes arranges containers into logical groupings for management and discoverability, then launches them onto clusters of Amazon Elastic Compute Cloud (Amazon EC2) instances. Using Kubernetes, you can run containerized applications including microservices, batch processing workers, and platforms as a service (PaaS) using the same toolset on premises and in the cloud."
588,Why should I use Amazon EKS?,"Amazon EKS provisions and scales the Kubernetes control plane, including the application programming interface (API) servers and backend persistence layer, across multiple AWS Availability Zones (AZs) for high availability and fault tolerance. Amazon EKS automatically detects and replaces unhealthy control plane nodes and patches the control plane. You can run EKS using AWS Fargate, which provides serverless compute for containers. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design."
589,How does Amazon EKS work?,"Amazon EKS works by provisioning (starting) and managing the Kubernetes control plane and worker nodes for you. At a high level, Kubernetes consists of two major components: a cluster of 'worker nodes' running your containers, and the control plane managing when and where containers are started on your cluster while monitoring their status."
590,Which operating systems does Amazon EKS support?,"Amazon EKS supports Kubernetes-compatible Linux x86, ARM, and Windows Server operating system distributions. Amazon EKS provides optimized AMIs for Amazon Linux 2, Bottlerocket, and Windows Server 2019. At this time, there is no Amazon EKS optimized AMI for AL2023. EKS- optimized AMIs for other Linux distributions, such as Ubuntu, are available from their respective vendors."
591,"I have a feature request, who do I tell?",Please let us know what we can add or do better by opening a feature request on the AWS Container Services Public Roadmap
592,Does Amazon EKS work with my existing Kubernetes applications and tools?,"Amazon EKS runs the open-source Kubernetes software, so you can use all the existing plug-ins and tooling from the Kubernetes community. Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises data centers or public clouds. This means that you can easily migrate any standard Kubernetes application to Amazon EKS without any code modifications."
593,Does Amazon EKS work with AWS Fargate?,Yes. You can run Kubernetes applications as serverless containers using AWS Fargate and Amazon EKS.
594,What are Amazon EKS add-ons?,"EKS Add-Ons let you enable and manage Kubernetes operational software, which provides capabilities like observability, scaling, networking, and AWS cloud resource integrations for your EKS clusters. At launch, EKS add-ons supports controlling the launch and version of the AWS VPC CNI plugin through the EKS API."
595,Why should I use Amazon EKS add-ons?,"Amazon EKS add-ons provides one-click installation and management of Kubernetes operational software. Go from cluster creation to running applications in a single command, while easily keeping the operational software required for your cluster up to date. This ensures your Kubernetes clusters are secure and stable and reduces the amount of work needed to start and manage production-ready Kubernetes clusters on AWS."
596,Which Kubernetes versions does Amazon EKS support?,See the Amazon EKS documentation for currently supported Kubernetes versions. Amazon EKS will continue to add support for additional Kubernetes versions in the future.
597,Can I update my Kubernetes cluster to a new version?,"Yes. Amazon EKS performs managed, in-place cluster upgrades for both Kubernetes and Amazon EKS platform versions. This simplifies cluster operations and lets you take advantage of the latest Kubernetes features, as well as the updates to Amazon EKS configuration and security patches."
598,What is an EKS platform version?,"Amazon EKS platform versions represent the capabilities of the cluster control plane, such as which Kubernetes API server flags are enabled, as well as the current Kubernetes patch version. Each Kubernetes minor version has one or more associated Amazon EKS platform versions. The platform versions for different Kubernetes minor versions are independent."
599,Why would I want manual control over Kubernetes version updates?,"New versions of Kubernetes introduce significant change to the Kubernetes API, which can change application behavior. Manual control over Kubernetes cluster versioning lets you test applications against new versions of Kubernetes before upgrading production clusters. Amazon EKS offers the ability to choose when you introduce changes to your EKS cluster."
600,How do I update my worker nodes?,"AWS publishes EKS-optimized Amazon Machine Images (AMIs) that include the necessary worker node binaries (Docker and Kubelet). This AMI is updated regularly and includes the most up-to-date version of these components. You can update your EKS managed nodes to the latest versions of the EKS-optimized AMIs with a single command in the EKS console, API, or CLI."
601,What is Amazon EKS extended support?,"Amazon EKS extended support for Kubernetes versions lets you use a Kubernetes minor version for up to 26 months from the time the version is generally available from Amazon EKS. Amazon EKS versions in extended support receive ongoing security patches for the Kubernetes control plane managed by Amazon EKS. Additionally, Amazon EKS will release critical patches for the Amazon VPC CNI, kube-proxy, and CoreDNS add-ons, AWS-published EKS Optimized Amazon Machine Images (AMIs) for Amazon Linux, Bottlerocket, Windows, and EKS Fargate nodes. AWS backs all Amazon EKS versions in both standard and extended support with full technical support. Extended support for Kubernetes versions is available in all AWS Regions where Amazon EKS is available, including AWS GovCloud (US) Regions. Learn more about the Amazon EKS version support policy in the Amazon EKS documentation."
602,How much does Amazon EKS cost?,"You pay $0.10 per hour for each Amazon EKS cluster you create and for the AWS resources you create to run your Kubernetes worker nodes. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments. Find more information in the  EKS pricing page."
603,Where is Amazon EKS available?,Please visit the AWS global infrastructure region table for the most up-to-date information on Amazon EKS Regional availability.
604,What is AWS Fargate?,"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). AWS Fargate makes it easy to focus on building your applications by eliminating the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design. "
605,Why should I use AWS Fargate?,"AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. AWS Fargate is compatible with both Amazon ECS and Amazon EKS. AWS Fargate makes it easy to scale and manage cloud applications by shifting as much management of the underlying infrastructure resources to AWS so development teams can focus on writing code that solve business problems. Shifting tasks such as server management, resource allocation, and scaling to AWS does not only improve your operational posture, but also accelerates the process of going from idea to production on the cloud and lowers the total cost of ownership (TCO). With multiple CPU architectures and operating systems supported, you can enjoy the serverless benefits of cost, agility and scale across a wide variety of applications. "
606,How does AWS Fargate work with Amazon ECS and Amazon EKS?,"Amazon ECS is a highly scalable, high performance container management service and Amazon EKS is a fully managed Kubernetes service. Both services can schedule containers onto AWS Fargate to automatically scale, load balance, and optimize container availability through managed scheduling, providing an easier way to build and operate containerized applications. "
607,Where does my workload run on AWS Fargate?,"With AWS Fargate each workload runs on its own single use, single tenant compute instance. Each workload is isolated by a virtualization boundary, with each Amazon ECS Task or Kubernetes pod running on a newly provisioned instance. Please see the AWS Fargate Security Whitepaper for more detail on the AWS Fargate architecture. "
608,How should I choose when to use AWS Fargate?,"Choose AWS Fargate for its isolation model and security. You should also select Fargate if you want to launch containers without having to provision or manage Amazon Elastic Compute Cloud (EC2) instances. AWS Fargate has built-in integrations with AWS services and third-party tools that allows you to monitor your applications and gather metrics and logs. And, with AWS Fargate, you pay only for compute resources used, with no upfront expenses. If you require greater control of your Amazon EC2 instances or broader customization options, then use Amazon ECS or Amazon EKS without AWS Fargate. Use Amazon EC2 for GPU workloads, which are not supported on AWS Fargate today. "
609,Can I run my Arm-based applications on AWS Fargate?,"Yes. When using Amazon ECS, AWS Fargate allows you to run your Arm-based applications by using Arm-compatible container images or multi-architecture container images in Amazon Elastic Container Registry (Amazon ECR). You can simply specify the CPU Architecture as Arm64 in your Amazon ECS Task Definition to target AWS Fargate powered by Arm-based AWS Graviton Processors. Use Amazon EC2 to run Arm workloads on Amazon EKS, which are not supported on AWS Fargate today. "
610,Can I run my Amazon ECS Windows containers on AWS Fargate?,Yes. AWS Fargate offers a serverless approach for running your Windows containers. It removes the need to provision and manage servers and lets you specify and pay for resources per application. AWS Fargate provides task-level isolation and handles the necessary patching and updating to help provide a secure compute environment. Please see the Windows platform versions documentation page for supported versions of Windows Server. 
611,Can I use my existing Microsoft Windows License with AWS Fargate?,"Since AWS Fargate is a serverless compute engine, customers do not need to manage the underlying compute instances running in AWS Fargate. Therefore, AWS Fargate will manage the Windows OS licenses for you and the cost of doing so is built into the AWS Fargate pricing. "
612,What are the service quotas for AWS Fargate?,"Service quotas in AWS Fargate are based on the number of vCPU cores used in a given region in a given account. New AWS accounts might have initial lower quotas that can increase over time, and request to raise these soft limits through the standard AWS service quota increase process. For workloads that will require significant scale (10,000s of cores) AWS recommends a multi-account strategy. "
613,What does the AWS Fargate SLA guarantee?,Our Compute SLA guarantees a Monthly Uptime Percentage of at least 99.99% for AWS Fargate. AWS makes two SLA commitments for the Included Containers Services: (1) a Multi-AZ Included Container Service SLA that governs Included Container Services deployed across multiple AZs; and (2) a Single Task/Pod SLA that governs Included Container Service tasks and pods individually. Please see the AWS Fargate and Amazon Elastic Container Service SLA page. 
614,How do I know if I qualify for an SLA Service Credit?,"You are eligible for an AWS Fargate SLA credit under the Compute SLA if more than one Availability Zone in which you are running a task, within the same region, has a Monthly Uptime Percentage of less than 99.99% during any monthly billing cycle. For full details on all of the terms and conditions of the SLA, as well as details on how to submit a claim, please see the Compute SLA details page. "
615,How can I improve the startup time of my container tasks running on AWS Fargate?,"Seekable OCI (SOCI) can help reduce the launch time of Amazon ECS tasks on AWS Fargate. SOCI is a technology open sourced by AWS that enables containers to launch faster by lazily loading the container image. To learn how to get started with SOCI, please visit the documentation and the blog post. Reducing AWS Fargate startup times with zstd compressed container images. The layers of a container image are compressed for efficiency, by default using the gzip format. However, containerd supports an alternative format known as zstd that has been shown to decompress more quickly, resulting in faster task launch times when using AWS Fargate. Please see this blogpost for more details on how to build container images with zstd. "
616,How do I size workloads for AWS Fargate?,"It is recommended to load test applications locally, or in development environments, to understand the requirements of the application to size the request appropriately. AWS Compute Optimizer can be used to provide recommendations if a workload is under or over sized. "
617,How do I get network traffic in and out of my workload on AWS Fargate?,"On AWS Fargate, each ECS Task or Kubernetes Pod is given a dedicated elastic network interface (ENI) attached into your virtual private cloud (VPC). All traffic in and out of the containerized workload goes through this ENI. Therefore VPC Security Groups and VPC network ACLs can be used to secure the ENI, and VPC Flow Logs can be used to monitor traffic flows. "
618,Can I run stateful workloads on AWS Fargate?,"Each workload that runs on AWS Fargate is given full access to 20 GiB of ephemeral storage to use as temporary storage while the workload is running. Once the workload has stopped, any data stored in this 20 GiB volume is erased. This ephemeral storage volume can be expanded up to 200GiB on Amazon ECS and 175 GiB on Amazon EKS. Amazon Elastic File System (EFS) an be used to provide persistent storage to workloads running on AWS Fargate. "
619,Can I build container images within AWS Fargate?,"Common approaches to building containers from within containers often require privileged mode (such as with Docker in Docker), which is unavailable in AWS Fargate's security model, or mounting of the docker socket into a container, which is unavailable due to AWS Fargate utilizing containerd as of Platform Version 1.4. Alternatively, rootless image build project such as Kaniko can be deployed within AWS Fargate's security model and are a viable option for building container images. "
620,Can AWS Fargate be used in solutions that must conform to a specific compliance programs?,"AWS Fargate meets the standards for a wide array of compliance programs, including PCI DSS, SOC, FIPS 140-2, FedRAMP, and HIPAA. For more information and a complete list of programs, please see the AWS Cloud Security Services in Scope documentation. "
621,Can I use AWS Fargate for Protected Health Information (PHI) and other HIPAA regulated workloads?,"Yes. AWS Fargate is HIPAA-eligible. If you have an executed Business Associate Addendum (BAA) with AWS, you can process encrypted Protected Health Information (PHI) using containers deployed onto AWS Fargate. For more information, please visit our page on HIPAA compliance. If you plan to process, store, or transmit PHI and do not have an executed BAA from AWS, please contact us for more information. "
622,Can I use AWS Fargate for US Government-regulated workloads or for processing sensitive Controlled Unclassified Information (CUI)?,"Yes. AWS Fargate is available in AWS GovCloud (US) Regions. AWS GovCloud (US) is Amazon's isolated cloud infrastructure and services designed to address specific regulatory and compliance requirements of US Government agencies, as well as contractors, educational institutions, and other US customers that run sensitive workloads in the cloud. For a full list of AWS Regions where AWS Fargate is available, please visit our Region table or documentation. "
623,How can AWS Fargate be extended through integrations with other systems?,"AWS Fargate offers a flexible integration model inclusive of both first party AWS services and third party Amazon Partner Network (APN) solutions. The common integration mechanism is to run a sidecar container within a AWS Fargate task that can interact with the primary application container, for example a runtime security agent or log router that interacts with the primary application and then ships data to a centralized system for analysis and review. "
624,What options do I have for monitoring AWS Fargate workloads?,"AWS provides several tools for monitoring and responding to various facets of your AWS Fargate resources, including Amazon CloudWatch Alarms, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CloudTrail Logs, AWS Trusted Advisor, and AWS Compute Optimizer. A popular approach is to utilize CloudWatch Container Insights to collect and analyze logs and view operational dashboards. Application logging has built-in log drivers for CloudWatch and Splunk, but FireLens for Amazon ECS can be used via task definition parameters to route logs to an AWS service or AWS Partner Network (APN) destination. "
625,What is the pricing of AWS Fargate?,"With AWS Fargate, you pay only for the amount of vCPU, memory, and storage resources provisioned by your containerized applications. vCPU and memory resources are calculated from the time your container images are pulled until the Amazon ECS task or EKS pod terminates, rounded up to the nearest second. A minimum charge of 1 minute applies. 20 GB of ephemeral storage is available for all AWS Fargate Tasks and Pods by default you only pay for any additional storage that you configure. AWS Fargate supports Spot and Compute Savings Plan pricing options just like with Amazon EC2 instances. You can find additional details on the pricing page. "
626,How do I optimize my spend on AWS Fargate?,"AWS offers Spot instances for AWS Fargate tasks, which utilizes spare compute capacity that is available at a lower price than on-demand instances. By using spot instances you can run interruption tolerant Amazon ECS Tasks at up to a 70 percent discount off the AWS Fargate price.  AWS introduced AWS Fargate Savings Plans, a discount model that provides you with the same discounts as Reserved Instances, in exchange for a commitment to use a specific amount (measured in dollars per hour) of compute power over a one- or three-year period.  AWS Graviton processors are designed by AWS to deliver the best price performance for your cloud workloads running in Amazon EC2, AWS managed containers, and other managed services. AWS Graviton provides up to 40 percent better price performance over comparable x86-based instances. AWS Graviton processors are more energy efficient and use up to 60 percent less energy for the same performance than comparable EC2 instances.  AWS Fargate is included in the AWS Compute Optimizer, which allows you to easily identify and remediate inefficient configurations. "
627,Why should I use AWS Fargate powered by Graviton processors?,"AWS Graviton processors are custom built by Amazon Web Services cores to deliver the best price performance for your cloud workloads. AWS Fargate powered by AWS Graviton processors delivers up to 40% improved price/performance at 20% lower cost over comparable Intel x86-based Fargate for a variety of workloads such as application servers, web services, high-performance computing, and media processing. You get the same serverless benefits of AWS Fargate while optimizing performance and cost for running your containerized workloads. "
628,What is AWS Identity and Access Management (IAM)?,"IAM provides fine-grained access control across all of AWS. With IAM, you can control access to services and resources under specific conditions. Use IAM policies to manage permissions for your workforce and systems to ensure least privilege. IAM is offered at no additional charge. For more information, see What is IAM?"
629,How does IAM work and what can I do with it?,"IAM provides authentication and authorization for AWS services. A service evaluates if an AWS request is allowed or denied. Access is denied by default and is allowed only when a policy explicitly grants access. You can attach policies to roles and resources to control access across AWS. For more information, see Understanding how IAM works."
630,What are least-privilege permissions?,"When you set permissions with IAM policies, grant only the permissions required to perform a task. This practice is known as granting least privilege. You can apply least-privilege permissions in IAM by defining the actions that can be taken on specific resources under specific conditions. For more information, see Access management for AWS resources."
631,How do I get started with IAM?,"To get started using IAM to manage permissions for AWS services and resources, create an IAM role and grant it permissions. For workforce users, create a role that can be assumed by your identity provider. For systems, create a role that can be assumed by the service you are using, such as Amazon EC2 or AWS Lambda. After you create a role, you can attach a policy to the role to grant permissions that meet your needs. When you are just starting out, you might not know the specific permissions you need, so you can start with broader permissions. AWS managed policies provide permissions to help you get started and are available in all AWS accounts. Then, reduce permissions further by defining customer managed policies specific to your use cases. You can create and manage policies and roles in the IAM console, or via AWS APIs or the AWS CLI. For more information, see Getting started with IAM."
632,What are IAM roles and how do they work?,"AWS Identity and Access Management (IAM) roles provide a way to access AWS by relying on temporary security credentials. Each role has a set of permissions for making AWS service requests, and a role is not associated with a specific user or group. Instead, trusted entities such as identity providers or AWS services assume roles. For more information, see IAM roles."
633,Why should I use IAM roles?,"You should use IAM roles to grant access to your AWS accounts by relying on short-term credentials, a security best practice. Authorized identities, which can be AWS services or users from your identity provider, can assume roles to make AWS requests. To grant permissions to a role, attach an IAM policy to it. For more information, see Common scenarios for roles."
634,What are IAM users and should I still be using them?,"IAM users are identities with long-term credentials. You might be using IAM users for workforce users. In this case, AWS recommends using an identity provider and federating into AWS by assuming roles. You also can use roles to grant cross-account access to services and features such as AWS Lambda functions. In some scenarios, you might require IAM users with access keys that have long-term credentials with access to your AWS account. For these scenarios, AWS recommends using IAM access last used information to rotate credentials often and remove credentials that are not being used. For more information, see Overview of AWS identity management: Users."
635,What are IAM policies?,"IAM policies define permissions for the entities you attach them to. For example, to grant access to an IAM role, attach a policy to the role. The permissions defined in the policy determine whether requests are allowed or denied. You also can attach policies to some resources, such as Amazon S3 buckets, to grant direct, cross-account access. And you can attach policies to an AWS organization or organizational unit to restrict access across multiple accounts. AWS evaluates these policies when an IAM role makes a request. For more information, see Identity-based policies."
636,How do I grant access to services and resources by using IAM?,"To grant access to services and resources by using AWS Identity and Access Management (IAM), attach IAM policies to roles or resources. You can start by attaching AWS managed policies, which are owned and updated by AWS and are available in all AWS accounts. If you know the specific permissions required for your use cases, you can create customer managed policies and attach them to roles. Some AWS resources provide a way to grant access by defining a policy attached to resources, such as Amazon S3 buckets. These resource-based policies allow you to grant direct, cross-account access to the resources they are attached to. For more information, see Access management for AWS resources."
637,How do I create IAM policies?,"To assign permissions to a role or resource, create a policy, which is a JavaScript Object Notation (JSON) document that defines permissions. This document includes permissions statements that grant or deny access to specific service actions, resources, and conditions. After you create a policy, you can attach it to one or more AWS roles to grant permissions to your AWS account. To grant direct, cross-account access to resources, such as Amazon S3 buckets, use resource-based policies. Create your policies in the IAM console or via AWS APIs or the AWS CLI. For more information, see Creating IAM policies."
638,What are AWS managed policies and when should I use them?,"AWS managed policies are created and administered by AWS and cover common use cases. Getting started, you can grant broader permissions by using the AWS managed policies that are available in your AWS account and common across all AWS accounts. Then, as you refine your requirements, you can reduce permissions by defining customer managed policies specific to your use cases with the goal of achieving least-privilege permissions. For more information, see AWS managed policies."
639,What are customer managed policies and when should I use them?,"To grant only the permissions required to perform tasks, you can create customer managed policies that are specific to your use cases and resources. Use customer managed policies to continue refining permissions for your specific requirements. For more information, see Customer managed policies."
640,What are inline policies and when should I use them?,"Inline policies are embedded in and inherent to specific IAM roles. Use inline policies if you want to maintain a strict one-to-one relationship between a policy and the identity to which it is applied. For example, you can grant administrative permissions to ensure they are not attached to other roles. For more information, see Inline policies."
641,What are resource-based policies and when should I use them?,"Resource-based policies are permissions policies that are attached to resources. For example, you can attach resource-based policies to Amazon S3 buckets, Amazon SQS queues, VPC endpoints, and AWS Key Management Service encryption keys. For a list of services that support resource-based policies, see AWS services that work with IAM. Use resource-based policies to grant direct, cross-account access. With resource-based policies, you can define who has access to a resource and which actions they can perform with it. For more information, see Identity-based policies and resource-based policies."
642,What is role-based access control (RBAC)?,"RBAC provides a way for you to assign permissions based on a person's job function, known outside of AWS as a role. IAM provides RBAC by defining IAM roles with permissions that align with job functions. You then can grant individuals access to assume these roles to perform specific job functions. With RBAC, you can audit access by looking at each IAM role and its attached permissions. For more information, see Comparing ABAC to the traditional RBAC model."
643,How do I grant access with RBAC?,"As a best practice, grant access only to the specific service actions and resources required to perform each task. This is known as granting least privilege. When employees add new resources, you must update policies to allow access to those resources."
644,What is attribute-based access control (ABAC)?,"ABAC is an authorization strategy that defines permissions based on attributes. In AWS, these attributes are called tags, and you can define them on AWS resources, IAM roles, and in role sessions. With ABAC, you define a set of permissions based on the value of a tag. You can grant fine-grained permissions to specific resources by requiring the tags on the role or session to match the tags on the resource. For example, you can author a policy that grants developers access to resources tagged with the job title developers. ABAC is helpful in environments that are growing rapidly by granting permissions to resources as they are created with specific tags. For more information, see Attribute-Based Access Control for AWS."
645,How do I grant access by using ABAC?,"To grant access by using ABAC, first define the tag keys and values you want to use for access control. Then, ensure your IAM role has the appropriate tag keys and values. If multiple identities use this role, you also can define session tag keys and values. Next, ensure that your resources have the appropriate tag keys and values. You also can require users to create resources with appropriate tags and restrict access to modify them. After your tags are in place, define a policy that grants access to specific actions and resource types, but only if the role or session tags match the resource tags. For a detailed tutorial that demonstrates how to use ABAC in AWS, see IAM tutorial: Define permissions to access AWS resources based on tags."
646,How do I restrict access by using IAM?,"With AWS Identity and Access Management (IAM), all access is denied by default and requires a policy that grants access. As you manage permissions at scale, you might want to implement permissions guardrails and restrict access across your accounts. To restrict access, specify a Deny statement in any policy. If a Deny statement applies to an access request, it always prevails over an Allow statement. For example, if you allow access to all actions in AWS but deny access to IAM, any request to IAM is denied. You can include a Deny statement in any type of policy, including identity-based, resource-based, and service control policies with AWS Organizations. For more information, see Controlling access with AWS Identity and Access Management."
647,What are AWS Organizations service control policies (SCPs) and when should I use them?,"SCPs are similar to IAM policies and use almost the same syntax. However, SCPs don't grant permissions. Instead, SCPs allow or deny access to AWS services for individual AWS accounts with Organizations member accounts, or for groups of accounts within an organizational unit. The specified actions from an SCP affect all IAM users and roles, including the root user of the member account. For more information, see Policy evaluation logic."
648,How do I work toward least-privilege permissions?,"When you get started granting permissions, you can start with broader permissions as you explore and experiment. As your use cases mature, AWS recommends that you refine permissions to grant only the permissions required with the goal of achieving least-privilege permissions. AWS provides tools to help you refine your permissions. You can start with AWS managed policies, which are created and administered by AWS and include permissions for common use cases. As you refine your permissions, define specific permissions in customer managed policies. To help you determine the specific permissions you require, use AWS Identity and Access Management (IAM) Access Analyzer, review AWS CloudTrail logs, and inspect last access information. You also can use the IAM policy simulator to test and troubleshoot policies."
649,What is IAM Access Analyzer?,"Achieving least privilege is a continuous cycle to grant the right fine-grained permissions as your requirements evolve. IAM Access Analyzer helps you streamline permissions management in each step of this cycle. Policy generation with IAM Access Analyzer generates a fine-grained policy based on the access activity captured in your logs. This means that after you build and run an application, you can generate policies that grant only the required permissions to operate the application. Policy validation with IAM Access Analyzer uses more than 100 policy checks to guide you to author and validate secure and functional policies. You can use these checks while creating new policies or to validate existing policies. Custom policy checks are a paid feature to validate that developer-authored policies adhere to your specified security standards ahead of deployments. Custom policy checks use the power of automated reasoning provable security assurance backed by mathematical proof  to enable security teams to proactively detect nonconformant updates to policies. Public and cross-account findings with IAM Access Analyzer help you verify and refine access allowed by your resource policies from outside your AWS organization or account. For more information, see Using IAM Access Analyzer. Unused access with IAM Access Analyzer continuously analyzes your accounts to identify unused access and creates a centralized dashboard with findings. The findings highlight unused roles, unused access keys for IAM users, and unused passwords for IAM users. For active IAM roles and users, the findings provide visibility into unused services and actions."
650,How do I remove unused permissions?,"You might have IAM users, roles, and permissions that you no longer require in your AWS account. We recommend that you remove them with the goal of achieving least-privilege access. For IAM users, you can review password and access key last used information. For roles, you can review role last used information. This information is available through the IAM console, APIs, and SDKs. Last used information helps you identify users and roles that are no longer in use and safe to remove. You also can refine permissions by reviewing service and last accessed information to identify unused permissions. For more information, see Refining permissions in AWS using last accessed information. If you enable the unused access analyzer as a paid feature, IAM Access Analyzer continuously analyzes your accounts to identify unused access and creates a centralized dashboard with findings. The dashboard helps security teams review findings centrally and prioritize accounts based on the volume of findings. Security teams can use the dashboard to review findings centrally and prioritize which accounts to review based on the volume of findings. The findings highlight unused roles, unused access keys for IAM users, and unused passwords for IAM users. For active IAM roles and users, the findings provide visibility into unused services and actions. that simplifies inspecting unused access to guide you toward least privilege. With this feature, you pay per IAM role or IAM user analyzed per month."
651,What is the IAM policy simulator and when should I use it?,"The IAM policy simulator evaluates policies you choose and determines the effective permissions for each of the actions you specify. Use the policy simulator to test and troubleshoot identity-based and resource-based policies, IAM permissions boundaries, and SCPs. For more information, see Testing IAM policies with the IAM policy simulator."
652,What are IAM Access Analyzer custom policy checks?,"IAM Access Analyzer custom policy checks validate that IAM policies adhere to your security standards ahead of deployments. Custom policy checks use the power of automated reasoning provable security assurance backed by mathematical proof  to enable security teams to proactively detect nonconformant updates to policies. For example, IAM policy changes that are more permissive than their previous version. Security teams can use these checks to streamline their reviews, automatically approving policies that conform with their security standards, and inspecting more deeply when they don't. This new kind of validation provides higher security assurance in the cloud. Security and development teams can automate policy reviews at scale by integrating these custom policy checks into the tools and environments where developers author their policies, such as their CI/CD pipelines."
653,What is IAM Access Analyzer unused access?,"IAM Access Analyzer simplifies inspecting unused access to guide you toward least privilege. Security teams can use IAM Access Analyzer to gain visibility into unused access across their AWS organization and automate how they rightsize permissions. When the unused access analyzer is enabled, IAM Access Analyzer continuously analyzes your accounts to identify unused access and creates a centralized dashboard with findings. The dashboard helps security teams review findings centrally and prioritize accounts based on the volume of findings. Security teams can use the dashboard to review findings centrally and prioritize which accounts to review based on the volume of findings. The findings highlight unused roles, unused access keys for IAM users, and unused passwords for IAM users. For active IAM roles and users, the findings provide visibility into unused services and actions."
654,What is AWS Lambda?,"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume - there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. "
655,What is serverless computing?,"Serverless computing allows you to build and run applications and services without thinking about servers. With serverless computing, your application still runs on servers, but all the server management is done by AWS. At the core of serverless computing is AWS Lambda, which lets you run your code without provisioning or managing servers. "
656,What events can trigger an AWS Lambda function?,Please see our documentation for a complete list of event sources. 
657,When should I use AWS Lambda versus Amazon EC2?,"Amazon Web Services offers a set of compute services to meet a range of needs. Amazon EC2 offers flexibility, with a wide range of instance types and the option to customize the operating system, network and security settings, and the entire software stack, allowing you to easily move existing applications to the cloud. With Amazon EC2 you are responsible for provisioning capacity, monitoring fleet health and performance, and designing for fault tolerance and scalability. AWS Elastic Beanstalk offers an easy-to-use service for deploying and scaling web applications in which you retain ownership and full control over the underlying EC2 instances. Amazon EC2 Container Service is a scalable management service that supports Docker containers and allows you to easily run distributed applications on a managed cluster of Amazon EC2 instances. AWS Lambda makes it easy to execute code in response to events, such as changes to Amazon S3 buckets, updates to an Amazon DynamoDB table, or custom events generated by your applications or devices. With Lambda, you do not have to provision your own instances; Lambda performs all the operational and administrative activities on your behalf, including capacity provisioning, monitoring fleet health, applying security patches to the underlying compute resources, deploying your code, running a web service front end, and monitoring and logging your code. AWS Lambda provides easy scaling and high availability to your code without additional effort on your part. "
658,What kind of code can run on AWS Lambda?,"AWS Lambda offers an easy way to accomplish many activities in the cloud. For example, you can use AWS Lambda to build mobile back-ends that retrieve and transform data from Amazon DynamoDB, handlers that compress or transform objects as they are uploaded to Amazon S3, auditing and reporting of API calls made to any Amazon Web Service, and server-less processing of streaming data using Amazon Kinesis. "
659,What languages does AWS Lambda support?,"AWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API which allows you to use any additional programming languages to author your functions. Please read our documentation on using Node.js, Python, Java, Ruby, C#, Go, and PowerShell. "
660,Can I access the infrastructure that AWS Lambda runs on?,"No. AWS Lambda operates the compute infrastructure on your behalf, allowing it to perform health checks, apply security patches, and do other routine maintenance. "
661,How does AWS Lambda isolate my code?,"Each AWS Lambda function runs in its own isolated environment, with its own resources and file system view. AWS Lambda uses the same techniques as Amazon EC2 to provide security and separation at the infrastructure and execution levels. "
662,How does AWS Lambda secure my code?,AWS Lambda stores code in Amazon S3 and encrypts it at rest. AWS Lambda performs additional integrity checks while your code is in use. 
663,What AWS regions are available for AWS Lambda?,Please refer to the AWS Global Infrastructure Region Table. 
664,What is an AWS Lambda function?,"The code you run on AWS Lambda is uploaded as a Lambda function. Each function has associated configuration information, such as its name, description, entry point, and resource requirements. The code must be written in a stateless style i.e. it should assume there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, Amazon EFS, or another Internet-available storage service. Lambda functions can include libraries, even native ones. "
665,Will AWS Lambda reuse function instances?,"To improve performance, AWS Lambda may choose to retain an instance of your function and reuse it to serve a subsequent request, rather than creating a new copy. To learn more about how Lambda reuses function instances, visit our documentation. Your code should not assume that this will always happen. "
666,What if I need scratch space on disk for my AWS Lambda function?,"You can configure each Lambda function with its own ephemeral storage between 512MB and 10,240MB, in 1MB increments. The ephemeral storage is available in each function's /tmp directory. Each function has access to 512MB of storage at no additional cost. When configuring your functions with more than 512MB of ephemeral storage, you will be charged based on the amount of storage you configure, and how long your function runs, metered in 1ms increments. For comparison, in the US East (Ohio) region, the AWS Fargate ephemeral storage price is $0.000111 per GB-hour, or $0.08 per GB-month. Amazon EBS gp3 storage volume pricing in US East (Ohio) is $0.08 per GB-month. AWS Lambda ephemeral storage pricing is $0.0000000309 per GB-second, or $0.000111 per GB-hour and $0.08 per GB-month. To learn more, see AWS Lambda Pricing. "
667,How do I configure my application to use AWS Lambda ephemeral storage?,"You can configure each Lambda function with its own ephemeral storage between 512MB and 10,240MB, in 1MB increments by using the AWS Lambda console, AWS Lambda API, or AWS CloudFormation template during function creation or update. "
668,Is AWS Lambda ephemeral storage encrypted?,Yes. All data stored in ephemeral storage is encrypted at rest with a key managed by AWS. 
669,What metrics can I use to monitor my AWS Lambda ephemeral storage usage?,"You can use AWS CloudWatch Lambda Insight metrics to monitor your ephemeral storage usage. To learn more, see the AWS CloudWatch Lambda Insights documentation. "
670,"When should I use Amazon S3, Amazon EFS, or AWS Lambda ephemeral storage for my serverless applications?","If your application needs durable, persistent storage, consider using Amazon S3 or Amazon EFS. If your application requires storing data needed by code in a single function invocation, consider using AWS Lambda ephemeral storage as a transient cache. To learn more, please see Choosing between AWS Lambda data storage options in web apps. "
671,Can I use ephemeral storage while Provisioned Concurrency is enabled for my function?,"Yes. However, if you application needs persistent storage, consider using Amazon EFS or Amazon S3. When you enable Provisioned Concurrency for your function, your function's initialization code runs during allocation and every few hours, as running instances of your function are recycled. You can see the initialization time in logs and traces after an instance processes a request. However, initialization is billed even if the instance never processes a request. This Provisioned Concurrency initialization behavior may affect how your function interacts with data you store in ephemeral storage, even when your function isn't processing requests. To learn more about Provisioned Concurrency, please see the relevant documentation. "
672,How do I configure my application to use AWS Lambda ephemeral storage?,"You can configure each Lambda function with its own ephemeral storage between 512MB and 10,240MB, in 1MB increments by using the AWS Lambda console, AWS Lambda API, or AWS CloudFormation template during function creation or update. "
673,Is AWS Lambda ephemeral storage encrypted?,Yes. All data stored in ephemeral storage is encrypted at rest with a key managed by AWS. 
674,What metrics can I use to monitor my AWS Lambda ephemeral storage usage?,"You can use AWS CloudWatch Lambda Insight metrics to monitor your ephemeral storage usage. To learn more, see the AWS CloudWatch Lambda Insights documentation. "
675,Why must AWS Lambda functions be stateless?,"Keeping functions stateless enables AWS Lambda to rapidly launch as many copies of the function as needed to scale to the rate of incoming events. While AWS Lambda's programming model is stateless, your code can access stateful data by calling other web services, such as Amazon S3 or Amazon DynamoDB. "
676,Can I use threads and processes in my AWS Lambda function code?,"Yes. AWS Lambda allows you to use normal language and operating system features, such as creating additional threads and processes. Resources allocated to the Lambda function, including memory, execution time, disk, and network use, must be shared among all the threads/processes it uses. You can launch processes using any language supported by Amazon Linux. "
677,What restrictions apply to AWS Lambda function code?,"Lambda attempts to impose as few restrictions as possible on normal language and operating system activities, but there are a few activities that are disabled: Inbound network connections are blocked by AWS Lambda, and for outbound connections, only TCP/IP and UDP/IP sockets are supported, and ptrace (debugging) system calls are blocked. TCP port 25 traffic is also blocked as an anti-spam measure. "
678,How do I create an AWS Lambda function using the Lambda console?,"If you are using Node.js or Python, you can author the code for your function using code editor in the AWS Lambda console, which lets you author and test your functions, and view the results of function executions in a robust, IDE-like environment. Go to the console to get started. You can also package the code (and any dependent libraries) as a ZIP and upload it using the AWS Lambda console from your local environment or specify an Amazon S3 location where the ZIP file is located. Uploads must be no larger than 50MB (compressed). You can use the AWS Eclipse plugin to author and deploy Lambda functions in Java. You can use the Visual Studio plugin to author and deploy Lambda functions in C#, and Node.js. "
679,How do I create an AWS Lambda function using the Lambda CLI?,"You can package the code (and any dependent libraries) as a ZIP and upload it using the AWS CLI from your local environment, or specify an Amazon S3 location where the ZIP file is located. Uploads must be no larger than 50MB (compressed). Visit the Lambda Getting Started guide to get started. "
680,Does AWS Lambda support environment variables?,"Yes. You can easily create and modify environment variables from the AWS Lambda Console, CLI, or SDKs. To learn more about environment variables, see the documentation. "
681,Can I store sensitive information in environment variables?,"For sensitive information, such as database passwords, we recommend you use client-side encryption using AWS Key Management Service and store the resulting values as ciphertext in your environment variable. You will need to include logic in your AWS Lambda function code to decrypt these values. "
682,How can I manage my AWS Lambda functions?,How can I manage my AWS Lambda functions? 
683,Can I share code across functions?,"Yes, you can package any code (frameworks, SDKs, libraries, and more) as a Lambda Layer and manage and share them easily across multiple functions. "
684,How do I monitor an AWS Lambda function?,"AWS Lambda automatically monitors Lambda functions on your behalf, reporting real-time metrics through Amazon CloudWatch, including total requests, account-level and function-level concurrency usage, latency, error rates, and throttled requests. You can view statistics for each of your Lambda functions via the Amazon CloudWatch console or through the AWS Lambda console. You can also call third-party monitoring APIs in your Lambda function.  Visit Troubleshooting CloudWatch metrics to learn more. Standard charges for AWS Lambda apply to use Lambda's built-in metrics. "
685,How do I troubleshoot failures in an AWS Lambda function?,"AWS Lambda automatically integrates with Amazon CloudWatch logs, creating a log group for each Lambda function and providing basic application lifecycle event log entries, including logging the resources consumed for each use of that function. You can easily insert additional logging statements into your code. You can also call third-party logging APIs in your Lambda function. Visit Troubleshooting Lambda functions to learn more. Amazon CloudWatch Logs rates will apply. "
686,How do I scale an AWS Lambda function?,"You do not have to scale your Lambda functions AWS Lambda scales them automatically on your behalf. Every time an event notification is received for your function, AWS Lambda quickly locates free capacity within its compute fleet and runs your code. Since your code is stateless, AWS Lambda can start as many copies of your function as needed without lengthy deployment and configuration delays. There are no fundamental limits to scaling a function. AWS Lambda will dynamically allocate capacity to match the rate of incoming events. "
687,How are compute resources assigned to an AWS Lambda function?,"In the AWS Lambda resource model, you choose the amount of memory you want for your function, and are allocated proportional CPU power and other resources. For example, choosing 256MB of memory allocates approximately twice as much CPU power to your Lambda function as requesting 128MB of memory and half as much CPU power as choosing 512MB of memory. To learn more, see our Function Configuration documentation.You can set your memory from 128MB to 10,240MB. "
688,When should I use AWS Lambda functions with more than 3008 MB of memory?,"Customers running memory or compute-intensive workloads can now use more memory for their functions. Larger memory functions help multithreaded applications run faster, making them ideal for data and computationally intensive applications like machine learning, batch and ETL jobs, financial modeling, genomics, HPC, and media processing. "
689,How long can an AWS Lambda function execute?,AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes. 
690,How will I be charged for using AWS Lambda functions?,AWS Lambda is priced on a pay-per-use basis. Please see the AWS Lambda pricing page for details. 
691,Can I save money on AWS Lambda with a Compute Savings Plan?,"Yes. In addition to saving money on Amazon EC2 and AWS Fargate, you can also use Compute Savings Plans to save money on AWS Lambda. Compute Savings Plans offer up to 17% discount on Duration, Provisioned Concurrency, and Duration (Provisioned Concurrency). Compute Savings Plans do not offer a discount on Requests in your Lambda bill. However, your Compute Savings Plans commitment can apply to Requests at regular rates. "
692,Does AWS Lambda support versioning?,"Yes. By default, each AWS Lambda function has a single, current version of the code. Clients of your Lambda function can call a specific version or get the latest implementation. Please read our documentation on versioning Lambda functions. "
693,How long after uploading my code will my AWS Lambda function be ready to call?,"Deployment times may vary with the size of your code, but AWS Lambda functions are typically ready to call within seconds of upload. "
694,Can I use my own version of a supported library?,Yes. You can include your own copy of a library (including the AWS SDK) in order to use a different version than the default one provided by AWS Lambda. 
695,How does tiered pricing work?,"AWS Lambda offers discounted pricing tiers for monthly on-demand function duration above certain thresholds. Tiered pricing is available for functions running on both x86 and Arm architectures. Lambda pricing tiers are applied to aggregate monthly on-demand duration of your functions running on the same architecture (x86 or Arm, respectively), in the same region, within the account. If you're using consolidated billing in AWS Organizations, pricing tiers are applied to the aggregate monthly duration of your functions running on the same architecture, in the same region, across the accounts in the organization. For example, if you are running x86 Lambda functions in the US East (Ohio) region, you will pay $0.0000166667 for every GB-second for the first 6 billion GB-seconds per month, $0.0000150000 for every GB-second for the next 9 billion GB-seconds per month, and $0.0000133334 for every GB-second over 15 billion GB-seconds per month, in that region. Pricing for Requests, Provisioned Concurrency, and Provisioned Concurrency Duration remains unchanged. For more information, please see AWS Lambda Pricing "
696,"Can I take advantage of both tiered pricing, and Compute Savings Plans?",Yes. Lambda usage that is covered by your hourly savings plan commitment is billed at the applicable CSP rate and discount. The remaining usage that is not covered by this commitment will be billed at the rate corresponding to the tier your monthly aggregate function duration falls in. 
697,What is an event source?,"An event source is an AWS service or developer-created application that produces events that trigger an AWS Lambda function to run. Some services publish these events to Lambda by invoking the cloud function directly (for example, Amazon S3). Lambda can also poll resources in other services that do not publish events to Lambda. For example, Lambda can pull records from an Amazon Kinesis stream or an Amazon SQS queue and execute a Lambda function for each fetched message.Many other services, such as AWS CloudTrail, can act as event sources simply by logging to Amazon S3 and using S3 bucket notifications to trigger AWS Lambda functions "
698,What event sources can be used with AWS Lambda?,Please see our documentation for a complete list of event sources. 
699,How are events represented in AWS Lambda?,"Events are passed to a Lambda function as an event input parameter. For event sources where events arrive in batches, such as Amazon SQS, Amazon Kinesis, and Amazon DynamoDB Streams, the event parameter may contain multiple events in a single call, based on the batch size you request. To learn more about Amazon S3 event notifications, visit Configuring Notifications for Amazon S3 Events. To learn more about Amazon DynamoDB Streams, visit the DynamoDB Stream Developers Guide. To learn more about invoking Lambda functions using Amazon SNS, visit the Amazon SNS Developers Guide. For more information on Amazon Cognito events, visit Amazon Cognito. For more information on AWS CloudTrail logs and auditing API calls across AWS services, see AWS CloudTrail. "
700,How do I make an AWS Lambda function respond to changes in an Amazon S3 bucket?,"From the AWS Lambda console, you can select a function and associate it with notifications from an Amazon S3 bucket. Alternatively, you can use the Amazon S3 console and configure the bucket's notifications to send to your AWS Lambda function. This same functionality is also available through the AWS SDK and CLI. "
701,How do I make an AWS Lambda function respond to updates in an Amazon DynamoDB table?,"You can trigger a Lambda function on DynamoDB table updates by subscribing your Lambda function to the DynamoDB Stream associated with the table. You can associate a DynamoDB Stream with a Lambda function using the Amazon DynamoDB console, the AWS Lambda console, or Lambda's registerEventSource API. "
702,How do I use an AWS Lambda function to process records in an Amazon Kinesis stream?,"From the AWS Lambda console, you can select a Lambda function and associate it with an Amazon Kinesis stream owned by the same account. This same functionality is also available through the AWS SDK and CLI. "
703,How does AWS Lambda process data from Amazon Kinesis streams and Amazon DynamoDB Streams?,"The Amazon Kinesis and DynamoDB Streams records sent to your AWS Lambda function are strictly serialized, per shard. This means that if you put two records in the same shard, Lambda guarantees that your Lambda function will be successfully invoked with the first record before it is invoked with the second record. If the invocation for one record times out, is throttled, or encounters any other error, Lambda will retry until it succeeds (or the record reaches its 24-hour expiration) before moving on to the next record. The ordering of records across different shards is not guaranteed, and processing of each shard happens in parallel. "
704,How should I choose between AWS Lambda and Amazon Kinesis Data Analytics for my analytics needs?,"AWS Lambda allows you to perform time-based aggregations (such as count, max, sum, average, etc.) over a short window of up to 15 minutes for your data in Amazon Kinesis or Amazon DynamoDB Streams over a single logical partition such as a shard. This gives you the option to easily set up simple analytics for your event-based application without adding architectural complexity, as your business and analytics logic can be located in the same function. Lambda allows aggregations over a maximum of a 15-minute tumbling window, based on the event timestamp. Amazon Kinesis Data Analytics allows you to build more complex analytics applications that support flexible processing choices and robust fault-tolerance with exactly-once processing without duplicates, and analytics that can be performed over an entire data stream across multiple logical partitions. With KDA, you can analyze data over multiple types of aggregation windows (tumbling window, stagger window, sliding window, session window) using either the event time or the processing time.    AWS Lambda Amazon KDA Tumbling Window Yes Yes Stagger Window No Yes Sliding Window No Yes Session Window No Yes Enrichment No Yes Joint input and reference tables No Yes Split input stream No Yes Exactly-once processing No Yes Maximum time window 15 mins No limit Aggregation scope Partition/shard Stream Time semantics Event time Event time, Processing time "
705,How do I use an AWS Lambda function to respond to notifications sent by Amazon Simple Notification Service (SNS)?,"From the AWS Lambda console, you can select a Lambda function and associate it with an Amazon SNS topic. This same functionality is also available through the AWS SDK and CLI. "
706,How do I use an AWS Lambda function to respond to emails sent by Amazon Simple Email Service (SES)?,"From the Amazon SES Console, you can set up your receipt rule to have Amazon SES deliver your messages to an AWS Lambda function. The same functionality is available through the AWS SDK and CLI. "
707,How do I use an AWS Lambda function to respond to Amazon CloudWatch alarms?,"First, configure the alarm to send Amazon SNS notifications. Then from the AWS Lambda console, select a Lambda function and associate it with that Amazon SNS topic. See the Amazon CloudWatch Developer Guide for more on setting up Amazon CloudWatch alarms. "
708,How do I use an AWS Lambda function to respond to changes in user or device data managed by Amazon Cognito?,"From the AWS Lambda console, you can select a function to trigger when any datasets associated with an Amazon Cognito identity pool are synchronized. This same functionality is also available through the AWS SDK and CLI. Visit Amazon Cognito for more information on using Amazon Cognito to share and synchronize data across a user's devices. "
709,How can my application trigger an AWS Lambda function directly?,You can invoke a Lambda function using a custom event through AWS Lambda's invoke API. Only the function owner or another AWS account that the owner has granted permission can invoke the function. Visit the Lambda Developers Guide to learn more. 
710,What is the latency of invoking an AWS Lambda function in response to an event?,"AWS Lambda is designed to process events within milliseconds. Latency will be higher immediately after a Lambda function is created, updated, or if it has not been used recently. "
711,How do I create a mobile backend using AWS Lambda?,"You upload the code you want AWS Lambda to execute and then invoke it from your mobile app using the AWS Lambda SDK included in the AWS Mobile SDK. You can make both direct (synchronous) calls to retrieve or check data in real time, as well as asynchronous calls. You can also define a custom API using Amazon API Gateway and invoke your Lambda functions through any REST compatible client. To learn more about the AWS Mobile SDK, visit the AWS Mobile SDK page. To learn more about Amazon API Gateway, visit the Amazon API Gateway page. "
712,How do I invoke an AWS Lambda function over HTTPS?,"You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. This gives you an endpoint for your function which can respond to REST calls like GET, PUT, and POST. Read more about using AWS Lambda with Amazon API Gateway. "
713,How can my AWS Lambda function customize its behavior to the device and app making the request?,"When called through the AWS Mobile SDK, AWS Lambda functions automatically gain insight into the device and application that made the call through the context' object. "
714,How can my AWS Lambda function personalize its behavior based on the identity of the end-user of an application?,"When your app uses the Amazon Cognito identity, end users can authenticate themselves using a variety of public login providers such as Amazon, Facebook, Google, and other OpenID Connect-compatible services. User identity is then automatically and secured presented to your Lambda function in the form of an Amazon Cognito id, allowing it to access user data from Amazon Cognito, or as a key to store and retrieve data in Amazon DynamoDB or other web services. "
715,How do I create an Alexa skill using AWS Lambda?,"AWS Lambda is integrated with the Alexa Skills Kit, a collection of self-service APIs, tools, documentation, and code samples that make it easy for you to create voice-driven capabilities (or skills) for Alexa. You simply upload the Lambda function code for the new Alexa skill you are creating, and AWS Lambda does the rest, executing the code in response to Alexa voice interactions and automatically managing the compute resources on your behalf. Read the Alexa Skills Kit documentation for more details. "
716,What happens if my function fails while processing an event?,"For Amazon S3 bucket notifications and custom events, AWS Lambda will attempt execution of your function three times in the event of an error condition in your code or if you exceed a service or resource limit. For ordered event sources that AWS Lambda polls on your behalf, such as Amazon DynamoDB Streams and Amazon Kinesis streams, Lambda will continue attempting execution in the event of a developer code error until the data expires. You can monitor progress through the Amazon Kinesis and Amazon DynamoDB consoles and through the Amazon CloudWatch metrics that AWS Lambda generates for your function. You can also set Amazon CloudWatch alarms based on error or execution throttling rates. "
717,What is a serverless application?,"Lambda-based applications (also referred to as serverless applications) are composed of functions triggered by events. A typical serverless application consists of one or more functions triggered by events such as object uploads to Amazon S3, Amazon SNS notifications, or API actions. These functions can stand alone or leverage other resources such as DynamoDB tables or Amazon S3 buckets. The most basic serverless application is simply a function. "
718,How do I deploy and manage a serverless application?,You can deploy and manage your serverless applications using the AWS Serverless Application Model (AWS SAM). AWS SAM is a specification that prescribes the rules for expressing serverless applications on AWS. This specification aligns with the syntax used by AWS CloudFormation today and is supported natively within AWS CloudFormation as a set of resource types (referred to as serverless resources). These resources make it easier for AWS customers to use CloudFormation to configure and deploy serverless applications using existing CloudFormation APIs. 
719,How can I discover existing serverless applications developed by the AWS community?,"You can choose from a collection of serverless applications published by developers, companies, and partners in the AWS community with the AWS Serverless Application Repository. After finding an application, you can configure and deploy it straight from the Lambda console. "
720,How do I automate deployment for a serverless application?,"You can automate your serverless application release process using AWS CodePipeline and AWS CodeDeploy. CodePipeline is a continuous delivery service that enables you to model, visualize and automate the steps required to release your serverless application. CodeDeploy provides a deployment automation engine for your Lambda-based applications. CodeDeploy lets you orchestrate deployments according to established best-practice methodologies such as canary and linear deployments, and helps you establish the necessary guardrails to verify that newly-deployed code is safe, stable, and ready to be fully released to production.  To learn more about serverless CI/CD, visit our documentation. "
721,How do I get started on building a serverless application?,"To get started, visit the AWS Lambda console and download one of our blueprints. The file you download will contain an AWS SAM file (which defines the AWS resources in your application) and a .ZIP file (which includes your function code). You can then use AWS CloudFormation commands to package and deploy the serverless application that you just downloaded. For more details, visit our documentation. "
722,How do I coordinate calls between multiple AWS Lambda functions?,"You can use AWS Step Functions to coordinate a series of AWS Lambda functions in a specific order. You can invoke multiple Lambda functions sequentially, passing the output of one to the other, and/or in parallel, and Step Functions will maintain state during executions for you. "
723,How do I troubleshoot a serverless application?,"You can enable your Lambda function for tracing with AWS X-Ray by adding X-Ray permissions to your Lambda function execution role and changing your function tracing mode to active.  When X-Ray is enabled for your Lambda function, AWS Lambda will emit tracing information to X-Ray regarding the Lambda service overhead incurred when invoking your function. This will provide you with insights such as Lambda service overhead, function init time, and function execution time. In addition, you can include the X-Ray SDK in your Lambda deployment package to create your own trace segments, annotate your traces, or view trace segments for downstream calls made from your Lambda function. X-Ray SDKs are currently available for Node.js and Java. Visit Troubleshooting Lambda-based applications to learn more. AWS X-Ray rates will apply. "
724,Can I build serverless applications that connect to relational databases?,"Yes. You can build highly scalable, secure, Lambda-based serverless applications that connect to relational databases using Amazon RDS Proxy, a highly available database proxy that manages thousands of concurrent connections to relational databases. Currently, RDS Proxy supports MySQL and Aurora databases. You can begin using RDS Proxy through the Amazon RDS console or the AWS Lambda console. Serverless applications that use fully managed connection pools from RDS Proxy will be billed according to RDS Proxy Pricing. "
725,How is AWS SAM licensed?,"The specification is open sourced under Apache 2.0, which allows you and others to adopt and incorporate AWS SAM into build, deployment, monitoring, and management tools with a commercial-friendly license. You can access the AWS SAM repository on GitHub here. "
726,What is Container Image Support for AWS Lambda?,"AWS Lambda now enables you to package and deploy functions as container images. Customers can leverage the flexibility and familiarity of container tooling, and the agility and operational simplicity of AWS Lambda to build applications. "
727,How can I use Container Image Support for AWS Lambda?,"You can start with either an AWS provided base images for Lambda or by using one of your preferred community or private enterprise images. Then, simply use Docker CLI to build the image, upload it to Amazon ECR, and then create the function by using all familiar Lambda interfaces and tools, such as the AWS Management Console, the AWS CLI, the AWS SDK, AWS SAM, and AWS CloudFormation. "
728,Which container image types are supported?,You can deploy third-party Linux base images (e.g. Alpine or Debian) to Lambda in addition to the Lambda provided images. AWS Lambda will support all images based on the following image manifest formats: Docker Image Manifest V2 Schema 2 (used with Docker version 1.10 and newer) or Open Container Initiative (OCI) Spec (v1.0 and up). Lambda supports images with a size of up to 10GB. 
729,What base images can I use?,"AWS Lambda provides a variety of base images customers can extend, and customers can also use their preferred Linux-based images with a size of up to 10GB. "
730,What container tools can I use to package and deploy functions as container images?,"You can use any container tooling as long as it supports one of the following container image manifest formats: Docker Image Manifest V2 Schema 2 (used with Docker version 1.10 and newer) or Open Container Initiative (OCI) Specifications (v1.0 and up). For example, you can use native container tools (i.e. docker run, docker compose, Buildah and Packer) to define your functions as a container image and deploy to Lambda. "
731,What AWS Lambda features are available to functions deployed as container images?,"All existing AWS Lambda features, with the exception of Lambda layers and Code Signing, can be used with functions deployed as container images. Once deployed, AWS Lambda will treat an image as immutable. Customers can use container layers during their build process to include dependencies. "
732,Will AWS Lambda patch and update my deployed container image?,"Not at this time. Your image, once deployed to AWS Lambda, will be immutable. The service will not patch or update the image. However, AWS Lambda will publish curated base images for all supported runtimes that are based on the Lambda managed environment. These published images will be patched and updated along with updates to the AWS Lambda managed runtimes. You can pull and use the latest base image from DockerHub or Amazon ECR Public, re-build your container image and deploy to AWS Lambda via Amazon ECR. This allows you to build and test the updated images and runtimes, prior to deploying the image to production "
733,What are the differences between functions created using ZIP archives vs. container images?,"There are three main differences between functions created using ZIP archives vs. container images: Functions created using ZIP archives have a maximum code package size of 250 MB unzipped, and those created using container images have a maximum image size of 10 GB.  Lambda uses Amazon ECR as the underlying code storage for functions defined as container images, so a function may not be invocable when the underlying image is deleted from ECR.  ZIP functions are automatically patched for the latest runtime security and bug fixes. Functions defined as container images are immutable, and customers are responsible for the components packaged in their function. Customers can leverage the AWS provided base images which are regularly updated by AWS for security and bug fixes, using the most recent patches available. "
734,Is there a performance difference between functions defined as zip and container images?,"No - AWS Lambda ensures that the performance profiles for functions packaged as container images are the same as for those packaged as ZIP archives, including typically sub-second start up times. "
735,How will I be charged for deploying Lambda functions as container images?,"There is no additional charge for packaging and deploying functions as container images to AWS Lambda. When you invoke your function deployed as a container image, you pay the regular price for requests and execution duration. To learn more, visit AWS Lambda pricing. You will be charged for storing your container images in Amazon ECR at the standard ECR prices. To learn more, visit Amazon ECR pricing. "
736,What is the Lambda Runtime Interface Emulator (RIE)?,"The Lambda Runtime Interface Emulator is a proxy for the Lambda Runtime API,which allows customers to locally test their Lambda function packaged as a container image. It is a lightweight web server that converts HTTP requests to JSON events and emulates the Lambda Runtime API. It allows you to locally test your functions using familiar tools such as cURL and the Docker CLI (when testing functions packaged as container images). It also simplifies running your application on additional compute services. You can include the Lambda Runtime Interface Emulator in your container image to have it accept HTTP requests natively instead of the JSON events required for deployment to Lambda. This component does not emulate the Lambda orchestrator, or security and authentication configurations. The Runtime Interface Emulator is open sourced on GitHub. You can get started by downloading and installing it on your local machine. "
737,Why do I need the Lambda Runtime Interface Emulator (RIE) during local testing?,"The Lambda Runtime API in the running Lambda service accepts JSON events and returns responses. The Lambda Runtime Interface Emulator allows the function packaged as a container image to accept HTTP requests during local testing with tools like cURL, and surface them via the same interface locally to the function. It allows you to use the docker run or docker-compose up command to locally test your lambda application. "
738,What function behaviors can I test locally with the emulator?,"You can use the emulator to test if your function code is compatible with the Lambda environment, runs successfully, and provides the expected output. For example, you can mock test events from different event sources. You can also use it to test extensions and agents built into the container image against the Lambda Extensions API. "
739,How does the Runtime Interface Emulator (RIE) help me run my Lambda compatible image on additional compute services?,"Customers can add the Runtime Interface Emulator as the entry point to the container image or package it as a sidecar to ensure the container image now accepts HTTP requests instead of JSON events. This simplifies the changes required to run their container image on additional compute services. Customers will be responsible for ensuring they follow all security, performance, and concurrency best practices for their chosen environment. RIE is pre-packaged into the AWS Lambda provided images, and is available by default in AWS SAM CLI. Base image providers can use the documentation to provide the same experience for their base images. "
740,How can I deploy my existing containerized application to AWS Lambda?,"You can deploy a containerized application to AWS Lambda if it meets the below requirements: The container image must implement the Lambda Runtime API. We have open-sourced a set of software packages, Runtime Interface Clients (RIC), that implement the Lambda Runtime API, allowing you to seamlessly extend your preferred base images to be Lambda compatible. The container image must be able to run on a read-only filesystem. Your function code can access a writable /tmp directory storage of 512 MB. If you are using an image that requires a writable root directory, configure it to write to the /tmp directory. The files required for the execution of function code can be read by the default Lambda user. Lambda defines a default Linux user with least-privileged permissions that follows security best practices. You need to verify that your application code does not rely on files that are restricted by other Linux users for execution. It is a Linux based container image. "
741,What is AWS Lambda SnapStart?,"AWS Lambda SnapStart for Java delivers up to 10x faster function startup performance. For on-demand functions, the initialization phase (where AWS Lambda loads the function's code and initializes external dependencies) is the largest contributor to start-up latency, and happens on the first invoke. With Lambda SnapStart, Lambda initializes the one-time initialization function code ahead of time when you publish a function version, instead of when you first invoke the function. Then, Lambda takes a snapshot and caches the memory and disk state of the initialized  execution environment. When you invoke the function and as it scales up Lambda resumes the function from the cached snapshot instead of initializing the function from scratch. "
742,How do I configure my Lambda function to use Lambda SnapStart?,"Lambda SnapStart is a simple function level configuration that can be configured for new and existing Java functions by using Lambda API, the AWS Management Console, AWS Command Line Interface (CLI), AWS SDK, AWS Cloud Development Kit (CDK), AWS CloudFormation, and the AWS Serverless Application Model (SAM). When you configure Lambda SnapStart, every function version that is published thereafter benefits from the improved startup performance offered by Lambda SnapStart. To learn more about Lambda SnapStart, see the documentation. "
743,How do I choose between Lambda SnapStart and Provisioned Concurrency (PC)?,"Lambda SnapStart is a performance optimization that helps your Java functions to achieve up to 10x faster start-up times by reducing the variable latency incurred during execution of one-time initialization code. Lambda SnapStart works broadly across all functions in your application or account at no additional cost. When a customer publishes a function version with Lambda SnapStart, the function's code is initialized ahead of time, instead of being initialized on the first invoke. Lambda then takes a snapshot of the initialized execution environment and persists it in a tiered cache for low-latency access. When the function is first invoked and then scaled, Lambda resumes the function from the cached snapshot instead of initializing from scratch, driving a lower startup latency. While Lambda SnapStart reduces startup latency, it works as a best-effort optimization, and does not guarantee elimination of cold starts. If your application has strict latency requirements and requires double-digit millisecond startup times, we recommend you use PC. "
744,Which runtimes does Lambda SnapStart support?,"Lambda SnapStart supports the Java 11 runtime. Future versions of Java will be supported after they are released. For all runtimes supported by Lambda, see the Lambda runtimes documentation. "
745,Can I enable both Lambda SnapStart and PC on the same function?,"No. Lambda SnapStart and PC cannot be enabled at the same time, on the same function. "
746,Can I configure a Lambda SnapStart function with a virtual private cloud (VPC)?,"Yes. You can configure a Lambda SnapStart function to access resources in a virtual private cloud (VPC). For more information on how to configure your function with a VPC, see the Lambda documentation. "
747,Can I configure Lambda SnapStart on both x86 and Arm architectures?,No. You can configure Lambda SnapStart only for functions running on x86 architecture at this time. 
748,Can I enable Lambda SnapStart with Amazon Elastic File System (EFS)?,No. You cannot enable Lambda SnapStart with Amazon EFS at this time. 
749,Can I enable Lambda SnapStart with larger ephemeral storage (/tmp) beyond 512 MB?,No. You cannot enable Lambda SnapStart with larger ephemeral storage (/tmp) beyond 512 MB at this time. 
750,Does the process of caching and resuming from snapshots introduce software compatibility considerations?,"Yes. If your code assumes uniqueness of state, you need to evaluate your code's resilience to snapshot operations (such as being cloned and resumed). To learn more on uniqueness considerations with Lambda SnapStart, see the documentation and blog on understanding uniqueness in VM snapshots with Lambda SnapStart. "
751,Can I execute my own code before a snapshot is created or when the function is resumed from snapshot?,"Yes. You can implement your own software logic before creating (checkpointing) a snapshot and after restoring a snapshot using runtime hooks. To learn more, see the Lambda SnapStart documentation. "
752,Will I be charged for Lambda SnapStart?,"No. There's no additional cost for enabling Lambda SnapStart. You are charged based on the number of requests for your functions and the duration your code executes based on current Lambda Pricing. Duration charges apply to code that runs in the handler of a function and runtime hooks, as well as initialization code that is declared outside of the handler. Please note that AWS Lambda may periodically recycle execution environments with security patches and rerun your initialization code. For more details, see the Lambda Programming Model documentation. "
753,How long do the snapshots for the published function version stay cached with Lambda SnapStart?,"With Lambda SnapStart, Lambda keeps a snapshot of the initialized execution environment for the last three published function versions, as long as the published versions continue to receive invokes. The snapshot associated with a published function version expires if it remains inactive for more than 14 days. "
754,How can I encrypt the snapshots of initialized execution environment created by Lambda SnapStart?,Snapshots are encrypted be default with customer-unique AWS Key Management Service (KMS) keys owned and managed by the Lambda service. Customers can also encrypt snapshots using a KMS key owned and managed by the customer. 
755,Is there a time limit for how long my code initialization can run with Lambda SnapStart?,The maximum allowed initialization duration for Lambda SnapStart will match the execution timeout duration you have configured for your function. The maximum configurable execution timeout limit for a function is 15 minutes. 
756,What is AWS Lambda Provisioned Concurrency?,"Provisioned Concurrency gives you greater control over the performance of your serverless applications. When enabled, Provisioned Concurrency keeps functions initialized and hyper-ready to respond in double-digit milliseconds. "
757,How do I set up and manage Provisioned Concurrency?,"You can configure concurrency on your function through the AWS Management Console, the Lambda API, the AWS CLI, and AWS CloudFormation. The simplest way to benefit from Provisioned Concurrency is by using AWS Auto Scaling. You can use Application Auto Scaling to configure schedules, or have Auto Scaling automatically adjust the level of Provisioned Concurrency in real time as demand changes. To learn more about Provisioned Concurrency, see the documentation. "
758,Do I need to change my code if I want to use Provisioned Concurrency?,You don't need to make any changes to your code to use Provisioned Concurrency. It works seamlessly with all existing functions and runtimes. There is no change to the invocation and execution model of Lambda when using Provisioned Concurrency. 
759,How will I be charged for Provisioned Concurrency?,"Provisioned Concurrency adds a pricing dimension, of Provisioned Concurrency', for keeping functions initialized. When enabled, you pay for the amount of concurrency that you configure and for the period of time that you configure it. When your function executes while Provisioned Concurrency is configured on it, you also pay for Requests and execution Duration. To learn more about the pricing of Provisioned Concurrency, see AWS Lambda Pricing. "
760,When should I use Provisioned Concurrency?,"Provisioned Concurrency is ideal for building latency-sensitive applications, such as web or mobile backends, synchronously invoked APIs, and interactive microservices. You can easily configure the appropriate amount of concurrency based on your application's unique demand. You can increase the amount of concurrency during times of high demand and lower it, or turn it off completely, when demand decreases. "
761,What happens if a function receives invocations above the configured level of Provisioned Concurrency?,"If the concurrency of a function reaches the configured level, subsequent invocations of the function have the latency and scale characteristics of regular Lambda functions. You can restrict your function to only scale up to the configured level. Doing so prevents the function from exceeding the configured level of Provisioned Concurrency. This is a mechanism to prevent undesired variability in your application when demand exceeds the anticipated amount. "
762,What are AWS Lambda functions powered by Graviton2 processors?,"AWS Lambda allows you to run your functions on either x86-based or Arm-based processors. AWS Graviton2 processors are custom built by Amazon Web Services using 64-bit Arm Neoverse cores to deliver increased price performance for your cloud workloads. Customers get the same advantages of AWS Lambda, running code without provisioning or managing servers, automatic scaling, high availability, and only paying for the resources you consume. "
763,Why should I use AWS Lambda functions powered by Graviton2 processors?,"AWS Lambda functions powered by Graviton2, using an Arm-based processor architecture designed by AWS, are designed to deliver up to 34% better price performance compared to functions running on x86 processors, for a variety of serverless workloads, such as web and mobile backends, data, and stream processing. With lower latency, up to 19% better performance, a 20% lower cost, and the highest power-efficiency currently available at AWS, Graviton2 functions can power mission critical serverless applications. Customers can configure both existing and new functions to target the Graviton2 processor. They can deploy functions running on Graviton2 as either zip files or container images. "
764,How do I configure my functions to run on Graviton2 processors?,"You can configure functions to run on Graviton2 through the AWS Management Console, the AWS Lambda API, the AWS CLI, and AWS CloudFormation by setting the architecture flag to arm64' for your function. "
765,How do I deploy my application built using functions powered by Graviton2 processors?,"There is no change between x86-based and Arm-based functions. Simply upload your code via the AWS Management Console, zip file, or container image, and AWS Lambda automatically runs your code when triggered, without requiring you to provision or manage infrastructure. "
766,"Do I need an Arm-based development machine to create, build, and test functions powered by Graviton2 processors locally?","Interpreted languages like Python, Java, and Node generally do not require recompilation unless your code references libraries that use architecture specific components. In those cases, you would need to provide the libraries targeted to arm64. For more details, please see the Getting started with AWS Graviton page. Non-interpreted languages will require compiling your code to target arm64. While more modern compilers will produce compiled code for arm64, you will need to deploy it into an arm-based environment to test. To learn more about using Lambda functions with Graviton2, please see the documentation. "
767,Can an application use both functions powered by Graviton2 processors and x86 processors?,"An application can contain functions running on both architectures. AWS Lambda allows you to change the architecture (x86_64' or arm64') of your function's current version. Once you create a specific version of your function, the architecture cannot be changed. "
768,Does AWS Lambda support multi-architecture container images?,No. Each function version can only use a single container image. 
769,Can I create AWS Lambda Layers that target functions powered by AWS Graviton2 processors?,Yes. Layers and extensions can be targeted to x86_64' or arm64' compatible architectures. The default architecture for functions and layers is x86_64'. 
770,What languages and runtimes are supported by Lambda functions running on Graviton2 processors?,"At launch, customers can use Python, Node.js, Java, Ruby, .Net Core, Custom Runtime (provided.al2), and OCI Base images. To learn more, please see the AWS Lambda Runtimes. "
771,What is the pricing of AWS Lambda functions powered by AWS Graviton2 processors? Does the AWS Lambda free tier apply to functions powered by Graviton2?,AWS Lambda functions powered by AWS Graviton2 processors are 20% cheaper compared to x86-based Lambda functions. The Lambda free tier applies to AWS Lambda functions powered by x86 and Arm-based architectures. 
772,How do I choose between running my functions on Graviton2 processors or x86 processors?,"Each workload is unique and we recommend customers test their functions to determine the price performance improvement they might see. To do that, we recommend using the AWS Lambda Power Tuning tool. We recommend starting with web and mobile backends, data, and stream processing when testing your workloads for potential price performance improvements. "
773,What is Amazon EFS for AWS Lambda?,"With Amazon Elastic File System (Amazon EFS) for AWS Lambda, customers can securely read, write and persist large volumes of data at virtually any scale using a fully managed elastic NFS file system that can scale on demand without the need for provisioning or capacity management. Previously, developers added code to their functions to download data from S3 or databases to local temporary storage, limited to 512MB. With EFS for Lambda, developers don't need to write code to download data to temporary storage in order to process it. "
774,How do I set up Amazon EFS for Lambda?,"Developers can easily connect an existing EFS file system to a Lambda function via an EFS Access Point by using the console, CLI, or SDK. When the function is first invoked, the file system is automatically mounted and made available to function code. You can learn more in the documentation. "
775,Do I need to configure my function with VPC settings before I can use my Amazon EFS file system?,Yes. Mount targets for Amazon EFS are associated with a subnet in a VPC. The AWS Lambda function needs to be configured to access that VPC. 
776,Who should use Amazon EFS for Lambda?,"Using EFS for Lambda is ideal for building machine learning applications or loading large reference files or models, processing or backing up large amounts of data, hosting web content, or developing internal build systems. Customers can also use EFS for Lambda to keep state between invocations within a stateful microservice architecture, in a Step Functions workflow, or sharing files between serverless applications and instance or container-based applications. "
777,Will my data be encrypted in transit?,Yes. Data encryption in transit uses industry-standard Transport Layer Security (TLS) 1.2 to encrypt data sent between AWS Lambda functions and the Amazon EFS file systems. 
778,Is my data encrypted at rest?,"Customers can provision Amazon EFS to encrypt data at rest. Data encrypted at rest is transparently encrypted while being written, and transparently decrypted while being read, so you don't have to modify your applications. Encryption keys are managed by the AWS Key Management Service (KMS), eliminating the need to build and maintain a secure key management infrastructure. "
779,How will I be charged for Amazon EFS for AWS Lambda?,"There is no additional charge for using Amazon EFS for AWS Lambda. Customers pay the standard price for AWS Lambda and for Amazon EFS. When using Lambda and EFS in the same availability zone, customers are not charged for data transfer. However, if they use VPC peering for Cross-Account access, they will incur data transfer charges. To learn more, please see Pricing. "
780,Can I associate more than one Amazon EFS file system with my AWS Lambda function?,No. Each Lambda function will be able to access one EFS file system. 
781,"Can I use the same Amazon EFS file system across multiple functions, containers, and instances?","Yes. Amazon EFS supports Lambda functions, ECS and Fargate containers, and EC2 instances. You can share the same file system and use IAM policy and Access Points to control what each function, container, or instance has access to. "
782,Do AWS Lambda functions support HTTP(S) endpoints?,"Yes. Lambda functions can be configured with a function URL, a built-in HTTPS endpoint that can be invoked using the browser, curl, and any HTTP client. Function URLs are an easy way to get started building HTTPS accessible functions. "
783,How do I configure a Lambda function URL for my function?,"You can configure a function URL for your function through the AWS Management Console, the AWS Lambda API, the AWS CLI, AWS CloudFormation, and the AWS Serverless Application Model. Function URLs can be enabled on the $LATEST unqualified version of your function, or on any function alias. To learn more about configuring a function URL, see the documentation. "
784,How do I secure my Lambda function URL?,Lambda function URLs are secured with IAM authorization by default. You can choose to disable IAM authorization to create a public endpoint or if you plan to implement custom authorization as part of the function's business logic. 
785,How do I invoke my function with a Lambda function URL?,"You can easily invoke your function from your web browser by navigating to the Lambda URL, from your client application's code using an HTTP library, or from the command line using curl. "
786,Do Lambda function URLs work with function versions and aliases?,"Yes. Lambda function URLs can be enabled on a function or function alias. If no alias is specified, the URL will point to $LATEST by default. Function URLs cannot target an individual function version. "
787,Can I enable custom domains for my Lambda function URL?,"Custom domain names are not currently supported with function URLs. You can use a custom domain with your function URL by creating an Amazon CloudFront distribution and a CNAME to map your custom domain to your CloudFront distribution name. Then, map your CloudFront distribution domain name to be routed to your function URL as an origin. "
788,Can Lambda function URLs be used to invoke a function in a VPC?,"Yes, function URLs can be used to invoke a Lambda function in a VPC. "
789,What is the pricing for using Lambda function URLs?,"There is no additional charge for using function URLs. You pay the standard price for AWS Lambda. To learn more, please see AWS Lambda Pricing. "
790,What is Lambda@Edge?,"Lambda@Edge allows you to run code across AWS locations globally without provisioning or managing servers, responding to end-users at the lowest network latency. You just upload your Node.js or Python code to AWS Lambda and configure your function to be triggered in response to Amazon CloudFront requests (i.e., when a viewer request lands, when a request is forwarded to or received back from the origin, and right before responding back to the end-user). The code is then ready to execute across AWS locations globally when a request for content is received, and scales with the volume of CloudFront requests globally. Learn more in our documentation. "
791,How do I use Lambda@Edge?,"To use Lambda@Edge, you just upload your code to AWS Lambda and associate a function version to be triggered in response to Amazon CloudFront requests. Your code must satisfy the Lambda@Edge service limits. Lambda@Edge supports Node.js and Python for global invocation by CloudFront events at this time. Learn more in our documentation. "
792,When should I use Lambda@Edge?,"Lambda@Edge is optimized for latency-sensitive use cases where your end viewers are distributed globally. All the information you need to make a decision should be available at the CloudFront edge, within the function and the request. This means that use cases where you are looking to make decisions on how to serve content based on user characteristics (e.g., location, client device, etc.) can now be executed and served close to your users without having to be routed back to a centralized server. "
793,Can I deploy my existing Lambda functions for global invocation?,You can associate existing Lambda functions with CloudFront events for global invocation if the function satisfies the Lambda@Edge service requirements and limits. Read more here on how to update your function properties. 
794,What Amazon CloudFront events can be used to trigger my functions?,"Your functions will automatically trigger in response to the following Amazon CloudFront events: Viewer Request - This event occurs when an end-user or a device on the Internet makes an HTTP(S) request to CloudFront, and the request arrives at the edge location closest to that user. Viewer Response - This event occurs when the CloudFront server at the edge is ready to respond to the end user or the device that made the request. Origin Request - This event occurs when the CloudFront edge server does not already have the requested object in its cache, and the viewer request is ready to be sent to your backend origin web server (e.g. Amazon EC2, or Application Load Balancer, or Amazon S3). Origin Response - This event occurs when the CloudFront server at the edge receives a response from your backend origin web server. "
795,How is AWS Lambda@Edge different from using AWS Lambda behind Amazon API Gateway?,The difference is that API Gateway and Lambda are regional services. Using Lambda@Edge and Amazon CloudFront allows you to execute logic across multiple AWS locations based on where your end viewers are located. 
796,How available are AWS Lambda functions?,AWS Lambda is designed to use replication and redundancy to provide high availability for both the service itself and for the Lambda functions it operates. There are no maintenance windows or scheduled downtimes for either. 
797,Do my AWS Lambda functions remain available when I change my code or its configuration?,"Yes. When you update a Lambda function, there will be a brief window of time, typically less than a minute, when requests could be served by either the old or the new version of your function. "
798,Is there a limit to the number of AWS Lambda functions I can execute at once?,"No. AWS Lambda is designed to run many instances of your functions in parallel. However, AWS Lambda has a default safety throttle, for the number of concurrent executions per account per region (visit here for info on default safety throttle limits). You can also control the maximum concurrent executions for individual AWS Lambda functions, which you can use to reserve a subset of your account concurrency limit for critical functions, or cap traffic rates to downstream resources.  If you wish to submit a request to increase the concurrent execution limit, you can use Service Quotas to request a limit increase request. "
799,What happens if my account exceeds the default throttle limit on concurrent executions?,"On exceeding the maximum concurrent executions limit, AWS Lambda functions being invoked synchronously will return a throttling error (429 error code). Lambda functions being invoked asynchronously can absorb reasonable bursts of traffic for approximately 15-30 minutes, after which incoming events will be rejected as throttled. In case the Lambda function is being invoked in response to Amazon S3 events, events rejected by AWS Lambda may be retained and retried by S3 for 24 hours. Events from Amazon Kinesis streams and Amazon DynamoDB streams are retried until the Lambda function succeeds or the data expires. Amazon Kinesis and Amazon DynamoDB Streams retain data for 24 hours. "
800,Are default maximum concurrent execution limits applied on a per function level?,"The default maximum concurrent execution limit is applied at the account level. However, you can also set limits on individually functions as well (visit here for info on Reserved Concurrency). "
801,How quickly do my AWS Lambda functions scale?,"Each synchronously invoked Lambda function can scale at a rate of up to 1000 concurrent executions every 10 seconds. While Lambda's scaling rate is suitable for most use cases, it is particularly ideal for those with predictable or unpredictable bursts of traffic. For example, SLA-bound data processing would require predictable yet rapid scaling to meet processing demand. Similarly, serving breaking news articles, or flash sales could drive unpredictable levels of traffic in a short period of time. Lambda's scaling rate can facilitate such use cases without additional configurations or tooling. Additionally, the concurrency scaling limit is a function-level limit, which means each function in your account scales independently of other functions. "
802,What happens if my Lambda function fails while processing an event?,"On failure, Lambda functions being invoked synchronously will respond with an exception. Lambda functions being invoked asynchronously are retried at least 3 times. Events from Amazon Kinesis streams and Amazon DynamoDB streams are retried until the Lambda function succeeds or the data expires. Kinesis and DynamoDB Streams retain data for a minimum of 24 hours. "
803,What happens if my Lambda function invocations exhaust the available policy?,"On exceeding the retry policy for asynchronous invocations, you can configure a dead letter queue (DLQ) into which the event will be placed; in the absence of a configured DLQ the event may be rejected. On exceeding the retry policy for stream-based invocations, the data would have already expired and therefore rejected. "
804,What resources can I configure as a dead letter queue for a Lambda function?,You can configure an Amazon SQS queue or an Amazon SNS topic as your dead letter queue. 
805,How do I allow my AWS Lambda function access to other AWS resources?,"You grant permissions to your Lambda function to access other resources using an IAM role. AWS Lambda assumes the role while executing your Lambda function, so you always retain full, secure control of exactly which AWS resources it can use. Visit Setting up AWS Lambda to learn more about roles. "
806,How do I control which Amazon S3 buckets can call which AWS Lambda functions?,"When you configure an Amazon S3 bucket to send messages to an AWS Lambda function, a resource policy rule will be created that grants access. Visit the Lambda Developer Guide to learn more about resource policies and access controls for Lambda functions. "
807,How do I control which Amazon DynamoDB table or Amazon Kinesis stream an AWS Lambda function can poll?,Access controls are managed through the Lambda function role. The role you assign to your Lambda function also determines which resource(s) AWS Lambda can poll on its behalf. Visit the Lambda Developer Guide to learn more. 
808,How do I control which Amazon SQS queue an AWS Lambda function can poll?,"Access controls can be managed by the Lambda function role or a resource policy setting on the queue itself. If both policies are present, the more restrictive of the two permissions will be applied. "
809,How do I access resources in Amazon VPC from my AWS Lambda function?,"You can enable Lambda functions to access resources in your VPC by specifying the subnet and security group as part of your function configuration. Lambda functions configured to access resources in a particular VPC will not have access to the internet as a default configuration. To grant internet to these functions, use internet gateways. By default, Lambda functions communicate with resources in a dual-stack VPC over IPv4. You can configure your functions to access resources in a dual-stack VPC over IPv6. For more details on Lambda functions configured with VPC, see Lambda Private Networking with VPC. "
810,What is Code Signing for AWS Lambda?,"Code Signing for AWS Lambda offers trust and integrity controls that enable you to verify that only unaltered code from approved developers is deployed in your Lambda functions. You can use AWS Signer, a fully-managed code signing service, to digitally sign code artifacts and configure your Lambda functions to verify the signatures at deployment. Code Signing for AWS Lambda is currently only available for functions packaged as ZIP archives. "
811,How do I create digitally signed code artifacts?,"You can create digitally signed code artifacts using a Signing Profile through the AWS Signer console, the Signer API, SAM CLI or AWS CLI. To learn more, please see the documentation for AWS Signer. "
812,How do I configure my Lambda functions to enable code signing?,"You can enable code signing by creating a Code Signing Configuration through the AWS Management Console, the Lambda API, the AWS CLI, AWS CloudFormation, and AWS SAM. Code Signing Configuration helps you specify the approved signing profiles and configure whether to warn or reject deployments if signature checks fail. Code Signing Configurations can be attached to individual Lambda functions to enable the code signing feature. Such functions now start verifying signatures at deployment. "
813,What signature checks does AWS Lambda perform on deployment?,"AWS Lambda can perform the following signature checks at deployment:  Corrupt signature - This occurs if the code artifact has been altered since signing. Mismatched signature - This occurs if the code artifact is signed by a signing profile that is not approved. Expired signature - This occurs if the signature is past the configured expiry date. Revoked signature - This occurs if the signing profile owner revokes the signing jobs. To learn more, please see the AWS Lambda documentation. "
814,Can I enable code signing for existing functions?,"Yes, you can enable code signing for existing functions by attaching a code signing configuration to the function. You can do this using the AWS Lambda console, the Lambda API, the AWS CLI, AWS CloudFormation, and AWS SAM. "
815,Is there any additional cost for using Code Signing for AWS Lambda?,"There is no additional cost when using Code Signing for AWS Lambda. You pay the standard price for AWS Lambda. To learn more, please see Pricing. "
816,What advanced logging controls are supported on Lambda?,"To provide you a simplified and enhanced logging experience by default, AWS Lambda offers advanced logging controls such as the ability to natively capture Lambda function logs in JSON structured format, control the log level filtering of Lambda function logs without making any code changes, and customize the Amazon CloudWatch log group Lambda sends logs to. "
817,What can I use advanced logging controls for?,"You can capture Lambda function logs in JSON structured format without having to use your own logging libraries. JSON structured logs make it easier to search, filter, and analyze large volumes of log entries. You can control the log level filtering of Lambda function logs without making any code changes, which enables you to choose the required logging granularity level for Lambda functions without sifting through large volumes of logs when debugging and troubleshooting errors. You can also set which Amazon CloudWatch log group Lambda sends logs to, making it easier to aggregate logs from multiple functions within an application in one place. You can then apply security, governance, and retention policies to logs at the application level rather than individually to every function. "
818,How do I use advanced logging controls?,"You can specify advanced logging controls for your Lambda functions using AWS Lambda API, AWS Lambda console, AWS CLI, AWS Serverless Application Model (SAM), and AWS CloudFormation. To learn more, visit the launch blog post for advanced logging controls or the Lambda Developer Guide. "
819,Can I use my own logging libraries to generate JSON structured logs for my Lambda function?,"Yes, you can use your own logging libraries to generate Lambda logs in JSON structured format. To ensure your logging libraries work seamlessly with Lambda's native JSON structured logging capability, Lambda will not double-encode any logs generated by your function that are already JSON encoded. You can also use Powertools for AWS Lambda library to capture Lambda logs in JSON structured format. "
820,How will I be charged for using the advanced logging controls?,There is no additional charge for using advanced logging controls on Lambda. You will continue to be charged for ingestion and storage of your Lambda logs by Amazon CloudWatch Logs. See CloudWatch pricing page for log pricing details. 
821,How do I compile my AWS Lambda function Java code?,"You can use standard tools like Maven or Gradle to compile your Lambda function. Your build process should mimic the same build process you would use to compile any Java code that depends on the AWS SDK. Run your Java compiler tool on your source files and include the AWS SDK 1.9 or later with transitive dependencies on your classpath. For more details, see our documentation. "
822,What is the JVM environment that Lambda uses for executing my function?,Lambda provides the Amazon Linux build of openjdk 1.8. 
823,Can I use packages with AWS Lambda?,Yes. You can use NPM packages as well as custom packages. Learn more here. 
824,Can I execute other programs from within my AWS Lambda function written in Node.js?,"Yes. Lambda's built-in sandbox lets you run batch (shell) scripts, other language runtimes, utility routines, and executables. Learn more here. "
825,Is it possible to use native modules with AWS Lambda functions written in Node.js?,"Yes. Any statically linked native module can be included in the ZIP file you upload, as well as dynamically linked modules compiled with an rpath pointing to your Lambda function root directory. Learn more here. "
826,Can I execute binaries with AWS Lambda written in Node.js?,Yes. You can use Node.js' child_process command to execute a binary that you included in your function or any executable from Amazon Linux that is visible to your function. Alternatively several NPM packages exist that wrap command line binaries such as node-ffmpeg. Learn more here. 
827,How do I deploy AWS Lambda function code written in Node.js?,"To deploy a Lambda function written in Node.js, simply package your Javascript code and dependent libraries as a ZIP. You can upload the ZIP from your local environment, or specify an Amazon S3 location where the ZIP file is located. For more details, see our documentation. "
828,Can I use Python packages with AWS Lambda?,Yes. You can use pip to install any Python packages needed. 
829,How do I package and deploy an AWS Lambda function in C#?,"You can create a C# Lambda function using the Visual Studio IDE by selecting Publish to AWS Lambda in the Solution Explorer. Alternatively, you can directly run the dotnet lambda publish command from the dotnet CLI, which has the [# Lambda CLI tools patch] installed, which creates a ZIP of your C# source code along with all NuGet dependencies as well as your own published DLL assemblies, and automatically uploads it to AWS Lambda using the runtime parameter dotnetcore1.0 "
830,How do I deploy AWS Lambda function code written in PowerShell?,"A PowerShell Lambda deployment package is a ZIP file that contains your PowerShell script, PowerShell modules that are required for your PowerShell script, and the assemblies needed to host PowerShell Core. You then use the AWSLambdaPSCore PowerShell module that you can install from the PowerShell Gallery to create your PowerShell Lambda deployment package. "
831,How do I package and deploy an AWS Lambda function in Go?,"Upload your Go executable artifact as a ZIP file through the AWS CLI or Lambda console and select the go1.x runtime. With Lambda, you can use Go's native tools to build and package your code. For more details, read our documentation. "
832,How do I deploy AWS Lambda function code written in Ruby?,"To deploy a Lambda function written in Ruby, package your Ruby code and gems as a ZIP. You can upload the ZIP from your local environment, or specify an Amazon S3 location where the ZIP file is located. "
833,"Which versions of Amazon Linux, Node.js, Python, JDK, .NET Core, SDKs, and additional libraries does AWS Lambda support?",You can view the list of supported versions here. 
834,Can I change the version of Amazon Linux or any language runtime?,No. AWS Lambda offers a single version of the operating system and managed language runtime to all users of the service. You can bring your own language runtime to use in Lambda. 
835,How can I record and audit calls made to the AWS Lambda API?,AWS Lambda is integrated with AWS CloudTrail. AWS CloudTrail can record and deliver log files to your Amazon S3 bucket describing the API usage of your account. 
836,How do I coordinate calls between multiple Lambda functions?,"You can use Amazon Step Functions to coordinate multiple invoking Lambda functions. You can invoke multiple Lambda functions serially, passing the output of one to the other, or in parallel. See our documentation for more details. "
837,Does AWS Lambda support Advanced Vector Extensions 2 (AVX2)?,"Yes, AWS Lambda supports the Advanced Vector Extensions 2 (AVX2) instruction set. To learn more about how to compile your application code to target this instruction set for improved performance, visit the AWS Lambda developer documentation. "
838,How can I set up a fully-managed relational database in the cloud?,"To set up a fully-managed relational database, use Amazon RDS. It automates database tasks such as patching, backups, and scaling."
839,I need a MySQL database for my application. Which AWS service should I choose?,"Choose Amazon RDS for MySQL. It provides a managed MySQL database with automated backups, high availability, and scalability."
840,What's the recommended way to achieve high availability for my PostgreSQL database?,"For high availability, use Amazon RDS Multi-AZ deployments. It automatically replicates your database to a standby instance in a different Availability Zone."
841,How can I easily scale my database to handle increased traffic?,Use Amazon RDS for automatic vertical scaling. It allows you to easily scale your database instance vertically by changing the DB instance class.
842,I want to migrate my existing database to the cloud. What AWS service supports database migration?,"For database migration, use AWS Database Migration Service (DMS). It supports migrating databases to and from most widely used commercial and open-source databases."
843,Which AWS service provides automated backups for my Microsoft SQL Server database?,"Amazon RDS for SQL Server provides automated backups, allowing you to easily restore your database to any point in time within your retention period."
844,How can I monitor the performance of my Amazon RDS database?,"Use Amazon CloudWatch for database monitoring. It provides metrics and alarms for RDS instances, helping you identify and react to performance issues."
845,I need a highly secure database solution. What AWS service offers encryption at rest for databases?,Amazon RDS provides encryption at rest for databases. It uses AWS Key Management Service (KMS) to encrypt your database storage.
846,I have a growing dataset. What's the best way to handle read scalability for my database?,Consider using Amazon Aurora. It is a MySQL and PostgreSQL-compatible relational database engine designed for performance and scalability.
847,How can I ensure my Amazon RDS database is compliant with regulatory requirements?,Amazon RDS supports various compliance certifications. Choose the appropriate configuration and encryption options to meet regulatory requirements.
848,What's the advantage of using Amazon RDS over managing my own database server?,"Amazon RDS simplifies database management by handling routine tasks, allowing you to focus on your application development."
849,I have unpredictable traffic patterns. How can I optimize costs for my Amazon RDS instance?,"Utilize Amazon RDS Multi-AZ deployments for high availability and consider Aurora Serverless for automatic, cost-efficient scaling based on demand."
850,I need to replicate my database for disaster recovery. Which service should I use?,Use Amazon RDS Read Replicas for database replication. They enhance read scalability and provide a disaster recovery option.
851,How can I automate database patching for my Amazon RDS instance?,"Amazon RDS provides automated software patching. You can define a maintenance window, and RDS will apply patches during that time."
852,I want to use PostgreSQL with advanced features. Which AWS service provides a PostgreSQL-compatible database with enhanced performance?,"Amazon Aurora for PostgreSQL offers enhanced performance and compatibility with PostgreSQL, including features like Aurora Global Databases."
853,How can I restore my Amazon RDS database to a specific point in time?,Amazon RDS supports point-in-time recovery. You can restore your database to any second during your specified retention period.
854,I need to run my Oracle Database in the cloud. What AWS service supports Oracle databases?,Amazon RDS for Oracle supports running Oracle Database in the cloud with features like automatic backups and patching.
855,What's the recommended backup strategy for my Amazon RDS database?,"Amazon RDS automatically takes daily backups. Additionally, you can enable automated backups to capture changes throughout the day."
856,How can I easily replicate my MySQL database across regions for improved performance?,Use Amazon RDS Cross-Region Read Replicas to replicate your MySQL database to a different AWS region.
857,I have a temporary burst in database traffic. What's the best way to handle it?,"Amazon RDS supports burstable performance instances, allowing your database to handle temporary increases in traffic."
858,I want to migrate my on-premises PostgreSQL database to the cloud. What AWS service should I use?,Use AWS Database Migration Service (DMS) to migrate your on-premises PostgreSQL database to Amazon RDS.
859,How can I implement read scalability for my MySQL database without impacting the primary instance?,"Use Amazon RDS Read Replicas to offload read traffic from the primary instance, improving read scalability."
860,I need a serverless database solution. What AWS service offers serverless capabilities for databases?,"Amazon Aurora Serverless offers serverless capabilities for MySQL and PostgreSQL databases, automatically adjusting capacity based on actual usage."
861,I have a time-sensitive workload. How can I optimize query performance for my Amazon RDS database?,"Amazon RDS Performance Insights provides a detailed view of database performance, helping you optimize queries and enhance performance."
862,I need to integrate my Amazon RDS database with other AWS services. How can I achieve this?,Use AWS Lambda for serverless computing to integrate your Amazon RDS database with other AWS services seamlessly.
863,What's the recommended way to handle failovers for my Amazon RDS instance?,"For automatic failover, use Amazon RDS Multi-AZ deployments. It provides a standby instance that automatically takes over in case of a failure."
864,How can I ensure data durability and reliability for my Amazon RDS database?,Amazon RDS automatically backs up your database and provides the option to store backups in Amazon S3 for long-term durability.
865,I need to run Microsoft SQL Server in the cloud. Which AWS service supports SQL Server databases?,Amazon RDS for SQL Server supports running Microsoft SQL Server databases in the cloud with automated backups and maintenance.
866,What's the advantage of using Amazon Aurora over traditional relational databases?,"Amazon Aurora offers greater performance, availability, and durability compared to traditional relational databases, with compatibility for MySQL and PostgreSQL."
867,How can I ensure my Amazon RDS database is secure against unauthorized access?,"Utilize Amazon RDS security features, such as Virtual Private Cloud (VPC) integration, security groups, and IAM roles, to secure your database."
868,I want to use a managed database that supports multiple database engines. What AWS service should I consider?,"Amazon RDS supports multiple database engines, including MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB, providing flexibility for different use cases."
869,How can I easily replicate my database for read scalability without impacting the primary instance's performance?,Amazon RDS Read Replicas allow you to replicate your database to enhance read scalability without affecting the performance of the primary instance.
870,I need to run a highly available database for my application. What's the recommended configuration?,Use Amazon RDS Multi-AZ deployments for high availability. It automatically replicates your database to a standby instance in a different Availability Zone.
871,How can I optimize costs for my Amazon RDS instance with varying usage patterns?,Consider using Amazon RDS On-Demand Instances for flexibility or Amazon RDS Reserved Instances for cost savings with a fixed commitment.
872,What's the advantage of using Amazon Aurora Serverless for databases with variable workloads?,"Amazon Aurora Serverless automatically adjusts capacity based on actual usage, making it ideal for databases with variable workloads."
873,How can I migrate my database from one AWS region to another?,"Use AWS Database Migration Service (DMS) to migrate your database from one region to another, ensuring minimal downtime."
874,I need to run a PostgreSQL database with improved performance and scalability. What AWS service should I choose?,Amazon RDS for PostgreSQL provides improved performance and scalability for PostgreSQL databases with features like Amazon Aurora.
875,How can I automate backups for my Amazon RDS instance and retain them for a specific duration?,Amazon RDS provides automated backups with a specified retention period. You can set the retention period based on your backup requirements.
876,I have a growing dataset. What's the best way to handle write scalability for my database?,Consider using Amazon Aurora for improved write scalability. It is designed to handle high-volume write operations efficiently.
877,How can I monitor and optimize the performance of my Amazon RDS instance?,"Utilize Amazon RDS Performance Insights for detailed performance monitoring, allowing you to identify and optimize database performance."
878,I need to run a database with minimal management overhead. What AWS service provides a serverless option?,"Amazon Aurora Serverless offers a serverless option, automatically adjusting capacity based on actual usage, reducing management overhead."
879,How can I implement a disaster recovery strategy for my Amazon RDS database?,Use Amazon RDS Multi-AZ deployments for automatic failover and consider creating cross-region Read Replicas for additional disaster recovery options.
880,I want to run a MariaDB database with enhanced performance. What AWS service should I choose?,"Amazon RDS for MariaDB provides enhanced performance for MariaDB databases, with features like automated backups and scaling."
881,How can I integrate my Amazon RDS database with other AWS services for data analytics?,"Use Amazon RDS with Amazon Redshift for seamless integration with data analytics services, enabling efficient data processing and analysis."
882,What's the recommended way to handle long-term data retention for my Amazon RDS database backups?,"Store your Amazon RDS backups in Amazon S3 for long-term data retention, ensuring durability and easy retrieval."
883,How can I ensure my Amazon RDS database is highly available across different geographic regions?,Utilize Amazon RDS Cross-Region Read Replicas to enhance availability by replicating your database to a different AWS region.
884,I have a variable workload with unpredictable traffic. How can I optimize costs for my database?,Consider using Amazon RDS On-Demand Instances for flexibility or Amazon RDS Aurora Serverless for automatic cost optimization based on actual usage.
885,How can I enhance security for my Amazon RDS database against unauthorized access?,"Implement security best practices such as VPC integration, security groups, and IAM roles to enhance security for your Amazon RDS database."
886,I need to run a database with minimal administrative effort. What's the recommended service?,Amazon RDS is the recommended service for running a fully-managed database with minimal administrative effort.
887,How can I ensure my Amazon RDS database is compliant with industry regulations?,Amazon RDS supports various compliance certifications. Configure your RDS instance to meet specific regulatory requirements for your industry.
888,What is Amazon RDS?,"Amazon Relational Database Service (Amazon RDS) is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity, while managing time-consuming database administration tasks, freeing you to focus on your applications and business. Amazon RDS gives you access to the capabilities of a familiar RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle, or RDS for Db2 database. This means that the code, applications, and tools you already use today with your existing databases should work seamlessly with Amazon RDS. Amazon RDS can automatically back up your database and keep your database software up to date with the latest version. You benefit from the flexibility of being able to scale the compute resources or storage capacity associated with your relational database instance. In addition, Amazon RDS makes it easy to use replication to enhance database availability, improve data durability, or scale beyond the capacity constraints of a single database instance for read-heavy database workloads. As with all AWS services, there are no upfront investments required, and you pay only for the resources you use. "
889,When would I use Amazon RDS vs. Amazon EC2 Relational Database AMIs?,Amazon Web Services provides a number of database alternatives for developers. Amazon RDS enables you to run a fully managed and fully featured relational database while offloading database administration. Using one of our many relational database AMIs on Amazon EC2 allows you to manage your own relational database in the cloud. There are important differences between these alternatives that may make one more appropriate for your use case. See Cloud Databases with AWS for guidance on which solution is best for you. 
890,Are there hybrid or on-premises deployment options for Amazon RDS?,"Yes, you can run Amazon RDS on premises using Amazon RDS on Outposts. Please see the Amazon RDS on Outposts FAQs for additional information. "
891,Can I get help to learn more about and onboard to Amazon RDS?,"Yes, Amazon RDS specialists are available to answer questions and provide support. Contact Us and you'll hear back from us in one business day to discuss how AWS can help your organization. "
892,How do I set up a connection between an application or a SQL based client running on an Amazon EC2 compute instance and my Amazon RDS database instance/cluster?,"You can set up a connection between an EC2 compute instance and a new Amazon RDS database using the Amazon RDS console. On the  Create database  page, select  Connect to an EC2 compute resource  option in the Connectivity Section. When you select this option, Amazon RDS automates the manual networking set up tasks such as creating a VPC, security groups, subnets, and ingress/egress rules to establish a connection between your application and database. Additionally, you can set up a connection between an existing Amazon RDS database and an EC2 compute instance. To do so, open the RDS console, select an RDS database from the database list page, and choose  Set up EC2 connection  from the Action menu dropdown list. Amazon RDS automatically sets up your related network settings to enable a secure connection between the selected EC2 instance and the RDS database. This connectivity automation improves productivity for new users and application developers. Users can now quickly and seamlessly connect an application or a client using SQL on an EC2 compute instance to an RDS database within minutes. "
893,How do I set up a connection between a serverless Lambda application and my Amazon RDS or Amazon Aurora database instance and/or cluster?,"You can set up a connection between an AWS Lambda function and an Amazon RDS or Amazon Aurora database from the Amazon RDS console. On the RDS console, select an RDS or Aurora database from the database list page, and choose  Set up Lambda connection  from the Action menu. Amazon RDS automatically sets up your related network settings to enable a secure connection between the selected Lambda function and the RDS or Aurora database. We recommend that you use RDS Proxy during this connection set up. You can set up this connection either by using an existing RDS Proxy or using a new RDS Proxy that you can auto create during the connection. This connectivity set up automation can improve productivity for new users and application developers. Users can now quickly and seamlessly connect a serverless application or Lambda function to an RDS or Aurora database within minutes. "
894,What is a database instance (DB instance)?,"You can think of a DB instance as a database environment in the cloud with the compute and storage resources you specify. You can create and delete DB instances, define/refine infrastructure attributes of your DB instance(s), and control access and security via the AWS Management Console, Amazon RDS APIs, and AWS Command Line Interface. You can run one or more DB instances and each DB instance can support one or more databases or database schemas, depending on engine type. "
895,How do I create a DB instance?,"DB instances are simple to create using either the AWS Management Console, Amazon RDS APIs, or AWS Command Line Interface. To launch a DB instance using the AWS Management Console, click RDS and then the  Launch DB Instance  button on the Instances tab. From there, you can specify the parameters for your DB instance, including DB engine and version, license model, instance type, storage type and amount, and primary user credentials. You also have the ability to change your DB instance's backup retention policy, preferred backup window, and scheduled maintenance window. Alternatively, you can create your DB instance using the CreateDBInstance API or create-db-instance command. "
896,How do I access my running DB instance?,"Once your DB instance is available, you can retrieve its endpoint via the DB instance description in the AWS Management Console, DescribeDBInstances API or describe-db-instances command. Using this endpoint, you can construct the connection string required to connect directly with your DB instance using your favorite database tool or programming language. In order to allow network requests to your running DB instance, you will need to authorize access. For a detailed explanation of how to construct your connection string and get started, please refer to our Getting Started Guide. "
897,How many DB instances can I run with Amazon RDS?,"By default, customers are allowed to have up to a total of 40 Amazon RDS DB instances. Of those 40, up to 10 can be RDS for Oracle or RDS for SQL Server DB instances under the License Included model. All 40 can be used for Amazon Aurora, RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, and RDS for Oracle under the Bring Your Own Licence (BYOL) model. Note that RDS for SQL Server has a limit of up to 100 databases on a single DB instance. To learn more, see the Amazon RDS for SQL Server User Guide. "
898,How many databases or schemas can I run within a DB instance?,RDS for Amazon Aurora: No limit imposed by software RDS for MySQL: No limit imposed by software RDS for MariaDB: No limit imposed by software RDS for Oracle: 1 database per instance; no limit on the number of schemas per database imposed by software RDS for SQL Server: Up to 100 databases per instance RDS for PostgreSQL: No limit imposed by software RDS for Db2: Up to 8 databases per instance 
899,How do I import data into an Amazon RDS DB instance?,"The following are a number of ways to import data into Amazon RDS: MySQL: mysqldump or mysqlimport utilities Oracle: Data Pump, import/export, or SQL Loader SQL Server: Import/Export Wizard, full backup files (.bak), or Bulk Copy Program (BCP) PostgreSQL: pg_dump For more information on data import and export, refer to the Data Import Guide for MySQL, the Data Import Guide for Oracle, the Data Import Guide for SQL Server, the Data Import Guide for PostgreSQL, or the Data Import Guide for Db2. In addition, AWS Database Migration Service can help you securely migrate databases to AWS. "
900,What is a maintenance window? Will my DB instance be available during maintenance events?,"The Amazon RDS maintenance window is your opportunity to control when DB instance modifications, database engine version upgrades, and software patching occurs, in the event they are requested or required. If a maintenance event is scheduled for a given week, it will be initiated during the maintenance window you identify. Maintenance events that require Amazon RDS to take your DB instance offline are scale compute operations (which generally take only a few minutes from start-to-finish), database engine version upgrades, and required software patching. Required software patching is automatically scheduled only for patches that are security and durability related. Such patching occurs infrequently (typically once every few months) and should seldom require more than a fraction of your maintenance window. If you do not specify a preferred weekly maintenance window when creating your DB instance, a 30-minute default value is assigned. If you wish to modify when maintenance is performed on your behalf, you can do so by modifying your DB instance in the AWS Management Console, the ModifyDBInstance API, or the modify-db-instance command. Each of your DB instances can have different preferred maintenance windows, if you so choose. Running your DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event. Please refer to the Amazon RDS User Guide for more information on maintenance operations. "
901,What should I do if my queries seem to be running slowly?,"For production databases, we encourage you to enable Enhanced Monitoring, which provides access to over 50 CPU, memory, file system, and disk I/O metrics. You can enable these features on a per-instance basis and you can choose the granularity (all the way down to 1 second). High levels of CPU utilization can reduce query performance and in this case, you may want to consider scaling your DB instance class. For more information on monitoring your DB instance, refer to the Amazon RDS User Guide. If you are using RDS for MySQL or MariaDB, you can access the slow query logs for your database to determine if there are slow-running SQL queries and, if so, the performance characteristics of each. You could set the slow_query_log DB Parameter and query the mysql.slow_log table to review the slow-running SQL queries. Please refer to the Amazon RDS User Guide to learn more. If you are using RDS for Oracle, you can use the Oracle trace file data to identify slow queries. For more information on accessing trace file data, please refer to Amazon RDS User Guide.  If you are using RDS for SQL Server, you can use the client side SQL Server traces to identify slow queries. For information on accessing server side trace file data, please refer to Amazon RDS User Guide. "
902,Which relational database engine versions does Amazon RDS support?,"For the list of supported database engine versions, please refer to the documentation for each engine: Amazon Aurora Amazon RDS for PostgreSQL Amazon RDS for MySQL Amazon RDS for MariaDB Amazon RDS for SQL Server Amazon RDS for Oracle Amazon RDS for Db2 "
903,How does Amazon RDS distinguish between  major  and  minor  DB engine versions?,Refer to the FAQs page for each Amazon RDS database engine for specifics on version numbering: Amazon RDS for MySQL Amazon RDS for MariaDB Amazon RDS for PostgreSQL Amazon RDS for Oracle Amazon RDS for SQL Server Amazon Aurora Amazon RDS for Db2 
904,Does Amazon RDS provide guidelines for support of new DB engine versions?,"Over time, Amazon RDS adds support for new major and minor database engine versions. The number of new versions supported will vary based on the frequency and content of releases and patches from the engine's vendor or development organization and the outcome of a thorough vetting of these releases and patches by our database engineering team. However, as a general guidance, we aim to support new engine versions within 5 months of their general availability. "
905,How do I specify which supported DB engine version I would like my DB instance to run?,You can specify any currently supported version (major and minor) when creating a new DB instance via the Launch DB Instance operation in the AWS Management Console or the CreateDBInstance API. Please note that not every database engine version is available in every AWS region. 
906,How do I control if and when the engine version of my DB instance is upgraded to new supported versions?,"Amazon RDS strives to keep your database instance up to date by providing you with newer versions of the supported database engines. After a new version of a database engine is released by the vendor or development organization, it is thoroughly tested by our database engineering team before it is made available in Amazon RDS. We recommend that you keep your database instance upgraded to the most current minor version as it will contain the latest security and functionality fixes. Unlike major version upgrades, minor version upgrades only include database changes that are backward-compatible with previous minor versions (of the same major version) of the database engine.  If a new minor version does not contain fixes that would benefit Amazon RDS customers, we may choose not to make it available in Amazon RDS. Soon after a new minor version is available in Amazon RDS, we will set it to be the preferred minor version for new DB instances.  To manually upgrade a database instance to a supported engine version, use the Modify DB Instance command on the AWS Management Console or the ModifyDBInstance API and set the DB Engine Version parameter to the desired version. By default, the upgrade will be applied during your next maintenance window. You can also choose to upgrade immediately by selecting the Apply Immediately option in the console API. If we determine that a new engine minor version contains significant bug fixes compared to a previously released minor version, we will schedule automatic upgrades for DB instances that have the Auto Minor Version Upgrade setting to  Yes . These upgrades will be scheduled to occur during customer-specified maintenance windows. We schedule them so you can plan around them because downtime is required to upgrade a DB engine version, even for Multi-AZ instances. If you wish to turn off automatic minor version upgrades you can do so by setting the Auto Minor Version Upgrade setting to  No . In the case of RDS for Oracle and RDS for SQL Server, if the upgrade to the next minor version requires a change to a different edition, then we may not schedule automatic upgrades even if you have enabled the Auto Minor Version Upgrade setting. The determination on whether to schedule automatic upgrades in such situations will be made on a case-by-case basis. Since major version upgrades involve some compatibility risk, they will not occur automatically and must be initiated by you (except in the case of major version deprecation, see below). For more information about upgrading a DB instance to a new DB engine version, refer to the Amazon RDS User Guide. "
907,Can I test my DB instance with a new version before upgrading?,"Yes. You can do so by creating a DB snapshot of your existing DB instance, restoring from the DB snapshot to create a new DB instance, and then initiating a version upgrade for the new DB instance. You can then experiment safely on the upgraded copy of your DB instance before deciding whether or not to upgrade your original DB instance. For more information about restoring a DB snapshot, refer to the Amazon RDS User Guide. "
908,Does Amazon RDS provide guidelines for deprecating database engine versions that are currently supported?,"We intend to support major version releases (e.g., MySQL 5.6, PostgreSQL 9.6) for at least 3 years after they are initially supported by Amazon RDS. We intend to support minor versions (e.g., MySQL 5.6.37, PostgreSQL 9.6.1) for at least 1 year after they are initially supported by Amazon RDS. Periodically, we will deprecate major or minor engine versions. Major versions are made available at least until the community end of life for the corresponding community version or the version is no longer receiving software fixes or security updates. For minor versions, this is when a minor version has significant bugs or security issues that have been resolved in a later minor version. While we strive to meet these guidelines, in some cases we may deprecate specific major or minor versions sooner, such as when there are security issues. In the unlikely event that such cases occur, Amazon RDS will automatically upgrade your database engine to address the issue. Specific circumstances may dictate different timelines depending on the issue being addressed. "
909,What happens when an Amazon RDS DB engine version is deprecated?,"When a minor version of a database engine is deprecated in Amazon RDS, we will provide a three (3) month period after the announcement before beginning automatic upgrades. At the end of this period, all instances still running the deprecated minor version will be scheduled for automatic upgrade to the latest supported minor version during their scheduled maintenance windows. When a major version of the database engine is deprecated in Amazon RDS, we will provide a minimum six (6) month period after the announcement of a deprecation for you to initiate an upgrade to a supported major version. At the end of this period, an automatic upgrade to the next major version will be applied to any instances still running the deprecated version during their scheduled maintenance windows. "
910,Why can I not create a particular version?,"In some cases, we may deprecate specific major or minor versions without prior notice, such as when we discover a version does not meet our high quality, performance, or security bar. In the unlikely event that such cases occur, Amazon RDS will discontinue the creation of new database instances and clusters with these versions. Existing customers may continue to be able to run their databases. Specific circumstances may dictate different timelines depending on the issue being addressed. "
911,How will I be charged and billed for my use of Amazon RDS?,"You pay only for what you use and there are no minimum or setup fees. You are billed based on: DB instance hours - Based on the class (e.g. db.t2.micro, db.m4.large) of the DB instance consumed. Partial DB instance hours consumed are billed in one-second increments with a 10 minute minimum charge following a billable status change, such as creating, starting, or modifying the DB instance class. For additional details, read our what's new announcement. Storage (per GB per month) - Storage capacity you have provisioned to your DB instance. If you scale your provisioned storage capacity within the month, your bill will be pro-rated. I/O requests per month - Total number of storage I/O requests you have (for Amazon RDS Magnetic Storage and Amazon Aurora only) Provisioned IOPS per month - Provisioned IOPS rate, regardless of IOPS consumed (for Amazon RDS Provisioned IOPS (SSD) Storage only) Backup Storage - Backup storage is the storage associated with your automated database backups and any customer-initiated database snapshots. Increasing your backup retention period or taking additional database snapshots increases the backup storage consumed by your database. Data transfer - Internet data transfer in and out of your DB instance. For Amazon RDS pricing information, please visit the pricing section on the Amazon RDS product page. "
912,When does billing of my Amazon RDS DB instances begin and end?,"Billing commences for a DB instance as soon as the DB instance is available. Billing continues until the DB instance terminates, which would occur upon deletion or in the event of an instance failure. "
913,What defines billable Amazon RDS instance hours?,"DB instance hours are billed for each hour your DB instance is running in an available state. If you no longer wish to be charged for your DB instance, you must stop or delete it to avoid being billed for additional instance hours. Partial DB instance hours consumed are billed in one-second increments with a 10 minute minimum charge following a billable status change, such as creating, starting, or modifying the DB instance class. "
914,How will I be billed for a stopped DB instance?,"While your database instance is stopped, you are charged for provisioned storage (including Provisioned IOPS) and backup storage (including manual snapshots and automated backups within your specified retention window), but not for DB instance hours. "
915,How will I be billed for backups storage?,"Free backup storage is provided up to your account's total provisioned database storage across the entire region. For example, if you have a MySQL DB instance with 100 GB of provisioned storage over the month, and a PostgreSQL DB instance with 150 GB of provisioned storage over the month, both in the same region and same account, we will provide 250 GB of backup storage in this account and region at no additional charge. You will only be charged for backup storage that exceeds this amount. Each day, your account's total provisioned database storage in the region is compared against your total backup storage in the region, and only the excess backup storage is charged. For example, if you have exactly 10 GB of excess backup storage each day, you will be charged for 10 GB-month of backup storage for the month. Alternatively, if you have 300 GB of provisioned storage each day, and 500 GB of backup storage each day, but only for half the month, then you will only be charged for 100 GB-month of backup storage (not 200 GB-month), since the charge is calculated daily (prorated), and the backups did not exist for the entire month. Please note that the free backup storage is account-specific and region-specific. The size of your backups are directly proportional to the amount of data on your instance. For example, if you have a DB instance with 100 GB of provisioned storage, but only store 5 GB of data on it, your first backup will only be approximately 5 GB (not 100 GB). Subsequent backups are incremental, and will only store the changed data on your DB instance. Please note that the backup storage size is not displayed in RDS Console nor in the DescribeDBSnapshots API response. "
916,Why does my additional backup storage cost more than the allocated DB instance storage?,"The storage provisioned to your DB instance for your primary data is located within a single Availability Zone. When your database is backed up, the backup data (including transactions logs) is geo-redundantly replicated across multiple Availability Zones to provide even greater levels of data durability. The price for backup storage beyond your free allocation reflects this extra replication that occurs to maximize the durability of your critical backups. "
917,How will I be billed for Multi-AZ DB instance deployments?,"If you specify that your DB instance should be a Multi-AZ deployment, you will be billed according to the Multi-AZ pricing posted on the Amazon RDS pricing page. Multi-AZ billing is based on: Multi-AZ DB instance hours - Based on the class (e.g. db.t2.micro, db.m4.large) of the DB instance consumed. As with standard deployments in a single Availability Zone, Partial DB instance hours consumed are billed in one-second increments with a 10 minutes minimum charge following a billable status change, such as creating, starting, or modifying the DB instance class. If you convert your DB instance deployment between standard and Multi-AZ within a given hour, you will be charged both applicable rates for that hour. Provisioned storage (for Multi-AZ DB instance) - If you convert your deployment between standard and Multi-AZ within a given hour, you will be charged the higher of the applicable storage rates for that hour. I/O requests per month - Total number of storage I/O requests you have. Multi-AZ deployments consume a larger volume of I/O requests than standard DB instance deployments, depending on your database write/read ratio. Write I/O usage associated with database updates will double as Amazon RDS synchronously replicates your data to the standby DB instance. Read I/O usage will remain the same. Backup Storage - Your backup storage usage will not change whether your DB instance is a standard or Multi-AZ deployment. Backups will simply be taken from your standby to avoid I/O suspension on the DB instance primary. Data transfer - You are not charged for the data transfer incurred in replicating data between your primary and standby. Internet data transfer in and out of your DB instance is charged the same as with a standard deployment. "
918,Do your prices include taxes?,"Except as otherwise noted, our prices are exclusive of applicable taxes and duties, including VAT and applicable sales tax. For customers with a Japanese billing address, the use of AWS services is subject to Japanese Consumption Tax. Learn more. "
919,What does the AWS Free Tier for Amazon RDS offer?,"The AWS Free Tier for Amazon RDS offer provides free use of Single-AZ Micro DB instances running MySQL, MariaDB, PostgreSQL, and SQL Server Express Edition. The free usage tier is capped at 750 instance hours per month. Customers also receive 20 GB of General Purpose (SSD) database storage and 20 GB of backup storage for free per month. "
920,For what time period will the AWS Free Tier for Amazon RDS be available to me?,New AWS accounts receive 12 months of AWS Free Tier access. Please see the AWS Free Tier FAQs for more information. 
921,Can I run more than one DB instance under the AWS Free Usage Tier for Amazon RDS?,"Yes. You can run more than one Single-AZ Micro DB instance simultaneously and be eligible for usage counted under the AWS Free Tier for Amazon RDS. However, any use exceeding 750 instance hours, across all Amazon RDS Single-AZ Micro DB instances and across all eligible database engines and regions, will be billed at standard Amazon RDS prices. For example, if you run two Single-AZ Micro DB instances for 400 hours each in a single month, you will accumulate 800 instance hours of usage, of which 750 hours will be free. You will be billed for the remaining 50 hours at the standard Amazon RDS price. "
922,"Do I have access to 750 instance hours each of the MySQL, MariaDB, PostgreSQL, and SQL Server Micro DB instances under the AWS Free Tier?","No. A customer with access to the AWS Free Tier can use up to 750 instance hours of Micro instances running either MySQL, PostgreSQL, or SQL Server Express Edition. Any use exceeding 750 instance hours, across all Amazon RDS Single-AZ Micro DB instances and across all eligible database engines and regions, will be billed at standard Amazon RDS prices. "
923,How am I billed when my instance-hour usage exceeds the Free Tier benefit?,You are billed at standard Amazon RDS prices for instance hours beyond what the Free Tier provides. See the Amazon RDS pricing page for details. 
924,What is a reserved instance (RI)?,"Amazon RDS reserved instances give you the option to reserve a DB instance for a one or three year term and in turn receive a significant discount compared to the on-demand instance pricing for the DB instance. There are three RI payment options -- No Upfront, Partial Upfront, All Upfront -- which enable you to balance the amount you pay upfront with your effective hourly price. "
925,How are reserved instances different from on-demand DB instances?,"Functionally, reserved instances and on-demand DB instances are exactly the same. The only difference is how your DB instance(s) are billed. With Reserved Instances, you purchase a one- or three-year reservation and in return receive a lower effective hourly usage rate (compared with on-demand DB instances) for the duration of the term. Unless you purchase reserved instances in a Region, all DB instances will be billed at on-demand hourly rates. "
926,How do I purchase and create reserved instances?,"You can purchase a reserved instance in the Reserved Instance section of the AWS Management Console for Amazon RDS. Alternatively, you can use the Amazon RDS API or AWS Command Line Interface to list the reservations available for purchase and then purchase a DB instance reservation. Once you have made a reserved purchase, using a reserved DB instance is no different than an On-Demand DB instance. Launch a DB instance using the same instance class, engine, and region for which you made the reservation. As long as your reservation purchase is active, Amazon RDS will apply the reduced hourly rate for which you are eligible to the new DB instance. "
927,Do reserved instances include a capacity reservation?,"Amazon RDS reserved instances are purchased for a Region rather than for a specific Availability Zone. As RIs are not specific to an Availability Zone, they are not capacity reservations. This means that even if capacity is limited in one Availability Zone, reservations can still be purchased in the Region and the discount will apply to matching usage in any Availability Zone within that Region. "
928,How many reserved instances can I purchase?,"You can purchase up to 40 reserved DB instances. If you wish to run more than 40 DB instances, please complete the Amazon RDS DB Instance request form. "
929,What if I have an existing DB instance that I'd like to cover with a reserved instance?,"Simply purchase a DB instance reservation with the same DB instance class, DB engine, Multi-AZ option, and License Model within the same Region as the DB instance you are currently running and would like to reserve. If the reservation purchase is successful, Amazon RDS will automatically apply your new hourly usage charge to your existing DB instance. "
930,"If I sign up for a reserved instance, when does the term begin? What happens to my DB instance when the term ends?","Pricing changes associated with a reserved instance are activated once your request is received while the payment authorization is processed. You can follow the status of your reservation on the AWS Account Activity page or by using the DescribeReservedDBInstances API or describe-reserved-db-instances command. If the one-time payment cannot be successfully authorized by the next billing period, the discounted price will not take effect. When your reservation term expires, your reserved instance will revert to the appropriate On-Demand hourly usage rate for your DB instance class and Region. "
931,How do I control which DB instances are billed at the reserved instance rate?,"The Amazon RDS operations for creating, modifying, and deleting DB instances do not distinguish between on-demand and reserved instances. When computing your bill, our system will automatically apply your Reservation(s) such that all eligible DB instances are charged at the lower hourly reserved DB instance rate. "
932,"If I scale my DB instance class up or down, what happens to my reservation?","Each reservation is associated with the following set of attributes: DB engine, DB instance class, Multi-AZ deployment option, license model, and Region. A reservation for a DB engine and license model that is eligible for size-flexibility (MySQL, MariaDB, PostgreSQL, Amazon Aurora, or Oracle Bring Your Own License) will automatically apply to a running DB instance of any size within the same instance family (e.g. M4, T2, or R3) for the same database engine and Region. In addition, the reservation will also apply to DB instances running in either Single-AZ or Multi-AZ deployment options. For example, let's say you purchased a db.m4.2xlarge MySQL reservation. If you decide to scale up the running DB instance to a db.m4.4xlarge, the discounted rate of this RI will cover 1/2 of the usage of the larger DB instance. If you are running a DB engine or license model that is not eligible for size-flexibility (Microsoft SQL Server or Oracle License Included), each reservation can only be applied to a DB instance with the same attributes for the duration of the term. If you decide to modify any of these attributes of your running DB instance before the end of the reservation term, your hourly usage rates for that DB instance will revert to on demand hourly rates. For more details on about size flexibility, see the Amazon RDS User Guide. "
933,Can I move a reserved instance from one Region or Availability Zone to another?,"Each reserved instance is associated with a specific Region, which is fixed for the lifetime of the reservation and cannot be changed. Each reservation can, however, be used in any of the available AZs within the associated Region. "
934,Are reserved instances available for Multi-AZ deployments?,"Yes. When you purchase a reserved instance, you can select the Multi-AZ option in the DB instance configuration available for purchase. In addition, if you are using a DB engine and license model that supports reserved instance size-flexibility, a Multi-AZ reserved instance will cover usage for two Single-AZ DB instances. "
935,Are reserved instances available for read replicas?,"A DB instance reservation can be applied to a read replica, provided the DB instance class and Region are the same. When computing your bill, our system will automatically apply your Reservation(s), such that all eligible DB instances are charged at the lower hourly reserved instance rate. "
936,Can I cancel a reservation?,"No, you cannot cancel your reserved DB instance and the one-time payment (if applicable) is not refundable. You will continue to pay for every hour during your Reserved DB instance term regardless of your usage. "
937,How do the payment options impact my bill?,"When you purchase an RI under the All Upfront payment option, you pay for the entire term of the RI in one upfront payment. You can choose to pay nothing upfront by choosing the No Upfront option. The entire value of the No Upfront RI is spread across every hour in the term and you will be billed for every hour in the term, regardless of usage. The Partial Upfront payment option is a hybrid of the All Upfront and No Upfront options. You make a small upfront payment, and you are billed a low hourly rate for every hour in the term regardless of usage. "
938,How do I determine which initial DB instance class and storage capacity are appropriate for my needs?,"In order to select your initial DB instance class and storage capacity, you will want to assess your application's compute, memory, and storage needs. For information about the DB instance classes available, please refer to the Amazon RDS User Guide. "
939,How do I scale the compute resources and/or storage capacity associated with my Amazon RDS Database Instance?,"You can scale the compute resources and storage capacity allocated to your DB instance with the AWS Management Console (selecting the desired DB instance and clicking the Modify button), the Amazon RDS API, or the AWS Command Line Interface. Memory and CPU resources are modified by changing your DB Instance class, and storage available is changed when you modify your storage allocation.  Please note that when you modify your DB Instance class or allocated storage, your requested changes will be applied during your specified maintenance window. Alternately, you can use the  apply-immediately  flag to apply your scaling requests immediately. Bear in mind that any other pending system changes will be applied as well. Some older RDS for SQL Server instances may not be eligible for scaled storage. See the RDS for SQL Server FAQ for more information. "
940,What is the hardware configuration for Amazon RDS storage?,"Amazon RDS uses EBS volumes for database and log storage. Depending on the size of storage requested, Amazon RDS automatically stripes across multiple EBS volumes to enhance IOPS performance. For MySQL and Oracle, for an existing DB instance, you may observe some I/O capacity improvement if you scale up your storage. You can scale the storage capacity allocated to your DB Instance using the AWS Management Console, the ModifyDBInstance API, or the modify-db-instance command. For more information, see Storage for Amazon RDS. "
941,Will my DB instance remain available during scaling?,"The storage capacity allocated to your DB Instance can be increased while maintaining DB Instance availability. However, when you decide to scale the compute resources available to your DB instance up or down, your database will be temporarily unavailable while the DB instance class is modified. This period of unavailability typically lasts only a few minutes, and will occur during the maintenance window for your DB Instance, unless you specify that the modification should be applied immediately. "
942,How can I scale my DB instance beyond the largest DB instance class and maximum storage capacity?,"Amazon RDS supports a variety of DB instance classes and storage allocations to meet different application needs. If your application requires more compute resources than the largest DB instance class or more storage than the maximum allocation, you can implement partitioning, thereby spreading your data across multiple DB instances. "
943,What is Amazon RDS General Purpose (SSD) storage?,"Amazon RDS General Purpose (SSD) Storage is suitable for a broad range of database workloads that have moderate I/O requirements. With the baseline of 3 IOPS/GB and ability to burst up to 3,000 IOPS, this storage option provides predictable performance to meet the needs of most applications. "
944,What is Amazon RDS Provisioned IOPS (SSD) storage?,"Amazon RDS Provisioned IOPS (SSD) Storage is an SSD-backed storage option designed to deliver fast, predictable, and consistent I/O performance. With Amazon RDS Provisioned IOPS (SSD) Storage, you specify an IOPS rate when creating a DB instance, and Amazon RDS provisions that IOPS rate for the lifetime of the DB instance. Amazon RDS Provisioned IOPS (SSD) Storage is optimized for I/O-intensive, transactional (OLTP) database workloads. For more details, please see the Amazon RDS User Guide. "
945,What is Amazon RDS magnetic storage?,Amazon RDS magnetic storage is useful for small database workloads where data is accessed less frequently. Magnetic storage is not recommended for production database instances. 
946,How do I choose among the Amazon RDS storage types?,Choose the storage type most suited for your workload. High-performance OLTP workloads: Amazon RDS Provisioned IOPS (SSD) Storage Database workloads with moderate I/O requirements: Amazon RDS General Purpose (SSD) Storage 
947,What are the minimum and maximum IOPS supported by Amazon RDS?,"The IOPS supported by Amazon RDS varies by database engine. For more details, please see the Amazon RDS User Guide. "
948,What is the difference between automated backups and DB Snapshots?,"Amazon RDS provides two different methods for backing up and restoring your DB instance(s) automated backups and database snapshots (DB Snapshots). The automated backup feature of Amazon RDS enables point-in-time recovery of your DB instance. When automated backups are turned on for your DB Instance, Amazon RDS automatically performs a full daily snapshot of your data (during your preferred backup window) and captures transaction logs (as updates to your DB Instance are made). When you initiate a point-in-time recovery, transaction logs are applied to the most appropriate daily backup in order to restore your DB instance to the specific time you requested.  Amazon RDS retains backups of a DB Instance for a limited, user-specified period of time called the retention period, which by default is 7 days but can be set to up to 35 days. You can initiate a point-in-time restore and specify any second during your retention period, up to the Latest Restorable Time. You can use the DescribeDBInstances API to return the latest restorable time for you DB instance, which is typically within the last five minutes.  Alternatively, you can find the Latest Restorable Time for a DB instance by selecting it in the AWS Management Console and looking in the  Description  tab in the lower panel of the Console. DB Snapshots are user-initiated and enable you to back up your DB instance in a known state as frequently as you wish, and then restore to that specific state at any time. DB Snapshots can be created with the AWS Management Console, CreateDBSnapshot API, or create-db-snapshot command and are kept until you explicitly delete them. The snapshots which Amazon RDS performs for enabling automated backups are available to you for copying (using the AWS console or the copy-db-snapshot command) or for the snapshot restore functionality. You can identify them using the automated Snapshot Type. In addition, you can identify the time at which the snapshot has been taken by viewing the Snapshot Created Time field.  Alternatively, the identifier of the automated snapshots also contains the time (in UTC) at which the snapshot has been taken. Please note: When you perform a restore operation to a point in time or from a DB Snapshot, a new DB Instance is created with a new endpoint (the old DB Instance can be deleted if so desired). This is done to enable you to create multiple DB Instances from a specific DB Snapshot or point in time. "
949,Do I need to enable backups for my DB Instance or is it done automatically?,"By default, Amazon RDS enables automated backups of your DB instance with a 7-day retention period. If you would like to modify your backup retention period, you can do so using the RDS Console, the CreateDBInstance API (when creating a new DB Instance), or the ModifyDBInstance API (for existing instances). You can use these methods to change the RetentionPeriod parameter to any number from 0 (which will disable automated backups) to the desired number of days, up to 35. The value cannot be set to 0 if the DB instance is a source to Read Replicas. For more information on automated backups, please refer to the Amazon RDS User Guide. "
950,What is a backup window and why do I need it? Is my database available during the backup window?,"The preferred backup window is the user-defined period of time during which your DB Instance is backed up. Amazon RDS uses these periodic data backups in conjunction with your transaction logs to enable you to restore your DB Instance to any second during your retention period, up to the LatestRestorableTime (typically up to the last few minutes). During the backup window, storage I/O may be briefly suspended while the backup process initializes (typically under a few seconds) and you may experience a brief period of elevated latency. There is no I/O suspension for Multi-AZ DB deployments, since the backup is taken from the standby. "
951,Where are my automated backups and DB snapshots stored and how do I manage their retention?,"Amazon RDS DB snapshots and automated backups are stored in S3. You can use the AWS Management Console, the ModifyDBInstance API, or the modify-db-instance command to manage the period of time your automated backups are retained by modifying the RetentionPeriod parameter. If you desire to turn off automated backups altogether, you can do so by setting the retention period to 0 (not recommended). You can manage your user-created DB Snapshots via the Snapshots section of the Amazon RDS Console. Alternatively, you can see a list of the user-created DB Snapshots for a given DB Instance using the DescribeDBSnapshots API or describe-db-snapshots command and delete snapshots with the DeleteDBSnapshot API or delete-db-snapshot command. "
952,Why do I have more automated DB snapshots than the number of days in the retention period for my DB instance?,"It is normal to have 1 or 2 more automated DB snapshots than the number of days in your retention period. One extra automated snapshot is retained to ensure the ability to perform a point in time restore to any time during the retention period. For example, if your backup window is set to 1 day, you will require 2 automated snapshots to support restores to any within the previous 24 hours. You may also see an additional automated snapshot as a new automated snapshot is always created before the oldest automated snapshot is deleted. "
953,What happens to my backups and DB snapshots if I delete my DB instance?,"When you delete a DB instance, you can create a final DB snapshot upon deletion; if you do, you can use this DB snapshot to restore the deleted DB instance at a later date. Amazon RDS retains this final user-created DB snapshot along with all other manually created DB snapshots after the DB instance is deleted. Refer to the pricing page for details of backup storage costs. Automated backups are deleted when the DB instance is deleted. Only manually created DB Snapshots are retained after the DB Instance is deleted. "
954,What is Amazon Virtual Private Cloud (VPC) and how does it work with Amazon RDS?,"Amazon VPC lets you create a virtual networking environment in a private, isolated section of the AWS cloud where you can exercise complete control over aspects, such as private IP address ranges, subnets, routing tables, and network gateways. With Amazon VPC, you can define a virtual network topology and customize the network configuration to closely resemble a traditional IP network that you might operate in your own data center. One way that you can take advantage of VPC is when you want to run a public-facing web application while still maintaining non-publicly accessible backend servers in a private subnet. You can create a public-facing subnet for your webservers that has access to the Internet, and place your backend Amazon RDS DB Instances in a private-facing subnet with no Internet access. For more information about Amazon VPC, refer to the Amazon Virtual Private Cloud User Guide. "
955,How is using Amazon RDS inside a VPC different from using it on the EC2-Classic platform (non-VPC)?,"If your AWS account was created before 2013-12-04, you may be able to run Amazon RDS in an Amazon Elastic Compute Cloud (EC2)-Classic environment. The basic functionality of Amazon RDS is the same regardless of whether EC2-Classic or EC2-VPC is used. Amazon RDS manages backups, software patching, automatic failure detection, read replicas, and recovery whether your DB Instances are deployed inside or outside a VPC. For more information about the differences between EC2-Classic and EC2-VPC, see the EC2 documentation. "
956,What is a DB Subnet Group and why do I need one?,"A DB Subnet Group is a collection of subnets that you may want to designate for your Amazon RDS DB Instances in a VPC. Each DB Subnet Group should have at least one subnet for every Availability Zone in a given Region. When creating a DB Instance in VPC, you will need to select a DB Subnet Group. Amazon RDS then uses that DB Subnet Group and your preferred Availability Zone to select a subnet and an IP address within that subnet. Amazon RDS creates and associates an Elastic Network Interface to your DB Instance with that IP address. Please note that we strongly recommend you use the DNS Name to connect to your DB Instance as the underlying IP address can change (e.g., during failover). For Multi-AZ deployments, defining a subnet for all Availability Zones in a Region will allow Amazon RDS to create a new standby in another Availability Zone should the need arise. You need to do this even for Single-AZ deployments, just in case you want to convert them to Multi-AZ deployments at some point. "
957,How do I create an Amazon RDS DB Instance in VPC?,"For a procedure that walks you through this process, refer to Creating a DB Instance in a VPC in the Amazon RDS User Guide. "
958,How do I control network access to my DB Instance(s)?,Visit the Security Groups section of the Amazon RDS User Guide to learn about the different ways to control access to your DB Instances. 
959,How do I connect to an Amazon RDS DB Instance in VPC?,"DB Instances deployed within a VPC can be accessed by EC2 Instances deployed in the same VPC. If these EC2 Instances are deployed in a public subnet with associated Elastic IPs, you can access the EC2 Instances via the internet. DB Instances deployed within a VPC can be accessed from the Internet or from EC2 Instances outside the VPC via VPN or bastion hosts that you can launch in your public subnet or using Amazon RDS's Publicly Accessible option: To use a bastion host, you will need to set up a public subnet with an EC2 instance that acts as a SSH Bastion. This public subnet must have an internet gateway and routing rules that allow traffic to be directed via the SSH host, which must then forward requests to the private IP address of your Amazon RDS DB instance. To use public connectivity, simply create your DB Instances with the Publicly Accessible option set to yes. With Publicly Accessible active, your DB Instances within a VPC will be fully accessible outside your VPC by default. This means you do not need to configure a VPN or bastion host to allow access to your instances. You can also set up a VPN Gateway that extends your corporate network into your VPC and allows access to the Amazon RDS DB instance in that VPC. Refer to the Amazon VPC User Guide for more details. We strongly recommend you use the DNS Name to connect to your DB Instance as the underlying IP address can change (e.g., during failover). "
960,Can I move my existing DB instances outside VPC into my VPC?,"If your DB instance is not in a VPC, you can use the AWS Management Console to easily move your DB instance into a VPC. See the Amazon RDS User Guide for more details. You can also take a snapshot of your DB Instance outside VPC and restore it to VPC by specifying the DB Subnet Group you want to use. Alternatively, you can perform a  Restore to Point in Time  operation as well. "
961,Can I move my existing DB instances from inside VPC to outside VPC?,"Migration of DB Instances from inside to outside VPC is not supported. For security reasons, a DB Snapshot of a DB Instance inside VPC cannot be restored to outside VPC. The same is true with  Restore to Point in Time  functionality. "
962,What precautions should I take to ensure that my DB Instances in VPC are accessible by my application?,"You are responsible for modifying routing tables and networking ACLs in your VPC to ensure that your DB instance is reachable from your client instances in the VPC. For Multi-AZ deployments, after failover, your client EC2 instance and Amazon RDS DB Instance may be in different Availability Zones. You should configure your networking ACLs to ensure that cross-AZ communication is possible. "
963,Can I change the DB Subnet Group of my DB Instance?,"An existing DB Subnet Group can be updated to add more subnets, either for existing Availability Zones or for new Availability Zones added since the creation of the DB Instance. Removing subnets from an existing DB Subnet Group can cause unavailability for instances if they are running in a particular AZ that gets removed from the subnet group. View the Amazon RDS User Guide for more information. "
964,What is an Amazon RDS primary user account and how is it different from an AWS account?,"To begin using Amazon RDS you will need an AWS developer account. If you do not have one prior to signing up for Amazon RDS, you will be prompted to create one when you begin the sign-up process. A primary user account is different from an AWS developer account and used only within the context of Amazon RDS to control access to your DB Instance(s). The primary user account is a native database user account that you can use to connect to your DB Instance.  You can specify the primary user name and password you want associated with each DB Instance when you create the DB Instance. Once you have created your DB Instance, you can connect to the database using the primary user credentials. Subsequently, you may also want to create additional user accounts so that you can restrict who can access your DB Instance. "
965,What privileges are granted to the primary user for my DB Instance?,"For MySQL, the default privileges for the primary user include: create, drop, references, event, alter, delete, index, insert, select, update, create temporary tables, lock tables, trigger, create view, show view, alter routine, create routine, execute, trigger, create user, process, show databases, grant option. For Oracle, the primary user is granted the dba role. The primary user inherits most of the privileges associated with the role. Please refer to the Amazon RDS User Guide for the list of restricted privileges and the corresponding alternatives to perform administrative tasks that may require these privileges. For SQL Server, a user that creates a database is granted the db_owner role. Please refer to the Amazon RDS User Guide for the list of restricted privileges and the corresponding alternatives to perform administrative tasks that may require these privileges. "
966,Is there anything different about user management with Amazon RDS?,"No, everything works the way you are familiar with when using a relational database you manage yourself. "
967,Can programs running on servers in my own data center access Amazon RDS databases?,"Yes. You have to intentionally turn on the ability to access your database over the internet by configuring Security Groups. You can authorize access for only the specific IPs, IP ranges, or subnets corresponding to servers in your own data center. "
968,Can I encrypt connections between my application and my DB Instance using SSL/TLS?,"Yes, this option is supported for all Amazon RDS engines. Amazon RDS generates an SSL/TLS certificate for each DB Instance. Once an encrypted connection is established, data transferred between the DB Instance and your application will be encrypted during transfer. While SSL offers security benefits, be aware that SSL/TLS encryption is a compute-intensive operation and will increase the latency of your database connection. SSL/TLS support within Amazon RDS is for encrypting the connection between your application and your DB Instance; it should not be relied on for authenticating the DB Instance itself. For details on establishing an encrypted connection with Amazon RDS, please visit Amazon RDS's MySQL User Guide, MariaDB User Guide, PostgreSQL User Guide, or Oracle User Guide. To learn more about how SSL/TLS works with these engines, you can refer directly to the MySQL documentation, the MariaDB documentation, the MSDN SQL Server documentation, the PostgreSQL documentation, or the Oracle Documentation. "
969,Can I encrypt data at rest on my Amazon RDS databases?,"Amazon RDS supports encryption at rest for all database engines, using keys you manage using AWS Key Management Service (KMS). On a database instance running with Amazon RDS encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots. Encryption and decryption are handled transparently. For more information about the use of KMS with Amazon RDS, see the Amazon RDS User's Guide. You can also add encryption to a previously unencrypted DB instance or DB cluster by creating a DB snapshot and then creating a copy of that snapshot and specifying a KMS encryption key. You can then restore an encrypted DB instance or DB cluster from the encrypted snapshot. Amazon RDS for Oracle and SQL Server support those engines' Transparent Data Encryption (TDE) technologies. For more information, see the Amazon RDS User's Guide for Oracle and SQL Server. "
970,How do I control the actions that my systems and users can take on specific Amazon RDS resources?,"You can control the actions that your AWS IAM users and groups can take on Amazon RDS resources. You do this by referencing the Amazon RDS resources in the AWS IAM policies that you apply to your users and groups. Amazon RDS resources that can be referenced in an AWS IAM policy include DB instances, DB snapshots, read replicas, DB security groups, DB option groups, DB parameter groups, event subscriptions, and DB subnet groups.  In addition, you can tag these resources to add additional metadata to your resources. By using tagging, you can categorize your resources (e.g. Development DB instances, Production DB instances, and Test DB instances), and write AWS IAM policies that list the permissions (i.e. actions) that can be taken on resources with the same tags. For more information, refer to Tagging Amazon RDS Resources. "
971,I wish to perform security analysis or operational troubleshooting on my Amazon RDS deployment. Can I get a history of all Amazon RDS API calls made on my account?,"Yes. AWS CloudTrail is a web service that records AWS API calls for your account and delivers log files to you. The AWS API call history produced by CloudTrail enables security analysis, resource change tracking, and compliance auditing. "
972,Can I use Amazon RDS with applications that require HIPAA compliance?,"Yes, all Amazon RDS database engines are HIPAA-eligible, so you can use them to build HIPAA-compliant applications and store healthcare-related information, including protected health information (PHI) under an executed Business Associate Agreement (BAA) with AWS. If you already have an executed BAA, no action is necessary to begin using these services in the account(s) covered by your BAA. If you do not have an executed BAA with AWS, or have any other questions about HIPAA-compliant applications on AWS, please contact your account manager. "
973,How do I choose the right configuration parameters for my DB Instance(s)?,"By default, Amazon RDS chooses the optimal configuration parameters for your DB Instance taking into account the instance class and storage capacity. However, if you want to change them, you can do so using the AWS Management Console, the Amazon RDS APIs, or the AWS Command Line Interface. Please note that changing configuration parameters from recommended values can have unintended effects, ranging from degraded performance to system crashes, and should only be attempted by advanced users who wish to assume these risks. "
974,What are DB Parameter groups? How are they helpful?,"A database parameter group (DB Parameter Group) acts as a  container  for engine configuration values that can be applied to one or more DB Instances. If you create a DB Instance without specifying a DB Parameter Group, a default DB Parameter Group is used. This default group contains engine defaults and Amazon RDS system defaults optimized for the DB Instance you are running. However, if you want your DB Instance to run with your custom-specified engine configuration values, you can simply create a new DB Parameter Group, modify the desired parameters, and modify the DB Instance to use the new DB Parameter Group. Once associated, all DB Instances that use a particular DB Parameter Group get all the parameter updates to that DB Parameter Group. For more information on configuring DB Parameter Groups, please read the Amazon RDS User Guide. "
975,How can I monitor the configuration of my Amazon RDS resources?,"You can use AWS Config to continuously record configuration changes to Amazon RDS DB Instances, DB Subnet Groups, DB Snapshots, DB Security Groups, and Event Subscriptions and receive notification of changes through Amazon Simple Notification Service (SNS). You can also create AWS Config Rules to evaluate whether these Amazon RDS resources have the desired configurations. "
976,What does it mean to run a DB instance as a Multi-AZ deployment?,"When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous  standby  replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.  During certain types of planned maintenance, or in the unlikely event of DB instance failure or Availability Zone failure, Amazon RDS will automatically failover to the standby so that you can resume database writes and reads as soon as the standby is promoted. Since the name record for your DB instance remains the same, your application can resume database operation without the need for manual administrative intervention. With Multi-AZ deployments, replication is transparent. You do not interact directly with the standby, and it cannot be used to serve read traffic. More information about Multi-AZ deployments is in the Amazon RDS User Guide. "
977,What is an Availability Zone?,"Availability Zones are distinct locations within a Region that are engineered to be isolated from failures in other Availability Zones. Each Availability Zone runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. Common points of failures like generators and cooling equipment are not shared across Availability Zones. Additionally, they are physically separate, such that even extremely uncommon disasters such as fires, tornados, or flooding would only affect a single Availability Zone. Availability Zones within the same Region benefit from low-latency network connectivity. "
978,What do  primary  and  standby  mean in the context of a Multi-AZ deployment?,"When you run a DB instance as a Multi-AZ deployment, the  primary  serves database writes and reads. In addition, Amazon RDS provisions and maintains a  standby  behind the scenes, which is an up-to-date replica of the primary. The standby is  promoted  in failover scenarios. After failover, the standby becomes the primary and accepts your database operations. You do not interact directly with the standby (e.g. for read operations) at any point prior to promotion. If you are interested in scaling read traffic beyond the capacity constraints of a single DB instance, please see the FAQs on Read Replicas. "
979,What are the benefits of a Multi-AZ deployment?,"The chief benefits of running your DB instance as a Multi-AZ deployment are enhanced database durability and availability. The increased availability and fault tolerance offered by Multi-AZ deployments make them a natural fit for production environments.   Running your DB instance as a Multi-AZ deployment safeguards your data in the unlikely event of a DB instance component failure or loss of availability in one Availability Zone. For example, if a storage volume on your primary fails, Amazon RDS automatically initiates a failover to the standby, where all of your database updates are intact. This provides additional data durability relative to standard deployments in a single AZ, where a user-initiated restore operation would be required and updates that occurred after the latest restorable time (typically within the last five minutes) would not be available.  You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete. The availability benefits of Multi-AZ also extend to planned maintenance.  For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby. In the case of patching or DB instance class scaling, these operations occur first on the standby, prior to automatic failover. As a result, your availability impact is limited to the time required for automatic failover to complete.  Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery (via the RestoreDBInstanceToPointInTime or RestoreDBInstanceFromSnapshot APIs) in the event of an Availability Zone failure or DB instance failure. "
980,Are there any performance implications of running my DB instance as a Multi-AZ deployment?,You may observe elevated latencies relative to a standard DB instance deployment in a single Availability Zone as a result of the synchronous data replication performed on your behalf. 
981,"When running my DB instance as a Multi-AZ deployment, can I use the standby for read or write operations?","No, a Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. Our implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations. If you are interested in a read scaling solution, please see the FAQs on Read Replicas. "
982,How do I set up a Multi-AZ DB instance deployment?,"In order to create a Multi-AZ DB instance deployment, simply click the  Yes  option for  Multi-AZ Deployment  when launching a DB Instance with the AWS Management Console. Alternatively, if you are using the Amazon RDS APIs, you would call the CreateDBInstance API and set the  Multi-AZ  parameter to the value  true.  To convert an existing standard (single-AZ) DB instance to Multi-AZ, modify the DB instance in the AWS Management Console or use the ModifyDBInstance API and set the Multi-AZ parameter to true. "
983,What happens when I convert my Amazon RDS instance from Single-AZ to Multi-AZ?,"For the RDS for PostgreSQL, RDS for MySQL, RDS for MariaDB, RDS for SQL Server, RDS for Oracle, and RDS for Db2 database engines, when you elect to convert your Amazon RDS instance from Single-AZ to Multi-AZ, the following happens: A snapshot of your primary instance is taken. A new standby instance is created in a different Availability Zone, from the snapshot. Synchronous replication is configured between primary and standby instances.   As such, there should be no downtime incurred when an instance is converted from Single-AZ to Multi-AZ. However, you might see increased latency while the data on the standby is caught up to match to the primary. "
984,What events would cause Amazon RDS to initiate a failover to the standby replica?,"Amazon RDS detects and automatically recovers from the most common failure scenarios for Multi-AZ deployments so that you can resume database operations as quickly as possible without administrative intervention. Amazon RDS automatically performs a failover in the event of any of the following: Loss of availability in primary Availability Zone Loss of network connectivity to primary Compute unit failure on primary Storage failure on primary Note: When operations such as DB instance scaling or system upgrades, like OS patching, are initiated for Multi-AZ deployments, for enhanced availability they are applied first on the standby prior to automatic failover. As a result, your availability impact is limited only to the time required for automatic failover to complete. Note that Amazon RDS Multi-AZ deployments do not failover automatically in response to database operations, such as long running queries, deadlocks, or database corruption errors. "
985,Will I be alerted when automatic failover occurs?,"Yes, Amazon RDS will emit a DB instance event to inform you that automatic failover occurred. You can click the  Events  section of the Amazon RDS Console or use the DescribeEvents API to return information about events related to your DB instance. You can also use Amazon RDS Event Notifications to be notified when specific DB events occur. "
986,What happens during Multi-AZ failover and how long does it take?,"Failover is automatically handled by Amazon RDS so that you can resume database operations as quickly as possible without administrative intervention. When failing over, Amazon RDS simply flips the canonical name record (CNAME) for your DB instance to point at the standby, which is in turn promoted to become the new primary. We encourage you to follow best practices and implement database connection retry at the application layer. Failovers, as defined by the interval between the detection of the failure on the primary and the resumption of transactions on the standby, typically complete within one to two minutes. Failover time can also be affected by whether large uncommitted transactions must be recovered; the use of adequately large instance types is recommended with Multi-AZ for best results. AWS also recommends the use of Provisioned IOPS with Multi-AZ instances, for fast, predictable, and consistent throughput performance. "
987,Can I initiate a  forced failover  for my Multi-AZ DB instance deployment?,"Amazon RDS will automatically failover without user intervention under a variety of failure conditions. In addition, Amazon RDS provides an option to initiate a failover when rebooting your instance. You can access this feature via the AWS Management Console or when using the RebootDBInstance API call. "
988,How do I control/configure Multi-AZ synchronous replication?,"With Multi-AZ deployments, you simply set the  Multi-AZ  parameter to true. The creation of the standby, synchronous replication, and failover are all handled automatically. This means you cannot select the Availability Zone your standby is deployed in or alter the number of standbys available (Amazon RDS provisions one dedicated standby per DB instance primary). The standby also cannot be configured to accept database read activity. Learn more about Multi-AZ configurations. "
989,Will my standby be in the same Region as my primary?,Yes. Your standby is automatically provisioned in a different Availability Zone of the same Region as your DB instance primary. 
990,Can I see which Availability Zone my primary is currently located in?,"Yes, you can gain visibility into the location of the current primary by using the AWS Management Console or DescribeDBInstances API. "
991,"After failover, my primary is now located in a different Availability Zone than my other AWS resources (e.g. EC2 instances). Should I be concerned about latency?","Availability Zones are engineered to provide low latency network connectivity to other Availability Zones in the same Region. In addition, you may want to consider architecting your application and other AWS resources with redundancy across multiple Availability Zones so your application will be resilient in the event of an Availability Zone failure. Multi-AZ deployments address this need for the database tier without administration on your part. "
992,How do DB Snapshots and automated backups work with my Multi-AZ deployment?,"You interact with automated backup and DB Snapshot functionality in the same way whether you are running a standard deployment in a Single-AZ or Multi-AZ deployment. If you are running a Multi-AZ deployment, automated backups and DB Snapshots are simply taken from the standby to avoid I/O suspension on the primary. Please note that you may experience increased I/O latency (typically lasting a few minutes) during backups for both Single-AZ and Multi-AZ deployments. Initiating a restore operation (point-in-time restore or restore from DB Snapshot) also works the same with Multi-AZ deployments as standard, Single-AZ deployments. New DB instance deployments can be created with either the RestoreDBInstanceFromSnapshot or RestoreDBInstanceToPointInTime APIs. These new DB instance deployments can be either standard or Multi-AZ, regardless of whether the source backup was initiated on a standard or Multi-AZ deployment. "
993,What does it mean to run a DB Instance as a read replica?,"Read replicas make it easier to take advantage of supported engines' built-in replication functionality to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create a read replica with a few clicks in the AWS Management Console or using the CreateDBInstanceReadReplica API. Once the read replica is created, database updates on the source DB instance will be replicated using a supported engine's native, asynchronous replication. You can create multiple read replicas for a given source DB Instance and distribute your application's read traffic amongst them. Since read replicas use supported engines' built-in replication, they are subject to its strengths and limitations. In particular, updates are applied to your read replica(s) after they occur on the source DB instance, and replication lag can vary significantly. Read replicas can be associated with Multi-AZ deployments to gain read scaling benefits in addition to the enhanced database write availability and data durability provided by Multi-AZ deployments. "
994,When would I want to consider using an Amazon RDS read replica?,"There are a variety of scenarios where deploying one or more read replicas for a given source DB instance may make sense. Common reasons for deploying a read replica include: Scaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas. Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replica(s). For this use case, keep in mind that the data on the read replica may be  stale  since the source DB Instance is unavailable. Business reporting or data warehousing scenarios. You may want business reporting queries to run against a read replica rather than your primary, production DB Instance. You may use a read replica for disaster recovery of the source DB instance either in the same AWS Region or in another Region. "
995,Do I need to enable automatic backups on my DB instance before I can create read replicas?,Yes. Enable automatic backups on your source DB Instance before adding read replicas by setting the backup retention period to a value other than 0. Backups must remain enabled for read replicas to work. 
996,Which versions of database engines support Amazon RDS read replicas?,"Amazon Aurora: All DB clusters. Amazon RDS for MySQL: All DB instances support creation of read replicas. Automatic backups must be and remain enabled on the source DB instance for read replica operations. Automatic backups on the replica are supported only for Amazon RDS read replicas running MySQL 5.6 and later, not 5.5. Amazon RDS for PostgreSQL: DB instances with PostgreSQL version 9.3.5 or newer support creation of read replicas. Existing PostgreSQL instances prior to version 9.3.5 need to be upgraded to PostgreSQL version 9.3.5 to take advantage of Amazon RDS read replicas. Amazon RDS for MariaDB: All DB instances support creation of read replicas. Automatic backups must be and remain enabled on the source DB Instance for read replica operations. Amazon RDS for Oracle: Supported for Oracle version 12.1.0.2.v12 and higher and for all 12.2 versions using the Bring Your Own License model with Oracle Database Enterprise Edition and licensed for the Active Data Guard Option. Amazon RDS for SQL Server: Read replicas are supported on Enterprise Edition in the Multi-AZ configuration when the underlying replication technology is using Always On availability groups for SQL Server versions 2016 and 2017. "
997,How do I deploy a read replica for a given DB instance?,"You can create a read replica in minutes using the standard CreateDBInstanceReadReplica API or a few steps on the AWS Management Console. When creating a read replica, you can identify it as a read replica by specifying a SourceDBInstanceIdentifier. The SourceDBInstanceIdentifier is the DB Instance Identifier of the  source  DB Instance from which you wish to replicate. As with a standard DB Instance, you can also specify the Availability Zone, DB instance class, and preferred maintenance window. The engine version (e.g., PostgreSQL 9.3.5) and storage allocation of a read replica is inherited from the source DB instance.  When you initiate the creation of a read replica, Amazon RDS takes a snapshot of your source DB instance and begins replication. As a result, you will experience a brief I/O suspension on your source DB instance as the snapshot occurs. The I/O suspension typically lasts on the order of one minute and is avoided if the source DB instance is a Multi-AZ deployment (in the case of Multi-AZ deployments, snapshots are taken from the standby). Amazon RDS is also currently working on an optimization (to be released shortly) such that if you create multiple Read Replicas within a 30 minute window, all of them will use the same source snapshot to minimize I/O impact ( catch-up  replication for each Read Replica will begin after creation). "
998,How do I connect to my read replica(s)?,"You can connect to a read replica just as you would connect to a standard DB instance, using the DescribeDBInstance API or AWS Management Console to retrieve the endpoint(s) for your read replica(s). If you have multiple read replicas, it is up to your application to determine how read traffic will be distributed amongst them. "
999,How many read replicas can I create for a given source DB instance?,"Amazon RDS for MySQL, MariaDB, and PostgreSQL allow you to create up to 15 read replicas for a given source DB instance. Amazon RDS for Oracle and SQL Server allow you to create up to 5 read replicas for a given source DB instance. "
1000,Can I create a read replica in an AWS Region different from that of the source DB instance?,"Yes, Amazon RDS (except RDS for SQL Server) supports cross-region read replicas. The amount of time between when data is written to the source DB instance and when it is available in the read replica will depend on the network latency between the two regions. "
1001,Do Amazon RDS read replicas support synchronous replication?,"No. Read replicas in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server are implemented using those engines' native asynchronous replication. Amazon Aurora uses a different, but still asynchronous, replication mechanism. "
1002,Can I use a read replica to enhance database write availability or protect the data on my source DB instance against failure scenarios?,"If you are looking to use replication to increase database write availability and protect recent database updates against various failure conditions, we recommend you run your DB instance as a Multi-AZ deployment. With Amazon RDS Read Replicas, which employ supported engines' native, asynchronous replication, database writes occur on a read replica after they have already occurred on the source DB instance, and this replication  lag  can vary significantly.  In contrast, the replication used by Multi-AZ deployments is synchronous, meaning that all database writes are concurrent on the primary and standby. This protects your latest database updates, since they should be available on the standby in the event failover is required.  In addition, with Multi-AZ deployments replication is fully managed. Amazon RDS automatically monitors for DB instance failure conditions or Availability Zone failure and initiates automatic failover to the standby (or to a read replica, in the case of Amazon Aurora) if an outage occurs. "
1003,Can I create a read replica with a Multi-AZ DB instance deployment as its source?,"Yes. Since Multi-AZ DB instances address a different need than read replicas, it makes sense to use the two in conjunction for production deployments and to associate a read replica with a Multi-AZ DB Instance deployment. The  source  Multi AZ-DB instance provides you with enhanced write availability and data durability, and the associated read replica would improve read traffic scalability. "
1004,Can I configure my Amazon RDS read replicas themselves Multi-AZ?,"Yes. Amazon RDS for MySQL, MariaDB, PostgreSQL, and Oracle allow you to enable Multi-AZ configuration on read replicas to support disaster recovery and minimize downtime from engine upgrades. "
1005,"If my read replica(s) use a Multi-AZ DB instance deployment as a source, what happens if Multi-AZ failover occurs?","In the event of Multi-AZ failover, any associated and available read replicas will automatically resume replication once failover has completed (acquiring updates from the newly promoted primary). "
1006,Can I create a read replica of another read replica?,"Amazon Aurora, Amazon RDS for MySQL, and MariaDB: You can create three tiers of read replicas. A second-tier read replica from an existing first-tier read replica and a third tier replica from second tier read replicas. By creating a second-tier and third-tier read replica, you may be able to move some of the replication load from the primary database instance to different tiers of Read Replica based on your application needs. Please note that a second-tier Read Replica may lag further behind the primary because of additional replication latency introduced as transactions are replicated from the primary to the first tier replica and then to the second-tier replica. Similarly, the third tier replica may lag behind the second-tier read replica. Amazon RDS for Oracle and Amazon RDS for SQL Server: Read Replicas of Read Replicas are not currently supported. "
1007,Can my read replicas only accept database read operations?,"Read replicas are designed to serve read traffic. However, there may be use cases where advanced users wish to complete Data Definition Language (DDL) SQL statements against a read replica. Examples might include adding a database index to a read replica that is used for business reporting without adding the same index to the corresponding source DB instance. Amazon RDS for MySQL can be configured to permit DDL SQL statements against a read replica. If you wish to enable operations other than reads for a given read replica, modify the active DB parameter group for the read replica setting the  read_only  parameter to  0.  Amazon RDS for PostgreSQL does not currently support the execution of DDL SQL statements against a read replica. "
1008,Can I promote my read replica into a  standalone  DB Instance?,Yes. Refer to the Amazon RDS User Guide for more details. 
1009,Will my read replica be kept up-to-date with its source DB instance?,"Updates to a source DB instance will automatically be replicated to any associated read replicas. However, with supported engines' asynchronous replication technology, a read replica can fall behind its source DB instance for a variety of reasons. Typical reasons include: Write I/O volume to the source DB instance exceeds the rate at which changes can be applied to the read replica (this problem is particularly likely to arise if the compute capacity of a read replica is less than the source DB Instance) Complex or long-running transactions to the source DB Instance hold up replication to the read replica Network partitions or latency between the source DB instance and a read replica Read Replicas are subject to the strengths and weaknesses of supported engines' native replication. If you are using Read Replicas, you should be aware of the potential for a lag between a Read Replica and its source DB Instance or  inconsistency . "
1010,How do I see the status of my active read replica(s)?,"You can use the standard DescribeDBInstances API to return a list of all the DB Instances you have deployed (including Read Replicas) or simply click on the Instances tab of the Amazon RDS Console. Amazon RDS allows you to gain visibility into how far a read replica has fallen behind its source DB instance. The number of seconds that the read replica is behind the primary is published as an Amazon CloudWatch metric (Replica Lag) available via the AWS Management Console or Amazon CloudWatch APIs. For Amazon RDS for MySQL, the source of this information is the same as that displayed by issuing a standard Show Replica Status MySQL command against the read replica. For Amazon RDS for PostgreSQL, you can use the pg_stat_replication view on the source DB instance to explore replication metrics. Amazon RDS monitors the replication status of your Read Replicas and updates the Replication State field in the AWS Management console to Error if replication stops for any reason (e.g. attempting DML queries on your replica that conflict with the updates made on the primary database instance could result in a replication error). You can review the details of the associated error thrown by the MySQL engine by viewing the Replication Error field and take appropriate action to recover from it. You can learn more about troubleshooting replication issues in the Troubleshooting a Read Replica Problem section of the User Guide for Amazon RDS for MySQL or PostgreSQL.  If a replication error is fixed, the Replication State changes to Replicating. "
1011,I scaled the compute and/or storage capacity of my source DB instance. Should I scale the resources for associated read replica(s) as well?,"For replication to work effectively, we recommend that read replicas have as much or more compute and storage resources as their respective source DB instances. Otherwise replication lag is likely to increase or your read replica may run out of space to store replicated updates. "
1012,How do I delete a read replica? Will it be deleted automatically if its source DB Instance is deleted?,"You can delete a read replica with a few steps of the AWS Management Console or by passing its DB Instance identifier to the DeleteDBInstance API.  An Amazon Aurora replica will stay active and continue accepting read traffic even after its corresponding source DB Instance has been deleted. One of the replicas in the cluster will automatically be promoted as the new primary and will start accepting write traffic. An Amazon RDS for MySQL or MariaDB read replica will stay active and continue accepting read traffic even after its corresponding source DB instance has been deleted. If you desire to delete the Read Replica in addition to the source DB instance, you must explicitly do so using the DeleteDBInstance API or AWS Management Console. If you delete an Amazon RDS for PostgreSQL DB Instance that has read replicas, all Read Replicas will be promoted to standalone DB Instances and will be able to accept both read and write traffic. The newly promoted DB Instances will operate independently of one another. If you desire to delete these DB Instances in addition to the original source DB Instance, you must explicitly do so using the DeleteDBInstance API or AWS Management Console. "
1013,How much do read replicas cost? When does billing begin and end?,"A read replica is billed as a standard DB Instance and at the same rates. Just like a standard DB instance, the rate per  DB Instance hour  for a read replica is determined by the DB instance class of the read replica - please see pricing page for up-to-date pricing. You are not charged for the data transfer incurred in replicating data between your source DB instance and read replica within the same AWS Region. Billing for a read replica begins as soon as the replica has been successfully created (i.e. when the status is listed as  active ). The read replica will continue being billed at standard Amazon RDS DB instance hour rates until you issue a command to delete it. "
1014,What is Enhanced Monitoring for Amazon RDS?,"Enhanced Monitoring for Amazon RDS gives you deeper visibility into the health of your Amazon RDS instances. Just turn on the  Enhanced Monitoring  option for your Amazon RDS DB Instance and set a granularity and Enhanced Monitoring will collect vital operating system metrics and process information, at the defined granularity. For an even deeper level of diagnostics and visualization of your database load, and a longer data retention period, you can try Performance Insights. "
1015,Which metrics and processes can I monitor in Enhanced Monitoring?,"Enhanced Monitoring captures your Amazon RDS instance system level metrics, such as the CPU, memory, file system, and disk I/O among others. The complete list of metrics can be found in the documentation. "
1016,Which engines are supported by Enhanced Monitoring?,Enhanced Monitoring supports all Amazon RDS database engines. 
1017,Which instance types are supported by Enhanced Monitoring?,"Enhanced Monitoring supports every instance type except t1.micro and m1.small. The software uses a small amount of CPU, memory, and I/O, and for general purpose monitoring, we recommend switching on higher granularities for instances that are medium or larger. For non-production DB Instances, the default setting for Enhanced Monitoring is  off  and you have the choice of leaving it disabled or modifying the granularity when it is on. "
1018,What information can I view on the Amazon RDS dashboard?,You can view all the system metrics and process information for your Amazon RDS DB Instances in a graphical format on the console. You can manage which metrics you want to monitor for each instance and customize the dashboard according to your requirements. 
1019,Will all the instances in my Amazon RDS account sample metrics at the same granularity?,No. You can set different granularities for each DB Instance in your Amazon RDS account. You can also choose the instances on which you want to enable Enhanced Monitoring as well as modify the granularity of any instance whenever you want. 
1020,How far back can I see the historical metrics on the Amazon RDS console?,You can see the performance values for all the metrics up to 1 hour back at a granularity of up to 1 second based on your settings. 
1021,How can I visualize the metrics generated by Amazon RDS Enhanced Monitoring in CloudWatch?,"The metrics from Amazon RDS Enhanced Monitoring are delivered into your CloudWatch Logs account. You can create metrics filters in CloudWatch from CloudWatch Logs and display the graphs on the CloudWatch dashboard. For more details, please visit the Amazon CloudWatch page. "
1022,When should I use CloudWatch instead of the Amazon RDS console dashboard?,"You should use CloudWatch if you want to view historical data beyond what is available on the Amazon RDS console dashboard. You can monitor your Amazon RDS instances in CloudWatch to diagnose the health of your entire AWS stack in a single location. Currently, CloudWatch supports granularities of up to 1 minute and the values will be averaged out for granularities less than that. "
1023,Can I set up alarms and notifications based on specific metrics?,"Yes. You can create an alarm in CloudWatch that sends a notification when the alarm changes state. The alarm watches a single metric over a time period that you specify and performs one or more actions based on the value of the metric relative to the specified threshold over a number of time periods. For more details on CloudWatch alarms, please visit the Amazon CloudWatch Developer Guide. "
1024,How do I integrate Enhanced Monitoring with my tool that I currently use?,"Amazon RDS Enhanced Monitoring provides a set of metrics formed as JSON payloads that are delivered into your CloudWatch Logs account. The JSON payloads are delivered at the granularity last configured for the Amazon RDS instance. There are two ways you can consume the metrics via a third-party dashboard or application. Monitoring tools can use CloudWatch Logs Subscriptions to set up a near real time feed for the metrics. Alternatively, you can use filters in CloudWatch Logs to bridge metrics across to CloudWatch and integrate your application with CloudWatch. Please visit Amazon CloudWatch Documentation for more details. "
1025,How can I delete historical data?,"Since Enhanced Monitoring delivers JSON payloads into a log in your CloudWatch Logs account, you can control its retention period just like any other CloudWatch Logs stream. The default retention period configured for Enhanced Monitoring in CloudWatch Logs is 30 days. For details on how to change retention settings, please visit Amazon CloudWatch Developer Guide. "
1026,What impact does Enhanced Monitoring have on my monthly bills?,"Since the metrics are ingested into CloudWatch Logs, your charges will be based on CloudWatch Logs data transfer and storage rates once you exceed CloudWatch Logs free tier. Pricing details can be found here. The amount of information transferred for an Amazon RDS instance is directly proportional to the defined granularity for the Enhanced Monitoring feature. Administrators can set different granularities for different instances in their accounts to manage costs. The approximate volume of data ingested into CloudWatch Logs by Enhanced Monitoring for an instance is as shown below: Granularity 60 seconds 30 seconds 15 seconds 10 seconds 5 seconds 1 second  Data ingested in CloudWatch Logs* (GB per month)   0.27   0.53   1.07   1.61   3.21   16.07 "
1027,What is Amazon RDS Proxy?,"Amazon RDS Proxy is a fully managed, highly available database proxy feature for Amazon RDS. RDS Proxy makes applications more scalable, more resilient to database failures, and more secure. "
1028,Why would I use Amazon RDS Proxy?,"Amazon RDS Proxy is a fully managed, highly available, and easy-to-use database proxy feature of Amazon RDS that enables your applications to: 1) improve scalability by pooling and sharing database connections, 2) improve availability by reducing database failover times by up to 66% and preserving application connections during failovers, and 3) improve security by optionally enforcing AWS IAM authentication to databases and securely storing credentials in AWS Secrets Manager. "
1029,What use cases does Amazon RDS Proxy address?,"Amazon RDS Proxy addresses a number of use cases related to scalability, availability, and security of your applications, including: Applications with unpredictable workloads: Applications that support highly variable workloads may attempt to open a burst of new database connections. Amazon RDS Proxy's connection governance allows you to gracefully scale applications dealing with unpredictable workloads by efficiently reusing database connections. First, RDS Proxy enables multiple application connections to share a database connection for efficient use of database resources. Second, RDS Proxy allows you to maintain predictable database performance by regulating the number of database connections that are opened. Third, RDS Proxy removes requests that cannot be served to preserve the overall performance and availability of the application. Applications that frequently open and close database connections: Applications built on technologies such as Serverless, PHP, or Ruby on Rails may open and close database connections frequently to serve application requests. Amazon RDS Proxy maintains a pool of database connections to avoid unnecessary stress on database compute and memory for establishing new connections. Applications that keep connections open but idle: Applications in industries such as SaaS or eCommerce may keep database connections idling to minimize the response time when a customer reengages. Instead of overprovisioning databases to support mostly idling connections, you can use Amazon RDS Proxy to hold idling connections while only establishing database connections as required to optimally serve active requests. Applications requiring availability through transient failures: With Amazon RDS Proxy, you can build applications that can transparently tolerate database failures without needing to write complex failure handling code. RDS Proxy automatically routes traffic to a new database instance while preserving application connections. RDS Proxy also bypasses Domain Name System (DNS) caches to reduce failover times by up to 66% for Amazon RDS and Aurora Multi-AZ databases. During database failovers, the application may experience increased latencies and ongoing transactions may have to be retried. Improved security and centralized credentials management: Amazon RDS Proxy aids you in building more secure applications by giving you a choice to enforce IAM based authentication with relational databases. RDS Proxy also enables you to centrally manage database credentials through AWS Secrets Manager. "
1030,When should I connect to the database directly versus using Amazon RDS Proxy?,"Depending on your workload, Amazon RDS Proxy can add an average of 5 milliseconds of network latency to query or transaction response time. If your application cannot tolerate 5 milliseconds of latency or does not need connection management and other features enabled by RDS Proxy, you may want your application to connect directly to the database endpoint. "
1031,How will serverless applications benefit from Amazon RDS Proxy?,"Amazon RDS Proxy transforms your approach to building modern serverless applications that leverage the power and simplicity of relational databases. First, RDS Proxy enables serverless applications to scale efficiently by pooling and reusing database connections. Second, with RDS Proxy, you no longer need to handle database credentials in your Lambda code. You can use the IAM execution role associated with your Lambda function to authenticate with RDS Proxy and your database. Third, you don't need to manage any new infrastructure or code to utilize the full potential of serverless applications backed by relational databases. RDS Proxy is fully managed and scales its capacity automatically based on your application demands. "
1032,Which database engines does Amazon RDS Proxy support?,"RDS Proxy is available for Amazon Aurora with MySQL compatibility, Amazon Aurora with PostgreSQL compatibility, Amazon RDS for MariaDB, Amazon RDS for MySQL, Amazon RDS for PostgreSQL, and Amazon RDS for SQL Server. For a list of supported engine versions see the Amazon Aurora User Guide or the Amazon RDS User Guide. "
1033,How can I enable Amazon RDS Proxy?,"You enable Amazon RDS Proxy for your Amazon RDS database with just a few clicks in the Amazon RDS console. While enabling RDS Proxy, you specify the VPC and subnets you want to access RDS Proxy from. As a Lambda user, you can enable Amazon RDS Proxy for your Amazon RDS database and set up a Lambda function to access it with just a few clicks in the Lambda console. "
1034,Can I access Amazon RDS Proxy using APIs?,"Yes. You can use Amazon RDS Proxy APIs to create a proxy and then define target groups to associate the proxy with specific database instances or clusters. For example: aws rds create-db-proxy          --db-proxy-name ''          --engine-family <mysql|postgresql>                --auth [{}, {}]          --role-arn ''         --subnet-ids {}         --require-tls <true|false>         --tags {} aws rds register-db-proxy-targets          --target-group-name ''         --db-cluster-identifier  ''         --db-instance-identifier '' "
1035,Why should I use Trusted Language Extensions for PostgreSQL?,"Trusted Language Extensions (TLE) for PostgreSQL enables developers to build high performance PostgreSQL extensions and run them safely on Amazon Aurora and Amazon RDS. In doing so, TLE improves your time to market and removes the burden placed on database administrators to certify custom and third-party code for use in production database workloads. You can move forward as soon as you decide an extension meets your needs. With TLE, independent software vendors (ISVs) can provide new PostgreSQL extensions to customers running on Aurora and Amazon RDS. "
1036,What are traditional risks of running extensions in PostgreSQL and how does TLE for PostgreSQL mitigate those risks?,"PostgreSQL extensions are executed in the same process space for high performance. However, extensions might have software defects that can crash the database.  TLE for PostgreSQL offers multiple layers of protection to mitigate this risk. TLE is designed to limit access to system resources. The rds_superuser role can determine who is permitted to install specific extensions. However, these changes can only be made through the TLE API. TLE is designed to limit the impact of an extension defect to a single database connection. In addition to these safeguards, TLE is designed to provide DBAs in the rds_superuser role fine-grained, online control over who can install extensions and they can create a permissions model for running them.  Only users with sufficient privileges will be able to run and create using the  CREATE EXTENSION  command on a TLE extension. DBAs can also allow-list  PostgreSQL hooks  required for more sophisticated extensions that modify the database's internal behavior and typically require elevated privilege. "
1037,How does TLE for PostgreSQL relate to/work with other AWS services?,TLE for PostgreSQL is available for Amazon Aurora PostgreSQL-Compatible Edition and Amazon RDS on PostgreSQL on versions 14.5 and higher. TLE is implemented as a PostgreSQL extension itself and you can activate it from the rds_superuser role similar to other extensions supported on Aurora and Amazon RDS. 
1038,In what versions of PostgreSQL can I run TLE for PostgreSQL?,You can run TLE for PostgreSQL in PostgreSQL 14.5 or higher in Amazon Aurora and Amazon RDS. 
1039,In what Regions is Trusted Language Extensions for PostgreSQL available?,TLE for PostgreSQL is currently available in all AWS Regions (excluding AWS China Regions) and the AWS GovCloud Regions. 
1040,How much does it cost to run TLE?,TLE for PostgreSQL is available to Aurora and Amazon RDS customers at no additional cost. 
1041,How is TLE for PostgreSQL different from extensions available on Amazon Aurora and Amazon RDS today?,Aurora and Amazon RDS support a curated set of over 85 PostgreSQL extensions. AWS manages the security risks for each of these extensions under the AWS shared responsibility model. The extension that implements TLE for PostgreSQL is included in this set. Extensions that you write or that you obtain from third-party sources and install in TLE are considered part of your application code. You are responsible for the security of your applications that use TLE extensions. 
1042,What are some examples of extensions I could run with TLE for PostgreSQL?,"You can build developer functions, such as bitmap compression and differential privacy (such as publicly accessible statistical queries that protect privacy of individuals). "
1043,What programming languages can I use to develop TLE for PostgreSQL?,"TLE for PostgreSQL currently supports JavaScript, PL/pgSQL, Perl, and SQL. "
1044,How do I deploy a TLE for PostgreSQL extension?,"Once the rds_superuser role activates TLE for PostgreSQL, you can deploy TLE extensions using the SQL CREATE EXTENSION command from any PostgreSQL client, such as psql. This is similar to how you would create a user-defined function written in a procedural language, such as PL/pgSQL or PL/Perl. You can control which users have permission to deploy TLE extensions and use specific extensions. "
1045,How do TLE for PostgreSQL extensions communicate with the PostgreSQL database?,"TLE for PostgreSQL accesses your PostgreSQL database exclusively through the TLE API. The TLE supported trusted languages include all functions of the PostgreSQL server programming interface (SPI) and support for PostgreSQL hooks, including the check password hook. "
1046,Where can I learn more about the TLE for PostgreSQL open-source project?,You can learn more about the TLE for PostgreSQL project on the official TLE GitHub page. 
1047,What engines support Amazon RDS Blue/Green Deployments?,"Amazon RDS Blue/Green Deployments are available in Amazon Aurora MySQL-Compatible Edition, Amazon Aurora PostgreSQL-Compatible Edition, Amazon RDS for MySQL, Amazon RDS for MariaDB, and Amazon RDS for PostgreSQL. "
1048,What versions does Amazon RDS Blue/Green Deployments support?,"Amazon RDS Blue/Green Deployments are available in for Amazon Aurora MySQL-Compatible Edition versions 5.6 and higher, RDS for MySQL versions 5.7 and higher, and RDS for versions MariaDB versions 10.2 and higher. Blue/Green Deployments are also supported for Amazon Aurora PostgreSQL-Compatible Edition and Amazon RDS for PostgreSQL for versions 11.21 and higher, 12.16 and higher, 13.12 and higher, 14.9 and higher, and 15.4 and higher. Learn more about available versions in the Amazon Aurora and Amazon RDS documentation. "
1049,What Regions does Amazon RDS Blue/Green Deployments support?,Amazon RDS Blue/Green Deployments are available in all applicable AWS Regions and the AWS GovCloud Regions. 
1050,When should I use Amazon RDS Blue/Green Deployments?,"Amazon RDS Blue/Green Deployments allow you to make safer, simpler, and faster database changes. Blue/Green Deployments are ideal for use cases such as major or minor version database engine upgrades, operating system updates, schema changes on green environments that do not break logical replication, like adding a new column at the end of a table, or database parameter setting changes. You can use Blue/Green Deployments to make multiple database updates at the same time using a single switchover. This allows you to stay current on security patches, improve database performance, and access newer database features with short, predictable downtime. "
1051,What is the cost of using Amazon RDS Blue/Green Deployments?,"You will incur the same price for running your workloads on green instances as you do for blue instances. The cost of running on blue and green instances include our current standard pricing for db.instances, cost of storage, cost of read/write I/Os, and any enabled features, such as cost of backups and Amazon RDS Performance Insights. Effectively, you are paying approximately 2x the cost of running workloads on db.instance for the lifespan of the blue-green-deployment. For example: You have an RDS for MySQL 5.7 database running on two r5.2xlarge db.instances, a primary database instance and a read replica, in us-east-1 AWS region with a Multi-AZ (MAZ) configuration. Each of the r5.2xlarge db.instances is configured for 20 GiB General Purpose Amazon Elastic Block Storge (EBS). You create a clone of the blue instance topology using Amazon RDS Blue/Green Deployments, run it for 15 days (360 hours), and then delete the blue instances after a successful switchover. The blue instances cost $1,387 for 15 days at an on-demand rate of $1.926/hr (Instance + EBS cost). The total cost to you for using Blue/Green Deployments for those 15 days is $2,774, which is 2x the cost of running blue instances for that time period. "
1052,What kind of changes can I make with Amazon RDS Blue/Green Deployments?,"Amazon RDS Blue/Green Deployments allow you to make safer, simpler, and faster database changes, such as major or minor version upgrades, schema changes, instance scaling, engine parameter changes, and maintenance updates. "
1053,What is the  blue environment  in Amazon RDS Blue/Green Deployments? What is the  green environment?,"In Amazon RDS Blue/Green Deployments, the blue environment is your current production environment. The green environment is your staging environment that will become your new production environment after switchover. "
1054,How do switchovers work with Amazon RDS Blue/Green Deployments?,"When Amazon RDS Blue/Green Deployments initiate a switchover, they block writes to both the blue and green environments, until switchover is complete. During switchover, the staging environment, or green environment, catches up with the production system, ensuring data is consistent between the staging and production environment. Once the production and staging environment are in complete sync, Blue/Green Deployments promote the staging environment as the production environment by redirecting traffic to the newly promoted production environment. Blue/Green Deployments are designed to enable writes on the green environment after switchover is complete, ensuring zero data loss during the switchover process. "
1055,Can I use Blue/Green Deployments when I have a blue environment as a subscriber/publisher for a self-managed logical replica?,"If your blue environment is a self-managed logical replica, or subscriber, we will block switchover. We recommend that you first stop replication to the blue environment, proceed with the switchover, and then resume replication. In contrast, if your blue environment is a source for a self-managed logical replica, or publisher, you can continue to switchover. However, you will need to update the self-managed replica to replicate from the green environment post switchover. "
1056,"After Amazon RDS Blue/Green Deployments switches over, what happens to my old production environment?","Amazon RDS Blue/Green Deployments do not delete your old production environment. If needed, you can access it for additional validations and performance/regression testing. If you no longer need the old production environment, you can delete it. Standard billing charges apply on old production instances until you delete them. "
1057,What do Amazon RDS Blue/Green Deployments switchover guardrails check for?,"Amazon RDS Blue/Green Deployments switchover guardrails block writes on your blue and green environments until your green environment catches up before switching over. Blue/Green Deployments also perform health checks of your primary and replicas in your blue and green environments. They also perform replication health checks, for example, to see if replication has stopped or if there are errors. They detect long running transactions between your blue and green environments. You can specify your maximum tolerable downtime, as low as 30 seconds, and if you have an ongoing transaction that exceeds this your switchover will time out. "
1058,"Do Amazon RDS Blue/Green Deployments support Global Databases, Amazon RDS Proxy, cross-Region read replicas, or cascaded read replicas?","No, Amazon RDS Blue/Green Deployments do not support Global Databases, Amazon RDS Proxy, cross-Region read replicas, or cascaded read replicas. "
1059,Can I use Amazon RDS Blue/Green Deployments to rollback changes?,"No, at this time you cannot use Amazon RDS Blue/Green Deployments to rollback changes. "
1060,How does Amazon RDS Optimized Writes write data files differently than MySQL?,MySQL protects users from data loss by writing data in 16KiB pages in memory twice to durable storage first to the  doublewrite buffer  and then to table storage. Amazon RDS Optimized Writes write your 16KiB data pages directly to your data files reliably and durably in one step using the Torn Write Prevention feature of the AWS Nitro System. 
1061,Which RDS for MySQL database versions support Amazon RDS Optimized Writes?,Amazon RDS Optimized Writes are available for MySQL major version 8.0.30 and higher. 
1062,Which database instance types support Amazon RDS Optimized Writes? In what Regions are they available?,"Amazon RDS Optimized Writes are available in db.r6i and db.r5b instances. They are available in all Regions where these instances are available, excluding AWS China Regions. "
1063,When should I use Amazon RDS Optimized Writes?,"All Amazon RDS for MySQL users should implement Amazon RDS Optimized Writes for up to 2x improved write transaction throughput. Applications with write-heavy workloads, such as digital payments, financial trading, and online gaming applications will find this feature especially helpful. "
1064,Is Amazon RDS Optimized Writes supported on Amazon Aurora MySQL-Compatible Edition?,"No. Amazon Aurora MySQL-Compatible Edition already avoids the use of the  doublewrite buffer.  Instead, Amazon Aurora replicates data six ways across three Availability Zones (AZs) and uses a quorum-based approach to durably write data and correctly read it thereafter. "
1065,Can customers convert their existing Amazon RDS databases to use Amazon RDS Optimized Writes?,"At this time, this initial release does not support enabling Amazon RDS Optimized Writes for your existing database instances even if the instance class supports Optimized Writes. "
1066,How much are Amazon RDS Optimized Writes?,Amazon RDS Optimized Writes are available to RDS for MySQL customers at no additional cost. 
1067,How do Amazon RDS Optimized Reads speed up query performance?,"Workloads that use temporary objects in MySQL and MariaDB for query processing benefit from Amazon RDS Optimized Reads. Optimized Reads place temporary objects on the database instance's NVMe-based instance storage, instead of the Amazon Elastic Block Store volume. This helps to speed up complex query processing by up to 2X. "
1068,Which RDS for MySQL and RDS for MariaDB database versions support Amazon RDS Optimized Reads?,"Amazon RDS Optimized Reads are available for RDS for MySQL on MySQL versions 8.0.28 and higher and on RDS for MariaDB on MariaDB versions 10.4.25, 10.5.16, 10.6.7 and higher. "
1069,Which database instance types support Amazon RDS Optimized Reads? In what regions are they available?,"Amazon RDS Optimized Reads are available in all regions where db.r5d, db.m5d, db.r6gd, and db.m6gd, X2idn, and X2iedn instances are available instances are available. For more information, see the Amazon RDS DB instance classes documentation. "
1070,When should I use Amazon RDS Optimized Reads?,"Customers should use Amazon RDS Optimized Reads when they have workloads that require complex queries; general purpose analytics; or require intricate groups, sorts, hash aggregations, high-load joins, and Common Table Expressions (CTEs). These use cases result in the creation of temporary tables, allowing Optimized Reads to speed up your workload's query processing. "
1071,Can customers convert their existing Amazon RDS databases to use Amazon RDS Optimized Reads?,"Yes, customers can convert their existing Amazon RDS database to use Amazon RDS Optimized Reads by moving your workload to an Optimized Read-enabled instance. Optimized Reads are also available by default on all supported instance classes. If you're running your workload on db.r5d, db.m5d, db.r6gd, db.m6gd, X2idn, and X2iedn instances, you're already benefiting from Optimized Reads. "
1072,How can I store and retrieve large amounts of data in the cloud?,"Use Amazon S3, a scalable object storage service designed for storing and retrieving any amount of data."
1073,I need a secure way to store my backup files. What AWS service should I consider?,"Amazon S3 provides secure and durable object storage, making it an ideal choice for storing backup files."
1074,What's the recommended way to host static website content in AWS?,Amazon S3 is the recommended service for hosting static website content. It allows you to serve files directly from S3.
1075,How can I share large files with others over the internet?,"Use Amazon S3 to store your files, and you can generate pre-signed URLs to securely share them with others."
1076,I have a mobile application that requires storage for user uploads. Which AWS service should I use?,Amazon S3 is suitable for storing and serving user uploads in a scalable and cost-effective manner for your mobile application.
1077,What AWS service supports data archiving for long-term storage?,Use Amazon S3 Glacier for data archiving. It provides low-cost storage with retrieval times ranging from minutes to hours.
1078,I need to store and distribute software updates. What AWS service is recommended?,Amazon S3 is ideal for storing and distributing software updates. It provides low-latency access to content.
1079,How can I optimize costs for storing infrequently accessed data?,"Consider using Amazon S3 Intelligent-Tiering, which automatically moves objects between two access tiers based on changing access pattern."
1080,What's the best way to transfer large amounts of data into and out of AWS?,"Use Amazon S3 Transfer Acceleration for fast, secure, and easy file transfers into and out of Amazon S3."
1081,I need versioning for my data. What AWS service provides this feature for object storage?,"Amazon S3 supports versioning, allowing you to preserve, retrieve, and restore every version of every object stored in a bucket."
1082,How can I implement data lifecycle policies to automatically delete old files?,Amazon S3 Lifecycle policies enable you to define rules for automatically transitioning and deleting objects based on their age.
1083,I want to protect my data from accidental deletion. What feature should I use in Amazon S3?,Enable versioning on your Amazon S3 bucket to protect your data from accidental deletion or overwrite.
1084,I have multimedia content for my application. What AWS service is suitable for scalable storage and retrieval of media files?,"Use Amazon S3 for scalable and reliable storage of multimedia content, including images, videos, and audio files."
1085,How can I secure my data at rest in Amazon S3?,"Amazon S3 supports server-side encryption (SSE) to encrypt data at rest. You can choose from SSE-S3, SSE-KMS, or SSE-C."
1086,"I need to grant temporary, limited access to objects in my S3 bucket. What AWS feature should I use?",Use Amazon S3 pre-signed URLs to grant temporary access to specific S3 objects without requiring AWS credentials.
1087,How can I monitor and log access to my S3 bucket?,Enable Amazon S3 Server Access Logging to capture detailed access logs for your S3 bucket.
1088,I want to host a private repository for Docker images. What AWS service supports this use case?,"Use Amazon S3 to store Docker images, and you can configure private access using AWS Identity and Access Management (IAM) roles."
1089,How can I replicate my S3 bucket's contents to another AWS region for disaster recovery?,Amazon S3 Cross-Region Replication allows you to automatically replicate objects across different AWS regions.
1090,"I need to serve dynamic, frequently changing content for my web application. What AWS service is suitable for this?","Amazon S3 is ideal for serving static content. For dynamic content, consider integrating Amazon S3 with AWS Lambda and Amazon CloudFront."
1091,I want to grant different levels of access to different users for my S3 bucket. How can I achieve this?,Use AWS Identity and Access Management (IAM) policies to control access to your S3 bucket based on user roles and permissions.
1092,How can I optimize costs for frequently accessed data in my S3 bucket?,Use Amazon S3 Standard for frequently accessed data. It provides low-latency access and is suitable for a wide range of use cases.
1093,I need to serve video content with low latency. What AWS service can help with this?,Amazon S3 combined with Amazon CloudFront provides a highly scalable and low-latency solution for serving video content.
1094,I have a regulatory requirement to store data in a specific AWS region. Can I enforce this for my S3 bucket?,"Yes, you can specify the AWS region for your S3 bucket, ensuring compliance with regulatory requirements."
1095,How can I set up event notifications for changes to objects in my S3 bucket?,Use Amazon S3 event notifications to trigger events and automate workflows based on changes to objects in your S3 bucket.
1096,I need to host a static website with HTTPS support. What AWS service should I use in conjunction with S3?,Combine Amazon S3 for static content storage with Amazon CloudFront for global content delivery and HTTPS support.
1097,I have large datasets for analytics. What AWS service can I use to efficiently analyze data stored in S3?,Use Amazon Athena for querying and analyzing data directly from Amazon S3 without the need for data loading.
1098,How can I grant access to my S3 bucket to users outside of my AWS account?,Use Amazon S3 bucket policies to grant cross-account access to other AWS accounts or make objects public if necessary.
1099,I need to ensure that my S3 bucket is resilient to the loss of data in a single facility. What feature should I enable?,Enable Amazon S3 cross-region replication to replicate your bucket's objects to a different AWS region for data resilience.
1100,I want to serve content over HTTP/2. What AWS service can help with this?,Amazon S3 combined with Amazon CloudFront provides HTTP/2 support for faster and more efficient content delivery.
1101,How can I restrict access to my S3 bucket to a specific set of IP addresses?,Use Amazon S3 bucket policies or Access Control Lists (ACLs) to restrict access to your S3 bucket based on IP addresses.
1102,I need to archive infrequently accessed data at a lower cost. What storage class should I use?,Use Amazon S3 Glacier storage class for archiving infrequently accessed data at a lower cost compared to standard storage.
1103,How can I synchronize files between my on-premises data center and my S3 bucket?,Use AWS Storage Gateway to enable seamless file synchronization between your on-premises data center and Amazon S3.
1104,What AWS service provides a durable and scalable storage solution for data lakes?,"Amazon S3 is widely used as a storage solution for building data lakes due to its durability, scalability, and easy integration with analytics services."
1105,I have a large number of small files to store. What AWS service is optimized for storing many small objects?,"Amazon S3 supports efficient storage of many small objects, making it suitable for use cases with a large number of small files."
1106,How can I accelerate access to my S3 bucket for users in different geographic locations?,"Use Amazon S3 Transfer Acceleration to accelerate uploads and downloads to and from your S3 bucket, especially for users in different geographic locations."
1107,I need to grant temporary access to my S3 bucket for an external application. What AWS feature should I use?,Use AWS Identity and Access Management (IAM) roles combined with temporary security credentials to grant temporary access to your S3 bucket.
1108,How can I ensure that my S3 bucket is not accessible to the public?,"Review and configure the bucket policies, access control lists (ACLs), and permissions to ensure that your S3 bucket is not publicly accessible."
1109,I want to migrate my existing data to S3. What AWS service can help with this process?,Use the AWS DataSync service to efficiently and securely migrate large amounts of data from on-premises storage systems to Amazon S3.
1110,I need to provide access to my S3 bucket for applications running on Amazon EC2 instances. How can I achieve this securely?,Use IAM roles and instance profiles to securely grant applications running on Amazon EC2 instances access to your S3 bucket without using access keys.
1111,How can I enforce encryption for data at rest in my S3 bucket?,"Use S3 bucket policies to enforce server-side encryption (SSE) for data at rest. Choose from SSE-S3, SSE-KMS, or SSE-C based on your requirements."
1112,I want to set up automated backups for files in my S3 bucket. What AWS feature should I use?,Use S3 Object Versioning combined with S3 Lifecycle policies to set up automated backups for files in your S3 bucket.
1113,How can I grant temporary access to my S3 bucket for a specific user without sharing AWS credentials?,Generate pre-signed URLs using AWS SDKs or the AWS Management Console to grant temporary access to your S3 bucket without sharing credentials.
1114,I need to index and search metadata associated with objects in my S3 bucket. What AWS service can help with this?,"Use Amazon S3 Inventory to generate reports about your objects and their metadata, and consider integrating with Amazon S3 Select for querying specific data within objects."
1115,How can I ensure data durability and availability for my S3 objects?,Amazon S3 provides 99.999999999% (11 9's) durability for objects and offers strong consistency for read-after-write operations.
1116,I want to grant different levels of access to different folders within my S3 bucket. How can I achieve this?,Use S3 bucket policies and IAM policies to define fine-grained access control for different folders within your S3 bucket.
1117,I need to analyze data stored in my S3 bucket using Apache Spark. What AWS service can facilitate this integration?,Use Amazon EMR (Elastic MapReduce) to process and analyze data stored in your S3 bucket using Apache Spark and other big data frameworks.
1118,How can I ensure the integrity of my data during transfer to and from my S3 bucket?,Use Amazon S3 Transfer Acceleration with secure HTTPS connections to ensure the integrity of data during transfers.
1119,I have a large number of users accessing files concurrently from my S3 bucket. What can I do to optimize performance?,Configure Amazon S3 Transfer Acceleration and CloudFront to optimize the performance of concurrent file access from your S3 bucket.
1120,What is Amazon S3?,"Amazon S3 is object storage built to store and retrieve any amount of data from anywhere. S3 is a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs."
1121,What can I do with Amazon S3?,"Amazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere. Using this service, you can easily build applications that make use of cloud native storage. Since Amazon S3 is highly scalable and you only pay for what you use, you can start small and grow your application as you wish, with no compromise on performance or reliability."
1122,How can I get started using Amazon S3?,"To sign up for Amazon S3, visit the S3 console. You must have an Amazon Web Services account to access this service. If you do not already have an account, you will be prompted to create one when you begin the Amazon S3 sign-up process. After signing up, refer to the Amazon S3 documentation, view the S3 getting started materials, and see the additional resources in the resource center to begin using Amazon S3."
1123,What can I do with Amazon S3 that I cannot do with an on-premises solution?,"Amazon S3 lets you leverage Amazon's own benefits of massive scale with no up-front investment or performance compromises. By using Amazon S3, it is inexpensive and simple to ensure your data is quickly accessible, always available, and secure."
1124,What kind of data can I store in Amazon S3?,You can store virtually any kind of data in any format. Refer to the Amazon Web Services Licensing Agreement for details.
1125,How much data can I store in Amazon S3?,"The total volume of data and number of objects you can store in Amazon S3 are unlimited. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. The largest object that can be uploaded in a single PUT is 5 GB. For objects larger than 100 MB, customers should consider using the multipart upload capability."
1126,What is an S3 general purpose bucket?,"A bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket. General purpose buckets are the original S3 bucket type, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. They are recommended for most use cases and access pattern."
1127,What is an S3 directory bucket?,"A bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket. S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone. They are recommended for low-latency use cases. Each S3 directory bucket can support hundreds of thousands of transactions per second (TPS), independent of the number of directories within the bucket."
1128,What is the difference between a general purpose bucket and a directory bucket?,"A bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket. General purpose buckets are the original S3 bucket type, and a single general purpose bucket can contain objects stored across all storage classes except S3 Express One Zone. They are recommended for most use cases and access pattern. S3 directory buckets only allow objects stored in the S3 Express One Zone storage class, which provides faster data processing within a single Availability Zone. They are recommended for low-latency use cases. Each S3 directory bucket can support hundreds of thousands of transactions per second (TPS), independent of the number of directories within the bucket."
1129,What does Amazon do with my data in Amazon S3?,"Amazon stores your data and tracks its associated usage for billing purposes. Amazon will not otherwise access your data for any purpose outside of the Amazon S3 offering, except when required to do so by law. Refer to the Amazon Web Services Licensing Agreement for details."
1130,Does Amazon store its own data in Amazon S3?,Yes. Organizations across Amazon use Amazon S3 for a wide variety of projects. Many of these projects use Amazon S3 as their authoritative data store and rely on it for business-critical operations.
1131,How is Amazon S3 data organized?,"Amazon S3 is a simple key-based object store. When you store data, you assign a unique object key that can later be used to retrieve the data. Keys can be any string, and they can be constructed to mimic hierarchical attributes. Alternatively, you can use S3 Object Tagging to organize your data across all of your S3 buckets and/or prefixes."
1132,How do I interface with Amazon S3?,"Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any internet-development toolkit. The operations are intentionally made simple to make it easy to add new distribution protocols and functional layers."
1133,How reliable is Amazon S3?,"Amazon S3 gives you access to the same highly scalable, highly available, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. The S3 Standard storage class is designed for 99.99% availability, the S3 Standard-IA storage class, S3 Intelligent-Tiering storage class, and the S3 Glacier Instant Retrieval storage classes are designed for 99.9% availability, the S3 One Zone-IA storage class is designed for 99.5% availability, and the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive class are designed for 99.99% availability and an SLA of 99.9%. All of these storage classes are backed by the Amazon S3 Service Level Agreement."
1134,How will Amazon S3 perform if traffic from my application suddenly spikes?,"Amazon S3 is designed from the ground up to handle traffic for any internet application. Pay-as-you-go pricing and unlimited capacity ensures that your incremental costs don't change and that your service is not interrupted. Amazon S3's massive scale lets you spread the load evenly, so that no individual application is affected by traffic spikes."
1135,Does Amazon S3 offer a Service Level Agreement (SLA)?,Yes. The Amazon S3 SLA provides for a service credit if a customer's monthly uptime percentage is below our service commitment in any billing cycle.
1136,What is the consistency model for Amazon S3?,"Amazon S3 delivers strong read-after-write consistency automatically, without changes to performance or availability, without sacrificing regional isolation for applications, and at no additional cost."
1137,Why does strong read-after-write consistency help me?,"Strong read-after-write consistency helps when you need to immediately read an object after a write; for example, when you often read and list immediately after writing objects. High-performance computing workloads also benefit in that when an object is overwritten and then read many times simultaneously, strong read-after-write consistency provides assurance that the latest write is read across all reads. These applications automatically and immediately benefit from strong read-after-write consistency. The strong consistency of S3 also reduces costs by removing the need for extra infrastructure to provide strong consistency."
1138,Where is my data stored?,"You specify an AWS Region when you create your Amazon S3 bucket. For S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive storage classes, your objects are automatically stored across multiple devices spanning a minimum of three Availability Zones (AZs). AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other. Objects stored in the S3 One Zone-IA storage class are stored redundantly within a single Availability Zone in the AWS Region you select. For S3 on Outposts, your data is stored in your Outpost on-premises environment, unless you manually choose to transfer it to an AWS Region. Refer to AWS regional services list for details of Amazon S3 service availability by AWS Region."
1139,What is an AWS Region?,"An AWS Region is a physical location around the world where AWS cluster data centers.  Each group of logical data centers within a Region is know as an Availability Zone (AZ). Each AWS Region consists of a minimum of three, isolated, and physically separate AZs within a geographic area. Unlike other cloud providers, who often define a Region as a single data center, the multiple AZ design of every AWS Region offers advantages for customers. Each AZ has independent power, cooling, and physical security and is connected via redundant, ultra-low-latency networks."
1140,What is an AWS Availability Zone (AZ)?,"An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. AZs give customers the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center. All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs."
1141,How do I decide which AWS Region to store my data in?,"There are several factors to consider based on your specific application. For instance, you may want to store your data in a Region that is near your customers, your data centers, or other AWS resources to reduce data access latencies. You may also want to store your data in a Region that is remote from your other operations for geographic redundancy and disaster recovery purposes. You should also consider Regions that let you address specific legal and regulatory requirements and/or reduce your storage costs you can choose a lower priced Region to save money. For S3 pricing information, visit the Amazon S3 pricing page."
1142,In which parts of the world is Amazon S3 available?,"Amazon S3 is available in AWS Regions worldwide, and you can use Amazon S3 regardless of your location. You just have to decide which AWS Region(s) you want to store your Amazon S3 data. See the AWS regional services list for a list of AWS Regions in which S3 is available today."
1143,How much does Amazon S3 cost?,"With Amazon S3, you pay only for what you use. There is no minimum charge. You can estimate your monthly bill using the AWS Pricing Calculator."
1144,How will I be charged and billed for my use of Amazon S3?,"There are no set up charges or commitments to begin using Amazon S3. At the end of the month, you will automatically be charged for that month's usage. You can view your charges for the current billing period at any time by logging into your Amazon Web Services account, and selecting the 'Billing Dashboard' associated with your console profile."
1145,Storage Used:,Amazon S3 storage pricing is summarized on the Amazon S3 pricing page.
1146,Storage Example:,"Assume you store 100 GB (107,374,182,400 bytes) of data in Amazon S3 Standard in your bucket for 15 days in March, and 100 TB (109,951,162,777,600 bytes) of data in Amazon S3 Standard for the final 16 days in March."
1147,Network Data Transferred In:,Amazon S3 Data Transfer In pricing is summarized on the Amazon S3 pricing page. This represents the amount of data sent to your Amazon S3 buckets.
1148,Network Data Transferred Out:,"Amazon S3 Data Transfer Out pricing is summarized on the Amazon S3 pricing page. For Amazon S3, this charge applies whenever data is read from any of your buckets from a location outside of the given Amazon S3 Region."
1149,Data Transfer Out Example:,"Your aggregate Data Transfer would be 62 TB (31 TB from Amazon S3 and 31 TB from Amazon EC2). This equates to 63,488 GB (62 TB * 1024 GB/TB)."
1150,Data Requests:,Amazon S3 Request pricing is summarized on the Amazon S3 pricing page.
1151,Data Retrieval:,Amazon S3 data retrieval pricing applies for the S3 Standard-Infrequent Access (S3 Standard-IA) and S3 One Zone-IA storage classes and is summarized on the Amazon S3 pricing page.
1152,Data Retrieval Example:,Your data retrieval charges for the month would be calculated as 300 GB x $0.01/GB = $3.00. Note that you would also pay network data transfer charges for the portion that went out to the internet.
1153,Why do prices vary depending on which Amazon S3 Region I choose?,"AWS charges less where our costs are less. For example, our costs are lower in the US East (Northern Virginia) Region than in the US West (Northern California) Region."
1154,How am I charged for using Versioning?,"Normal Amazon S3 rates apply for every version of an object stored or requested. For example, let's look at the following scenario to illustrate storage costs when utilizing Versioning (let's assume the current month is 31 days long):"
1155,How am I charged for accessing Amazon S3 through the AWS Management Console?,"Normal Amazon S3 pricing applies when accessing the service through the AWS Management Console. To provide an optimized experience, the AWS Management Console may proactively execute requests. Also, some interactive operations result in more than one request to the service."
1156,How am I charged if my Amazon S3 buckets are accessed from another AWS account?,"Normal Amazon S3 pricing applies when your storage is accessed by another AWS Account. Alternatively, you may choose to configure your bucket as a Requester Pays bucket, in which case the requester will pay the cost of requests and downloads of your Amazon S3 data."
1157,Do your prices include taxes?,"Except as otherwise noted, our prices are exclusive of applicable taxes and duties, including VAT and applicable sales tax. For customers with a Japanese billing address, use of AWS services is subject to Japanese Consumption Tax."
1158,Will I incur any data transfer out to the internet charges when I move my data out of AWS?,"AWS offers eligible customers free data transfer out to the internet when they move all of their data off of AWS, in accordance with the process below."
1159,I want to move my data out of AWS. How do I request free data transfer out to the internet?,Complete the following steps:
1160,Why do I have to request AWS' pre-approval for free data transfer out to the internet before moving my data out of AWS?,"AWS customers make hundreds of millions of data transfers each day, and we generally don't know the reason for any given data transfer. For example, customers may be transferring data to an end user of their application, to a visitor of their website, or to another cloud or on-premises environment for backup purposes. Accordingly, the only way we know that your data transfer is to support your move off of AWS is if you tell us beforehand."
1161,What is IPv6?,"Every server and device connected to the internet must have a unique address. Internet Protocol Version 4 (IPv4) was the original 32-bit addressing scheme. However, the continued growth of the internet means that all available IPv4 addresses will be utilized over time. Internet Protocol Version 6 (IPv6) is an addressing mechanism designed to overcome the global address limitation on IPv4."
1162,What can I do with IPv6?,"Using IPv6 support for Amazon S3, applications can connect to Amazon S3 without the need for any IPv6 to IPv4 translation software or systems. You can meet compliance requirements, more easily integrate with existing IPv6-based on-premises applications, and remove the need for expensive networking equipment to handle the address translation. You can also now utilize the existing source address filtering features in IAM policies and bucket policies with IPv6 addresses, expanding your options to secure applications interacting with Amazon S3."
1163,How do I get started with IPv6 on Amazon S3?,"You can get started by pointing your application to Amazon S3's dual-stack endpoint, which supports access over both IPv4 and IPv6. In most cases, no further configuration is required for access over IPv6, because most network clients prefer IPv6 addresses by default. Applications that are impacted by using IPv6 can switch back to the standard IPv4-only endpoints at any time. IPv6 with Amazon S3 is supported in all commercial AWS Regions, including AWS GovCloud (US) Regions, the Amazon Web Services China (Beijing) Region, operated by Sinnet, and the Amazon Web Services China (Ningxia) Region, operated by NWCD."
1164,Should I expect a change in Amazon S3 performance when using IPv6?,"No, you will see the same performance when using either IPv4 or IPv6 with Amazon S3."
1165,What are Amazon S3 Event Notifications?,"You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket, such as PUT, POST, COPY, and DELETE events. You can publish notifications to Amazon EventBridge, Amazon SNS, Amazon SQS, or directly to AWS Lambda."
1166,What can I do with Amazon S3 Event Notifications?,"Amazon S3 Event Notifications let you run workflows, send alerts, or perform other actions in response to changes in your objects stored in S3. You can use S3 Event Notifications to set up triggers to perform actions including transcoding media files when they are uploaded, processing data files when they become available, and synchronizing S3 objects with other data stores. You can also set up event notifications based on object name prefixes and suffixes. For example, you can choose to receive notifications on object names that start with images/."""
1167,What is included in Amazon S3 Event Notifications?,"For a detailed description of the information included in Amazon S3 Event Notification messages, refer to the configuring Amazon S3 Event Notifications documentation."
1168,How do I set up Amazon S3 Event Notifications?,"For a detailed description of how to configure event notifications, refer to the configuring Amazon S3 Event Notifications documentation. You can learn more about AWS messaging services in the Amazon SNS documentation and the Amazon SQS documentation."
1169,What does it cost to use Amazon S3 Event Notifications?,"There are no additional charges for using Amazon S3 for event notifications. You pay only for use of Amazon SNS or Amazon SQS to deliver event notifications, or for the cost of running an AWS Lambda function. Visit the Amazon SNS, Amazon SQS, or AWS Lambda pricing pages to view the pricing details for these services."
1170,What is S3 Transfer Acceleration?,"Amazon S3 Transfer Acceleration creates fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. S3 Transfer Acceleration leverages Amazon CloudFront's globally distributed AWS Edge locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path."
1171,How do I get started with S3 Transfer Acceleration?,"To get started with S3 Transfer Acceleration enable S3 Transfer Acceleration on an S3 bucket using the Amazon S3 console, the Amazon S3 API, or the AWS CLI. After S3 Transfer Acceleration is enabled, you can point your Amazon S3 PUT and GET requests to the s3-accelerate endpoint domain name. Your data transfer application must use one of the following two types of endpoints to access the bucket for faster data transfer: .s3-accelerate.amazonaws.com or .s3-accelerate.dualstack.amazonaws.com for the dual-stack endpoint. If you want to use standard data transfer, you can continue to use the regular endpoints."
1172,How fast is S3 Transfer Acceleration?,"S3 Transfer Acceleration helps you fully use your bandwidth, minimize the effect of distance on throughput, and is designed to ensure consistently fast data transfer to Amazon S3 regardless of your client's location. The amount of acceleration primarily depends on your available bandwidth, the distance between the source and destination, and packet loss rates on the network path. Generally, you will see more acceleration when the source is farther from the destination, when there is more available bandwidth, and/or when the object size is bigger."
1173,Who should use S3 Transfer Acceleration?,"S3 Transfer Acceleration is designed to optimize transfer speeds from across the world into S3 buckets. If you are uploading to a centralized bucket from geographically dispersed locations or if you regularly transfer GBs or TBs of data across continents, you may save hours or days of data transfer time with S3 Transfer Acceleration."
1174,How secure is S3 Transfer Acceleration?,"S3 Transfer Acceleration provides the same security as regular transfers to Amazon S3. All Amazon S3 security features, such as access restriction based on a client's IP address, are supported as well. S3 Transfer Acceleration communicates with clients over standard TCP and does not require firewall changes. No data is ever saved at AWS Edge locations."
1175,What if S3 Transfer Acceleration is not faster than a regular Amazon S3 transfer?,"Each time you use S3 Transfer Acceleration to upload an object, we will check whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If we determine that S3 Transfer Acceleration is not likely to be faster than a regular Amazon S3 transfer of the same object to the same destination AWS Region, we will not charge for the use of S3 Transfer Acceleration for that transfer, and we may bypass the S3 Transfer Acceleration system for that upload."
1176,Can I use S3 Transfer Acceleration with multipart uploads?,"Yes, S3 Transfer Acceleration supports all bucket level features including multipart uploads."
1177,How should I choose between S3 Transfer Acceleration and Amazon CloudFront's PUT/POST?,"S3 Transfer Acceleration optimizes the TCP protocol and adds additional intelligence between the client and the S3 bucket, making S3 Transfer Acceleration a better choice if a higher throughput is desired. If you have objects that are smaller than 1 GB or if the data set is less than 1 GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance."
1178,How should I choose between S3 Transfer Acceleration and AWS Snow Family?,"The AWS Snow Family is ideal for customers moving large batches of data at once. The AWS Snowball has a typical 5 7 days turnaround time. As a rule of thumb, S3 Transfer Acceleration over a fully-utilized 1 Gbps line can transfer up to 75 TBs in the same time period. In general, if it will take more than a week to transfer over the internet, or there are recurring transfer jobs and there is more than 25Mbps of available bandwidth, S3 Transfer Acceleration is a good option. Another option is to use both: perform initial heavy lift moves with an AWS Snowball (or series of AWS Snowballs) and then transfer incremental ongoing changes with S3 Transfer Acceleration."
1179,Can S3 Transfer Acceleration complement AWS Direct Connect?,"AWS Direct Connect is a good choice for customers who have a private networking requirement or who have access to AWS Direct Connect exchanges. S3 Transfer Acceleration is best for submitting data from distributed client locations over the public internet, or where variable network conditions make throughput poor. Some AWS Direct Connect customers use S3 Transfer Acceleration to help with remote office transfers where they may suffer from poor internet performance."
1180,Can S3 Transfer Acceleration complement AWS Storage Gateway or a third-party gateway?,You can benefit from configuring the bucket destination in your third-party gateway to use an S3 Transfer Acceleration endpoint domain.
1181,Can S3 Transfer Acceleration complement third-party integrated software?,Yes. Software packages that connect directly into Amazon S3 can take advantage of S3 Transfer Acceleration when they send their jobs to Amazon S3.
1182,Is S3 Transfer Acceleration HIPAA eligible?,"Yes, AWS has expanded its HIPAA compliance program to include S3 Transfer Acceleration as a HIPAA eligible service. If you have an executed Business Associate Agreement (BAA) with AWS, you can use S3 Transfer Acceleration to make fast, easy, and secure transfers of files, including protected health information (PHI) over long distances between your client and your Amazon S3 bucket."
1183,How secure is my data in Amazon S3?,"Amazon S3 is secure by default. Upon creation, only you have access to Amazon S3 buckets that you create, and you have complete control over who has access to your data. Amazon S3 supports user authentication to control access to data. You can use access control mechanisms, such as bucket policies, to selectively grant permissions to users and groups of users. The Amazon S3 console highlights your publicly accessible buckets, indicates the source of public accessibility, and also warns you if changes to your bucket policies or bucket ACLs would make your bucket publicly accessible. You should enable Amazon S3 Block Public Access for all accounts and buckets that you do not want publicly accessible. All new buckets have Block Public Access turned on by default."
1184,How can I control access to my data stored on Amazon S3?,"Customers can use a number of mechanisms for controlling access to Amazon S3 resources, including AWS Identity and Access Management (IAM) policies, bucket policies, access point policies, access control lists (ACLs), Query String Authentication, Amazon Virtual Private Cloud (Amazon VPC) endpoint policies, service control policies (SCPs) in AWS Organizations, and Amazon S3 Block Public Access."
1185,S3 Block Public Access,Learn more about policies and permissions in the AWS IAM documentation.
1186,Does Amazon S3 support data access auditing?,"Yes, customers can optionally configure an Amazon S3 bucket to create access log records for all requests made against it. Alternatively, customers who need to capture IAM/user identity information in their logs can configure AWS CloudTrail Data Events."
1187,What options do I have for encrypting data stored on Amazon S3?,"Amazon S3 encrypts all new data uploads to any bucket. Amazon S3 applies S3-managed server-side encryption (SSE-S3) as the base level of encryption to all object uploads (as of January 5, 2023). SSE-S3 provides a fully-managed solution where Amazon handles key management and key protection using multiple layers of security. You should continue to use SSE-S3 if you prefer to have Amazon manage your keys. Additionally, you can choose to encrypt data using SSE-C, SSE-KMS, DSSE-KMS, or a client library such as the Amazon S3 Encryption Client. Each option allows you to store sensitive data encrypted at rest in Amazon S3."
1188,Can I comply with European data privacy regulations using Amazon S3?,"Customers can choose to store all data in Europe by using the Europe (Frankfurt), Europe (Ireland), Europe (Paris), Europe (Stockholm), Europe (Milan), Europe (Spain), Europe (London), or Europe (Zurich) Region. You can also use Amazon S3 on Outposts to keep all of your data on premises on the AWS Outpost, and you may choose to transfer data between AWS Outposts or to an AWS Region. It is your responsibility to ensure that you comply with European privacy laws. View the AWS General Data Protection Regulation (GDPR) Center and AWS Data Privacy Center for more information. If you have more specific location requirements or other data privacy regulations that require you to keep data in a location where there is not an AWS Region, you can use S3 on Outposts."
1189,What is an Amazon VPC Endpoint for Amazon S3?,"An Amazon VPC Endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity to S3 over the AWS global network. There are two types of VPC endpoints for S3: gateway VPC endpoints and interface VPC endpoints. Gateway endpoints are a gateway that you specify in your route table to access S3 from your VPC over the AWS network. Interface endpoints extend the functionality of gateway endpoints by using private IPs to route requests to S3 from within your VPC, on-premises, or from a different AWS Region. For more information, visit the AWS PrivateLink for Amazon S3 documentation."
1190,Can I allow a specific Amazon VPC Endpoint access to my Amazon S3 bucket?,"You can limit access to your bucket from a specific Amazon VPC Endpoint or a set of endpoints using Amazon S3 bucket policies. S3 bucket policies now support a condition, aws:sourceVpce, that you can use to restrict access. For more details and example policies, read the gateway endpoints for S3 documentation."
1191,What is AWS PrivateLink for Amazon S3?,"AWS PrivateLink for S3 provides private connectivity between Amazon S3 and on-premises. You can provision interface VPC endpoints for S3 in your VPC to connect your on-premises applications directly to S3 over AWS Direct Connect or AWS VPN. You no longer need to use public IPs, change firewall rules, or configure an internet gateway to access S3 from on-premises. To learn more visit the AWS PrivateLink for S3 documentation."
1192,How do I get started with interface VPC endpoints for S3?,"You can create an interface VPC endpoint using the AWS VPC Management Console, AWS Command Line Interface (AWS CLI), AWS SDK, or API. To learn more, visit the documentation."
1193,When should I choose gateway VPC endpoints versus AWS PrivateLink-based interface VPC endpoints?,"AWS recommends that you use interface VPC endpoints to access S3 from on-premises or from a VPC in another AWS Region. For resources that are accessing S3 from VPC in the same AWS Region as S3, we recommend using gateway VPC endpoints as they are not billed. To learn more, visit the documentation."
1194,Can I use both Interface Endpoints and Gateway Endpoints for S3 in the same VPC?,"Yes. If you have an existing gateway VPC endpoint, create an interface VPC endpoint in your VPC and update your client applications with the VPC endpoint specific endpoint names. For example, if your VPC endpoint id of the interface endpoint is vpce-0fe5b17a0707d6abc-29p5708s in the us-east-1 Region, then your endpoint specific DNS name will be vpce-0fe5b17a0707d6abc-29p5708s.s3.us-east-1.vpce.amazonaws.com. In this case, only the requests to the VPC endpoint specific names will route through Interface VPC endpoints to S3 while all other requests would continue to route through the gateway VPC endpoint. To learn more, visit the documentation."
1195,What is Amazon Macie and how can I use it to secure my data?,"Amazon Macie is an AI-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization. Amazon Macie continuously monitors data access activity for anomalies, and delivers alerts when it detects risk of unauthorized access or inadvertent data leaks."
1196,What is IAM Access Analyzer for Amazon S3 and how does it work?,"Access Analyzer for S3 is a feature that helps you simplify permissions management as you set, verify, and refine policies for your S3 buckets and access points. Access Analyzer for S3 monitors your existing access policies to verify that they provide only the required access to your S3 resources. Access Analyzer for S3 evaluates your bucket access policies and helps you discover and swiftly make changes to buckets that do not require access."
1197,What are Amazon S3 Access Grants?,"Amazon S3 Access Grants map identities in directories such as Active Directory, or AWS Identity and Access Management (IAM) principals, to datasets in S3. This helps you manage data permissions at scale by automatically granting S3 access to end-users based on their corporate identity. Additionally, S3 Access Grants log end-user identity and the application used to access S3 data in AWS CloudTrail. This helps to provide a detailed audit history down to the end-user identity for all access to the data in your S3 buckets."
1198,Why should I use S3 Access Grants?,"You should use S3 Access Grants if your S3 data is shared and accessed by many users and applications, where some of their identities are in your corporate directory such as Okta or Entra ID, and you need a scalable, simple, and auditable way to grant access to these S3 datasets at scale."
1199,How do I get started with S3 Access Grants?,"You can get started with S3 Access Grants in four steps. First, configure an S3 Access Grants instance. In this step, if you want to use S3 Access Grants with users and groups in your corporate directory, enable AWS Identity Center and connect S3 Access Grants to your Identity Center instance. Second, register a location with S3 Access Grants. During this process, you give S3 Access Grants an IAM role that is used to create temporary S3 credentials that users and applications can use to access S3. Third, define permission grants that specify who can access what. Finally, at the time of access, have your application request temporary credentials from S3 Access Grants and use Access Grants-vended credentials to access S3."
1200,What types of identity are supported for S3 Access Grants permission grants?,"S3 Access Grants supports two kinds of identities: enterprise user or group identities from AWS Identity Center, and AWS IAM principals including IAM users and roles. When you use S3 Access Grants with AWS Identity Center, you can define data permissions on the basis of directory group memberships. AWS Identity Center is an AWS service that connects to commonly-used identity providers, including Entra ID, Okta, Ping, and others. In addition to supporting directory identities via AWS Identity Center, S3 Access Grants also supports permission rules for AWS IAM principal including IAM users and roles. This is for use cases where you either manage a custom identity federation not through AWS Identity Center but via IAM and SAML assertion (example implementation), or manage application identities based on IAM principals, and still would like to use S3 Access Grants due to its scalability and auditability."
1201,What are the different access levels that S3 Access Grants offers?,"S3 Access Grants offers three access levels: READ, WRITE, and READWRITE. READ allows you to view and retrieve objects from S3. WRITE allows you to write to and delete from S3. READWRITE allows you to do both READ and WRITE."
1202,Can I customize my access levels?,No. You can only use the three pre-defined access levels (READ/WRITE/READWRITE) that S3 Access Grants offers.
1203,Are there any limits on S3 Access Grants?,"Yes. You can create up to 100,000 grants per S3 Access Grants instance, and up to 1,000 locations per S3 Access Grants instance."
1204,Is there any performance impact for data access when I use S3 Access Grants?,"No. The latency for obtaining temporary credentials from S3 Access Grants is similar to obtaining temporary credentials from AWS STS today. Once you have obtained the credentials from S3 Access Grants, you can reuse unexpired credentials for subsequent requests. For these subsequent requests, there is no additional latency for requests authenticated via S3 Access Grants credentials compared to other methods."
1205,What other AWS services are required to use S3 Access Grants?,"If you intend to use S3 Access Grants for directory identities, you will need to set up AWS IAM Identity Center first. AWS IAM Identity Center helps you create or connect your workforce identities, whether the identities are created and stored in Identity Center, or in an external third-party Identity Provider. Refer to the Identity Center documentation for the setup process. Once you have set up the Identity Center instance, you can connect the instance to S3 Access Grants. Thereafter, S3 Access Grants relies on Identity Center to retrieve user attributes such as group membership to evaluate requests and make authorization decisions."
1206,Does S3 Access Grants require client-side modifications?,"Yes. Whereas today, you initialize your S3 client with IAM credentials associated with your application (for example, IAM role credentials for EC2 or IAM Roles Anywhere; or using long-term IAM user credentials), your application will need to instead obtain S3 Access Grants credentials first before initializing the S3 client. These S3 Access Grants credentials will be specific to the authenticated user in your application. Once the S3 client is initialized with these S3 Access Grants credentials, it can make requests for S3 data as usual using the credentials."
1207,"Since client-side modifications are necessary, what AWS services and third-party applications are integrated with S3 Access Grants out-of-box today?","S3 Access Grants today already integrates with EMR and open-source Spark via the S3A connector. In addition, S3 Access Grants integrates with third-party software including Immuta and Informatica so that you can centralize permission management. And finally, S3 Access Grants supports Terraform and CloudFormation for you to programmatically provision S3 Access Grants."
1208,Is S3 Access Grants a replacement for AWS IAM?,"No. S3 Access Grants does not replace IAM and in fact works well with your existing IAM-based data protection strategies (encryption, network, data-perimeter rules). S3 Access Grants is built on IAM primitives and enables you to express finer-grained S3 permissions at scale."
1209,Does S3 Access Grants work with KMS?,"Yes. To utilize S3 Access Grants for objects encrypted with KMS, bucket owners include the necessary KMS permissions in the IAM role that they grant to S3 Access Grants as part of the location registration. S3 Access Grants can then subsequently utilize that IAM role to access the KMS-encrypted objects in the buckets."
1210,How do I view and manage my S3 Access Grants permission grants?,You can use either the S3 Access Grants console experience in the AWS Management Console or SDK and CLI APIs for you to view and manage your S3 Access Grants permissions.
1211,Can you grant public access to data with S3 Access Grants?,"No, you cannot grant public access to data with S3 Access Grants."
1212,How can I audit requests that were authorized via S3 Access Grants?,The request by the application to initiate a data access session with S3 Access Grants will be recorded in CloudTrail. CloudTrail will distinguish the identity of the user making the request and the application identity accessing the data on the user's behalf. This helps you audit end-user identity of who accessed what data at what time.
1213,How is S3 Access Grants priced?,S3 Access Grants is charged based on the number of requests to S3 Access Grants. See the pricing page for details.
1214,What is the relationship between S3 Access Grants and Lake Formation?,"AWS Lake Formation is for use cases where you need to manage access for tabular data (e.g., Glue tables), where you might want to enforce row- and column-level access. S3 Access Grants is for managing access for direct S3 permissions such as unstructured data including videos, images, logs, etc."
1215,Is S3 Access Grants integrated with IAM Access Analyzer?,"No. S3 Access Grants is not integrated with IAM Access Analyzer at this time. You can't yet use IAM Access Analyzer to analyze S3 Access Grants permission grants. Customers can audit S3 Access Grants directly by going to the S3 Access Grants page in the S3 console, or programmatically using the ListAccessGrants API."
1216,What are Amazon S3 Access Points?,"Today, customers manage access to their S3 buckets using a single bucket policy that controls access for hundreds of applications with different permission levels."
1217,Why should I use an access point?,"S3 Access Points simplify how you manage data access to your shared datasets on S3. You no longer have to manage a single, complex bucket policy with hundreds of different permission rules that need to be written, read, tracked, and audited. With S3 Access Points, you can create access points or delegate permissions to trusted accounts to create cross-account access points on your bucket. This permits access to shared data sets with policies tailored to the specific application."
1218,How do S3 Access Points work?,"Each S3 Access Point is configured with an access policy specific to a use case or application, and a bucket can have thousands of access points. For example, you can create an access point for your S3 bucket that grants access for groups of users or applications for your data lake. An Access Point can support a single user or application, or groups of users or applications within and across accounts, allowing separate management of each access point."
1219,Is there a quota on how many access points I can create?,"By default, you can create 10,000 access points per Region per account on buckets in your account and cross-account. Unlike S3 buckets, there is no hard limit on the number of access points per AWS account. Visit AWS Service Quotas to request an increase in this quota."
1220,"When using an access point, how are requests authorized?","S3 access points have their own IAM access point policy. You write access point policies like you would a bucket policy, using the access point ARN as the resource. Access point policies can grant or restrict access to the S3 data requested through the access point. Amazon S3 evaluates all the relevant policies, including those on the user, bucket, access point, VPC Endpoint, and service control policies as well as Access Control Lists, to decide whether to authorize the request."
1221,How do I write access point policies?,"You can write an access point policy just like a bucket policy, using IAM rules to govern permissions and the access point ARN in the policy document."
1222,How is restricting access to specific VPCs using network origin controls on access points different from restricting access to VPCs using the bucket policy?,"You can continue to use bucket policies to limit bucket access to specified VPCs. Access points provide an easier, auditable way to lock down all or a subset of data in a shared data set to VPC-only traffic for all applications in your organization using API controls. You can use an AWS Organizations Service Control Policy (SCP) to mandate that any access point created in your organization set the network origin control API parameter value to vpc. Then, any new access point created automatically restricts data access to VPC-only traffic. No additional access policy is required to make sure that data requests are processed only from specified VPCs."
1223,Can I enforce a No internet data access policy for all access points in my organization?,"Yes. To enforce a No internet data access policy for access points in your organization, you would want to make sure all access points enforce VPC only access. To do so, you will write an AWS SCP that only supports the value vpc for the network origin control parameter in the create_access_point() API. If you had any internet-facing access points that you created previously, they can be removed. You will also need to modify the bucket policy in each of your buckets to further restrict internet access directly to your bucket through the bucket hostname. Since other AWS services may be directly accessing your bucket, make sure you set up access to allow the AWS services you want by modifying the policy to permit these AWS services. Refer to the S3 documentation for examples of how to do this."
1224,Can I completely disable direct access to a bucket using the bucket hostname?,"Not currently, but you can attach a bucket policy that rejects requests not made using an access point. Refer to the S3 documentation for more details."
1225,Can I replace or remove an access point from a bucket?,"Yes. When you remove an access point, any access to the associated bucket through other access points, and through the bucket hostname, will not be disrupted."
1226,What is the cost of Amazon S3 Access Points?,There is no additional charge for access points or buckets that use access points. Usual Amazon S3 request rates apply.
1227,How do I get started with S3 Access Points?,"You can start creating S3 Access Points on new buckets as well as existing buckets through the AWS Management Console, the AWS Command Line Interface (CLI), the Application Programming Interface (API), and the AWS Software Development Kit (SDK) client. To learn more about S3 Access Points, visit the user guide."
1228,How durable is Amazon S3?,"Amazon S3 provides the most durable storage in the cloud. Based on its unique architecture, S3 is designed to provide 99.999999999% (11 nines) data durability. Additionally, S3 stores data redundantly across a minimum of 3 Availability Zones (AZ) by default, providing built-in resilience against widespread disaster. Customers can store data in a single AZ to minimize storage cost or latency, in multiple AZs for resilience against the permanent loss of an entire data center, or in multiple AWS Regions to meet geographic resilience requirements."
1229,How is Amazon S3 designed for 99.999999999% durability?,"Amazon S3's designed for durability is a function of storage device failure rates and the rate at which S3 can detect failure, and then re-replicate data on those devices. S3 has end-to-end integrity checking on every object upload and verifies that all data is correctly and redundantly stored across multiple storage devices before it considers your upload to be successful. Once your data is stored in S3, S3 continuously monitors data durability over time with periodic integrity checks of all data at rest. S3 also actively monitors the redundancy of your data to help verify that your objects are able to tolerate the concurrent failure of multiple storage devices."
1230,Is data stored in a One Zone storage class protected against damage or loss of the Availability Zone?,"In the unlikely case of the loss or damage to all or part of an AWS Availability Zone, data in a One Zone storage class may be lost. For example, events like fire and water damage could result in data loss. Apart from these types of events, One Zone storage classes use similar engineering designs as Regional storage classes to protect objects from independent disk, host, and rack-level failures, and each are designed to deliver 99.999999999% data durability."
1231,How does Amazon S3 go beyond 99.999999999% durability?,"Amazon S3 has a strong durability culture, and durability best practices are designed into our systems and software from the ground up. AWS has more experience operating high-durability storage than any other cloud provider, and we use this experience to mitigate durability risk and to incorporate durability safeguards into everything we do."
1232,"With such high durability, do I still need to back up my critical data?","Yes. Amazon S3's durability system does not protect against accidental or malicious deletes. S3 relies on customers to decide what data they want to keep, what data they want to get rid of, and what optional controls they need to protect against deletes that are incorrect, either due to accidents or malice. When you tell Amazon S3 to delete data, that data is immediately deleted, and it cannot be recovered by AWS. Honoring a delete request in this way is an important characteristic of the service."
1233,What capabilities does Amazon S3 provide to protect my data against accidental or malicious deletes?,"S3 Object Versioning, S3 Replication, and S3 Object Lock are all optional features that you can use to add additional data protection, beyond the durability that S3 automatically provides. In addition, you can use a backup application to back up all or part of the data in your S3 buckets."
1234,What checksums does Amazon S3 support for data integrity checking?,"Amazon S3 uses a combination of Content-MD5 checksums, secure hash algorithms (SHAs), and cyclic redundancy checks (CRCs) to verify data integrity. Amazon S3 performs these checksums on data at rest and repairs any disparity using redundant data. In addition, S3 calculates checksums on all network traffic to detect alterations of data packets when storing or retrieving data. You can choose from four supported checksum algorithms for data integrity checking on your upload and download requests. You can choose a SHA-1, SHA-256, CRC32, or CRC32C checksum algorithm, depending on your application needs. You can automatically calculate and verify checksums as you store or retrieve data from S3, and can access the checksum information at any time using the GetObjectAttributes S3 API or an S3 Inventory report. Calculating a checksum as you stream data into S3 saves you time as you're able to both verify and transmit your data in a single pass, instead of as two sequential operations. Using checksums for data validation is a best practice for data durability, and these capabilities increase the performance and reduce the cost to do so."
1235,What is Versioning?,"Versioning allows you to preserve, retrieve, and restore every version of every object stored in an Amazon S3 bucket. Once you enable Versioning for a bucket, Amazon S3 preserves existing objects anytime you perform a PUT, POST, COPY, or DELETE operation on them. By default, GET requests will retrieve the most recently written version. Older versions of an overwritten or deleted object can be retrieved by specifying a version in the request."
1236,Why should I use Versioning?,Amazon S3 provides customers with a highly durable storage infrastructure. Versioning offers an additional level of protection by providing a means of recovery when customers accidentally overwrite or delete objects. This allows you to easily recover from unintended user actions and application failures. You can also use Versioning for data retention and archiving.
1237,How do I start using Versioning?,"You can start using Versioning by enabling a setting on your Amazon S3 bucket. For more information on how to enable Versioning, refer to the Amazon S3 documentation."
1238,How does Versioning protect me from accidental deletion of my objects?,"When a user performs a DELETE operation on an object, subsequent simple (un-versioned) requests will no longer retrieve the object. However, all versions of that object will continue to be preserved in your Amazon S3 bucket and can be retrieved or restored. Only the owner of an Amazon S3 bucket can permanently delete a version. You can set Lifecycle rules to manage the lifetime and the cost of storing multiple versions of your objects."
1239,"Can I set up a trash, recycle bin, or rollback window on my Amazon S3 objects to recover from deletes and overwrites?","You can use Amazon S3 Lifecycle rules along with S3 Versioning to implement a rollback window for your S3 objects. For example, with your versioning-enabled bucket, you can set up a rule that archives all of your previous versions to the lower-cost S3 Glacier Flexible Retrieval storage class and deletes them after 100 days, giving you a 100-day window to roll back any changes on your data while lowering your storage costs. Additionally, you can save costs by deleting old (noncurrent) versions of an object after five days and when there are at least two newer versions of the object. You can change the number of days or the number of newer versions based on your cost optimization needs. This allows you to retain additional versions of your objects when needed, but saves you cost by transitioning or removing them after a period of time."
1240,How can I ensure maximum protection of my preserved versions?,"Versioning's Multi-Factor Authentication (MFA) Delete capability can be used to provide an additional layer of security. By default, all requests to your Amazon S3 bucket require your AWS account credentials. If you enable Versioning with MFA Delete on your Amazon S3 bucket, two forms of authentication are required to permanently delete a version of an object: your AWS account credentials and a valid six-digit code and serial number from an authentication device in your physical possession. To learn more about enabling Versioning with MFA Delete, including how to purchase and activate an authentication device, refer to the Amazon S3 documentation."
1241,How am I charged for using Versioning?,"Normal Amazon S3 rates apply for every version of an object stored or requested. For example, let's look at the following scenario to illustrate storage costs when utilizing Versioning (let's assume the current month is 31 days long):"
1242,What is Amazon S3 Object Lock?,"Amazon S3 Object Lock is an Amazon S3 feature that prevents an object version from being deleted or overwritten for a fixed amount of time or indefinitely, so that you can enforce retention policies as an added layer of data protection or for regulatory compliance. You can migrate workloads from existing write-once-read-many (WORM) systems into Amazon S3, and configure S3 Object Lock at the object- and bucket-level to prevent object version deletions prior to pre-defined Retain Until Dates or indefinitely (Legal Hold Dates). S3 Object Lock protection is maintained regardless of which storage class the object version resides in and throughout S3 Lifecycle transitions between storage classes."
1243,How does Amazon S3 Object Lock work?,"Amazon S3 Object Lock prevents deletion of an object version for the duration of a specified retention period or indefinitely until a legal hold is removed. With S3 Object Lock, you're able to ensure that an object version remains immutable for as long as WORM protection is applied. You can apply WORM protection by either assigning a Retain Until Date or a Legal Hold to an object version using the AWS SDK, CLI, REST API, or the S3 Management Console. You can apply retention settings within a PUT request, or apply them to an existing object after it has been created."
1244,How do I enable Amazon S3 Object Lock on a bucket?,"You can use the Amazon S3 console, AWS API, or AWS CLI to enable S3 Object Lock while creating a new bucket or to configure S3 Object Lock on existing buckets. To enable S3 Object Lock on existing buckets, you can use the Amazon S3 console to edit S3 Object Lock settings in the bucket Properties tab the PutObjectLockConfiguration AWS API, or the AWS CLI. Once S3 Object Lock is enabled, you can set a default bucket level retention mode and time that will be applicable to all new objects uploaded to the bucket. For more information see the documentation on configuring S3 Object Lock using the S3 console, using the AWS API, and using the AWS CLI."
1245,Can I disable S3 Object Lock after I have enabled it?,"No, you cannot disable S3 Object Lock or S3 Versioning for buckets once S3 Object Lock is enabled."
1246,How do I get started with replicating objects from buckets with S3 Object Lock enabled?,"To start replicating objects with S3 Replication from buckets with S3 Object Lock enabled , you can add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS Region and in the same or different AWS account. You can choose to replicate all objects at the S3 bucket level, or filter objects on a shared prefix level, or an object level using S3 object tags. You will also need to specify an AWS Identity and Access Management (IAM) role with the required permissions to perform the replication operation. You can use the S3 console, AWS API, AWS CLI, AWS SDKs, or AWS CloudFormation to enable replication and must have S3 Versioning enabled for both the source and destination buckets. Additionally, to replicate objects from S3 Object Lock enabled buckets, your destination bucket must also have S3 Object Lock enabled. For more information see the documentation on setting up S3 Replication and using S3 Object Lock with S3 Replication."
1247,Do I need additional permissions to replicate objects from buckets with S3 Object Lock enabled?,"Yes, to replicate objects from S3 Object Lock enabled buckets you need to grant two new permissions, s3:GetObjectRetention and s3:GetObjectLegalHold, on the source bucket in the IAM role that you use to set up replication. Alternatively, if the IAM role has an s3:Get* permission, it satisfies the requirement. For more information see the documentation on using S3 Object Lock with S3 Replication."
1248,Are there any limitations for using S3 Replication while replicating from S3 Object Lock buckets?,"No, all features of S3 Replication, such as S3 Same-Region Replication (S3 SRR), S3 Cross-Region Replication (S3 CRR), S3 Replication metrics to track progress, S3 Replication Time Control (S3 RTC), and S3 Batch Replication, are supported while replicating from S3 Object Lock buckets."
1249,How can I replicate existing objects from S3 Object Lock enabled buckets?,"You can use S3 Batch Replication to replicate existing objects from S3 Object Lock enabled buckets. For more information on replicating existing objects, see the documentation on S3 Batch Replication."
1250,Retention mode,S3 Intelligent-Tiering | S3 Standard | S3 Express One Zone | S3 Standard-Infrequent Access | S3 One Zone-Infrequent Access | Amazon S3 Glacier Instant Retrieval | Amazon S3 Glacier Flexible Retrieval | Amazon S3 Glacier Deep Archive | S3 on Outposts
1251,How do I decide which S3 storage class to use?,"The Amazon S3 Glacier storage classes are purpose-built for data archiving, providing you with the highest performance, most retrieval flexibility, and the lowest cost archive storage in the cloud. You can now choose from three archive storage classes optimized for different access pattern and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval, with retrieval in minutes or free bulk retrievals in 5 12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval within 12 hours. All these storage classes provide multi-Availability Zone (AZ) resiliency by redundantly storing data on multiple devices and physically separated AWS Availability Zones in an AWS Region.  For data that has a lower resiliency requirement, you can reduce costs by selecting a single-AZ storage class, like S3 One Zone-Infrequent Access. If you have data residency or latency requirements that can't be met by an existing AWS Region, you can choose S3 on Outposts to store data on-premises.  You can learn more about these storage classes on the Amazon S3 Storage Classes page."
1252,What is S3 Intelligent-Tiering?,"S3 Intelligent-Tiering is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. S3 Intelligent-Tiering delivers milliseconds latency and high throughput performance for frequently, infrequently, and rarely accessed data in the Frequent, Infrequent, and Archive Instant Access tiers. For a small monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors the access pattern and moves the objects automatically from one tier to another. There are no retrieval charges in S3 Intelligent-Tiering, so you won't see unexpected increases in storage bills when access pattern change."
1253,How does S3 Intelligent-Tiering work?,"The Amazon S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access pattern change. For a low monthly object monitoring and automation charge, S3 Intelligent-Tiering monitors access pattern and automatically moves objects that have not been accessed for 30 consecutive days to the Infrequent Access tier to save up to 40% on storage costs. After 90 days consecutive days of no access, objects are moved to the Archive Instant Access tier to save up to 68% on storage costs. There is no impact on performance and there are no retrieval charges in S3 Intelligent-Tiering. If an object in the Infrequent Access tier or Archive Instant Access tier is accessed later, it is automatically moved back to the Frequent Access tier."
1254,Why would I choose to use S3 Intelligent-Tiering?,"You can use S3 Intelligent-Tiering as the default storage class for virtually any workload, especially data lakes, data analytics, machine learning, new applications, and user-generated content. S3 Intelligent-Tiering is the first cloud storage that automatically reduces your storage costs on a granular object level by automatically moving data to the most cost-effective access tier based on access frequency, without performance impact, retrieval fees, or operational overhead. If you have data with unknown or changing access pattern, including data lakes, data analytics, and new applications, we recommend using S3 Intelligent-Tiering. If you have data that does not require immediate retrieval, we recommend activating the Deep Archive Access tier where you pay as little as $1 per TB per month for data that may become rarely accessed over long periods of time. S3 Intelligent-Tiering is for data with unknown or changing access pattern. There are no retrieval fees when using the S3 Intelligent-Tiering storage class."
1255,What performance does S3 Intelligent-Tiering offer?,"S3 Intelligent-Tiering automatically optimizes your storage costs without an impact to your performance. The S3 Intelligent-Tiering Frequent, Infrequent, and Archive Instant Access tiers provide milliseconds latency and high throughput performance."
1256,How durable and available is S3 Intelligent-Tiering?,"S3 Intelligent-Tiering is designed for the same 99.999999999% durability as the S3 Standard storage class. S3 Intelligent-Tiering is designed for 99.9% availability, and carries a service level agreement providing service credits if availability is less than our service commitment in any billing cycle."
1257,How do I get my data into S3 Intelligent-Tiering?,There are two ways to get data into S3 Intelligent-Tiering. You can directly PUT into S3 Intelligent-Tiering by specifying INTELLIGENT_TIERING in the x-amz-storage-class header or set lifecycle policies to transition objects from S3 Standard or S3 Standard-IA to S3 INTELLIGENT_TIERING.
1258,How am I charged for S3 Intelligent-Tiering?,"S3 Intelligent-Tiering charges you for monthly storage, requests, and data transfer, and charges a small monthly charge for monitoring and automation per object. The S3 Intelligent-Tiering storage class automatically stores objects in three access tiers: a Frequent Access tier priced at S3 Standard storage rates, an Infrequent Access tier priced at S3 Standard-Infrequent Access storage rates, and an Archive Instant Access tier priced at the S3 Glacier Instant Retrieval storage rates. S3 Intelligent-Tiering also has two optional archive tiers designed for asynchronous access, an Archive Access tier priced at S3 Glacier Flexible Retrieval storage rates, and a Deep Archive Access tier priced at S3 Glacier Deep Archive storage rates.  For a small monitoring and automation fee, S3 Intelligent-Tiering monitors access pattern and automatically moves objects through low latency and high throughput access tiers, as well as two opt in asynchronous archive access tiers where customers get the lowest storage costs in the cloud for data that can be accessed asynchronously.  There is no minimum billable object size in S3 Intelligent-Tiering, but objects smaller than 128KB are not eligible for auto-tiering. These small objects will not be monitored and will always be charged at the Frequent Access tier rates, with no monitoring and automation charge. For each object archived to the Archive Access tier or Deep Archive Access tier in S3 Intelligent-Tiering, Amazon S3 uses 8 KB of storage for the name of the object and other metadata (billed at S3 Standard storage rates) and 32 KB of storage for index and related metadata (billed at S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage rates)."
1259,Is there a charge to retrieve data from S3 Intelligent-Tiering?,"No. There are no retrieval fees for S3 Intelligent-Tiering. S3 Intelligent-Tiering monitors the access pattern of your data and if you access an object in the Infrequent Access, Archive Instant Access, or the asynchronous archive tiers, S3 Intelligent-Tiering automatically moves that object to the Frequent Access tier."
1260,Can I extend the time before objects are archived within S3 Intelligent-Tiering storage class?,"Yes. In the bucket, prefix, or object tag level configuration, you can extend the last access time for archiving objects in S3 Intelligent-Tiering. When enabled, by default objects that haven't been accessed for a minimum of 90 consecutive days automatically move to the Archive Access tier, skipping the Archive Instant Access tier. Objects that haven't been accessed for a minimum of 180 consecutive days automatically move to the Deep Archive Access tier. The default configuration for the consecutive days since last access before automatic archiving in S3 Intelligent-Tiering can be extended for up to 2 years."
1261,How do I access an object from the Archive Access or Deep Archive Access tiers in the S3 Intelligent-Tiering storage class?,"To access an object in the Archive or Deep Archive Access tiers, you need to issue a Restore request and the object will begin moving back to the Frequent Access tier, all within the S3 Intelligent-Tiering storage class. Objects in the Archive Access Tier are moved to the Frequent Access tier in 3-5 hours, objects in the Deep Archive Access tier are moved to the Frequent Access tier within 12 hours. Once the object is in the Frequent Access tier, you can issue a GET request to retrieve the object."
1262,How do I know in which S3 Intelligent-Tiering access tier my objects are stored in?,"You can use Amazon S3 Inventory to report the access tier of objects stored in the S3 Intelligent-Tiering storage class. Amazon S3 Inventory provides CSV, ORC, or Parquet output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix. You can also make a HEAD request on your objects to report the S3 Intelligent-Tiering archive access tiers."
1263,Can I lifecycle objects from S3 Intelligent-Tiering to another storage class?,"Yes. You can lifecycle objects from S3 Intelligent-Tiering Frequent Access, Infrequent, and Archive Instant Access tiers to S3 One-Zone Infrequent Access, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive. In addition, you can lifecycle objects from the S3 Intelligent-Tiering optional archive access tiers to S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive, and from the S3 Intelligent-Tiering Deep Archive Access tier to S3 Glacier Deep Archive."
1264,Is there a minimum duration for S3 Intelligent-Tiering?,No. The S3 Intelligent-Tiering storage class has no minimum storage duration.
1265,Is there a minimum billable object size for S3 Intelligent-Tiering?,"No. The S3 Intelligent-Tiering storage class has no minimum billable object size, but objects smaller than 128KB are not eligible for auto-tiering. These smaller objects will always be charged at the Frequent Access tier rates, with no monitoring and automation charge. For each object archived to the opt-in Archive Access tier or Deep Archive Access tier in S3 Intelligent-Tiering, Amazon S3 uses 8 KB of storage for the name of the object and other metadata (billed at S3 Standard storage rates) and 32 KB of storage for index and related metadata (billed at S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage rates). For more details, visit the Amazon S3 pricing page."
1266,What is S3 Standard?,"Amazon S3 Standard delivers durable storage with millisecond access latency and high throughput performance for frequently accessed data, typically more than once per month. S3 Standard is designed for performance-sensitive uses cases, such as data lakes, cloud-native applications, dynamic websites, content distribution, mobile and gaming applications, analytics, and machine learning models. S3 Standard is designed for 99.99% data availability and durability of 99.999999999% of objects across multiple Availability Zones in a given year. You can use S3 Lifecycle policies to control exactly when data is transitioned between S3 Standard and lower costs storage classes without any application changes."
1267,Why would I choose to use S3 Standard?,"S3 Standard is ideal for your most frequently accessed or modified data that requires access in milliseconds and high throughput performance. S3 Standard is ideal for data that is read or written very often, as there are no retrieval charges. S3 Standard is optimized for a wide variety of use cases, including data lakes, cloud native applications, dynamic websites, content distribution, mobile and gaming applications, and analytics."
1268,What is the Amazon S3 Express One Zone storage class?,"Amazon S3 Express One Zone is a high-performance, single-Availability Zone Amazon S3 storage class purpose-built to deliver consistent single-digit millisecond data access for customers' most latency-sensitive applications. Amazon S3 Express One Zone is the lowest latency cloud object storage class available today, with data access speed up to 10x faster and with request costs 50% lower than Amazon S3 Standard. With S3 Express One Zone, you can select a specific AWS Availability Zone within an AWS Region to store your data. You can choose to co-locate your storage and compute resources in the same Availability Zone to further optimize performance."
1269,Why would I choose to use the Amazon S3 Express One Zone storage class?,"S3 Express One Zone is the ideal storage class for applications that need the fastest data access speed and highest performance for latency-sensitive applications. S3 Express One Zone is the best storage class for request-intensive operations such as machine learning (ML) training and inference, interactive analytics, and media content creation."
1270,How do I get started with the Amazon S3 Express One Zone storage class?,"You can get started by creating an S3 directory bucket in an AWS Availability Zone (AZ) of your choosing. You can choose to co-locate your storage and compute resources in the same AZ to further optimize performance. Directory buckets have S3 Block Public Access on by default. After creating the directory bucket, you can directly upload objects to the S3 Express One Zone storage class or copy objects from existing S3 storage classes into S3 Express One Zone. You can also import data with a single click in the AWS Management Console into S3 Express One Zone or use S3 Batch Operations to copy an entire bucket, prefix, or subsets of data from an existing S3 storage class into S3 Express One Zone."
1271,How many Availability Zones are Amazon S3 Express One Zone objects stored in?,"S3 Express One Zone objects are stored in a single AWS Availability Zone (AZ) that you choose. Storing objects in one zone gives you the ability to store your data local to your compute to minimize latency. You can access data from across Availability Zones, although latency will increase."
1272,What performance does the Amazon S3 Express One Zone storage class provide?,"S3 Express One Zone provides similar performance elasticity as other S3 storage classes, but with consistent single-digit millisecond first-byte read and write latency request latencies up to 10x faster than existing S3 storage classes. With S3 Express One Zone, customers don't need to plan or provision capacity or throughput requirements in advance, and benefit immediately from requests completing up to an order of magnitude faster. S3 Express One Zone is ideal for analytics jobs where storage latency speeds job completion times and reduces overall TCO. It's also ideal for interactive workloads, like video editing, where creative professionals need the most responsive possible access to their S3 data."
1273,How does the Amazon S3 Express One Zone storage class achieve high performance?,"S3 Express One Zone uses a unique architecture to optimize for performance and deliver consistently low request latency. S3 Express One Zone stores data on high-performance hardware and its object protocol has been enhanced to streamline authentication and metadata overheads. Additionally, to further increase access speed and support hundreds of thousands of requests per second, data is stored in a new bucket type an Amazon S3 directory bucket. With S3 Express One Zone, you can select a specific AWS Availability Zone within an AWS Region to store your data. You can choose to co-locate your storage and compute resources in the same Availability Zone to further optimize performance."
1274,What request rate performance does an S3 directory bucket support?,"Each S3 directory bucket can support hundreds of thousands of transactions per second (TPS), independent of the number of directories within the bucket."
1275,How should I plan for my application's throughput needs with the S3 Express One Zone storage class?,"S3 Express One Zone provides similar high, elastic throughput as other Amazon S3 storage classes. S3 Express One Zone is designed from the ground up to allow individual customers to burst throughput to very high aggregate levels. For example, machine learning model training applications can train against millions of objects and petabytes of data. You can achieve the highest performance by spreading these requests over separate connections to maximize the accessible bandwidth."
1276,How is request authorization different with Amazon S3 Express One Zone compared to other S3 storage classes?,"With S3 Express One Zone, you authenticate and authorize requests through a new session-based mechanism, S3 CreateSession, which is optimized to provide the lowest latency. You can use CreateSession to request temporary credentials that provide low latency access to your bucket. These temporary credentials are scoped to a specific S3 directory bucket. For more information on this session-based model, refer to S3 Create Session in the developer guide."
1277,How reliable is the Amazon S3 Express One Zone storage class?,"S3 Express One Zone is designed to deliver 99.95% availability within a single Availability Zone, with an availability SLA of 99.9%."
1278,How is the Amazon S3 Express One Zones storage class designed to provide 99.95% availability?,"With S3 Express One Zone, your data is redundantly stored on multiple devices within a single AZ. S3 Express One Zone is designed to sustain concurrent device failures by quickly detecting and repairing any lost redundancy. This means that S3 Express One Zone automatically shifts requests to new devices within an AZ if the existing device encounters a failure. This redundancy gives you uninterrupted access to your data within an AZ."
1279,How am I charged for Amazon S3 Express One Zone?,"There are no set up charges or commitments to begin using S3 Express One Zone. S3 Express One Zone charges you for storage and requests. The volume of storage billed in a month is accrued based on total storage used per hour, measured in gigabyte per month (GB-Month). You are also charged a per request fee for access based on the request type such as PUTs and GETs. You will pay an additional per-GB fee for the portion of the request size exceeding 512 KB."
1280,Are there any additional Data Transfer charges for using the Amazon S3 Express One Zone storage class within the same Region?,"The request charges to access data in S3 Express One Zone includes costs to transfer data within the AWS network in a Region, and there is no additional Data Transfer charge for data transferred between Amazon EC2 (or any AWS service) and S3 Express One Zone within the same Region, for example, data transferred within the US East (Northern Virginia) Region."
1281,Are there any additional networking charges for using Gateway VPC endpoints with the Amazon S3 Express One Zone storage class?,"The request charges to access data in S3 Express One Zone includes costs to use Gateway VPC endpoints, and there is no additional charge for using Gateway endpoints with S3 Express One Zone."
1282,What is S3 Standard-Infrequent Access?,"Amazon S3 Standard-Infrequent Access (S3 Standard-IA) is an Amazon S3 storage class for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers the high durability, throughput, and low latency of the Amazon S3 Standard storage class, with a low per-GB storage price and per-GB retrieval charge. This combination of low cost and high performance make S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery. The S3 Standard-IA storage class is set at the object level and can exist in the same bucket as the S3 Standard or S3 One Zone-IA storage classes, allowing you to use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes."
1283,Why would I choose to use S3 Standard-IA?,"S3 Standard-IA is ideal for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA is ideally suited for long-term file storage, older sync and share storage, and other aging data."
1284,What performance does S3 Standard-IA offer?,S3 Standard-IA provides the same milliseconds latency and high throughput performance as the S3 Standard storage class.
1285,How do I get my data into S3 Standard-IA?,There are two ways to get data into S3 Standard-IA. You can directly PUT into S3 Standard-IA by specifying STANDARD_IA in the x-amz-storage-class header. You can also set Lifecycle policies to transition objects from the S3 Standard to the S3 Standard-IA storage class.
1286,What charges will I incur if I change the storage class of an object from S3 Standard-IA to S3 Standard with a COPY request?,"You will incur charges for an S3 Standard (destination storage class) COPY request and an S3 Standard-IA (source storage class) data retrieval. For more information, visit the Amazon S3 pricing page."
1287,Is there a minimum storage duration charge for S3 Standard-IA?,"S3 Standard-IA is designed for long-lived, infrequently accessed data that is retained for months or years. Data that is deleted from S3 Standard-IA within 30 days will be charged for a full 30 days. See the Amazon S3 pricing page for information about S3 Standard-IA pricing."
1288,Is there a minimum object storage charge for S3 Standard-IA?,"S3 Standard-IA is designed for larger objects and has a minimum object storage charge of 128KB. Objects smaller than 128KB in size will incur storage charges as if the object were 128KB. For example, a 6KB object in S3 Standard-IA will incur S3 Standard-IA storage charges for 6KB and an additional minimum object size charge equivalent to 122KB at the S3 Standard-IA storage price. See the Amazon S3 pricing page for information about S3 Standard-IA pricing."
1289,Can I tier objects from S3 Standard-IA to S3 One Zone-IA or to the S3 Glacier Flexible Retrieval storage class?,"Yes. In addition to using Lifecycle policies to migrate objects from S3 Standard to S3 Standard-IA, you can also set up Lifecycle policies to tier objects from S3 Standard-IA to S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and the S3 Glacier Deep Archive storage class."
1290,What is S3 One Zone-IA storage class?,"S3 One Zone-IA storage class is an Amazon S3 storage class that customers can choose to store objects in a single availability zone. S3 One Zone-IA storage redundantly stores data within that single Availability Zone to deliver storage at 20% less cost than geographically redundant S3 Standard-IA storage, which stores data redundantly across multiple geographically separate Availability Zones."
1291,What use cases are best suited for S3 One Zone-IA storage class?,"Customers can use S3 One Zone-IA for infrequently-accessed storage, like backup copies, disaster recovery copies, or other easily re-creatable data."
1292,What performance does S3 One Zone-IA storage offer?,S3 One Zone-IA storage class offers the same latency and throughput performance as the S3 Standard and S3 Standard-Infrequent Access storage classes.
1293,How durable is the S3 One Zone-IA storage class?,"S3 One Zone-IA storage class is designed for 99.999999999% of durability within an Availability Zone. However, data in the S3 One Zone-IA storage class is not resilient to the loss of availability or physical loss of an Availability Zone. In contrast, S3 Standard, S3 Intelligent-Tiering, S3 Standard-Infrequent Access, and the S3 Glacier storage classes are designed to withstand loss of availability or the destruction of an Availability Zone. S3 One Zone-IA can deliver the same or better durability and availability than most modern, physical data centers, while providing the added benefit of elasticity of storage and the Amazon S3 feature set."
1294,Is an S3 One Zone-IA Zone the same thing as an AWS Availability Zone?,"Yes. Each AWS Region is a separate geographic area. Each Region has multiple, isolated locations known as Availability Zones. The Amazon S3 One Zone-IA storage class uses an individual AWS Availability Zone within the Region."
1295,How much disaster recovery protection do I forgo by using S3 One Zone-IA?,"Each Availability Zone uses redundant power and networking. Within an AWS Region, Availability Zones are on different flood plains, earthquake fault zones, and geographically separated for fire protection. S3 Standard and S3 Standard-IA storage classes offer protection against these sorts of disasters by storing your data redundantly in multiple Availability Zones. S3 One Zone-IA offers protection against equipment failure within an Availability Zone, but the data is not resilient to the physical loss of the Availability Zone resulting from disasters, such as earthquakes and floods. Using S3 One Zone-IA, S3 Standard, and S3 Standard-IA options, you can choose the storage class that best fits the durability and availability needs of your storage."
1296,What is the S3 Glacier Instant Retrieval storage class?,"The S3 Glacier Instant Retrieval storage class delivers the lowest cost storage for long-lived data that is rarely accessed and requires milliseconds retrieval. S3 Glacier Instant Retrieval delivers the fastest access to archive storage, with the same throughput and milliseconds access as S3 Standard and S3 Standard-IA storage classes. S3 Glacier Instant Retrieval is designed for 99.999999999% (11 9s) of data durability and 99.9% availability by redundantly storing data across a minimum of three physically separated AWS Availability Zones."
1297,Why would I choose to use S3 Glacier Instant Retrieval?,"S3 Glacier Instant Retrieval is ideal if you have data that is rarely accessed (once a quarter) and requires milliseconds retrieval times. It's the ideal storage class if you want the same low latency and high throughput performance as S3 Standard-IA, but store data that is accessed less frequently than S3 Standard-IA, with a lower storage price and slightly higher data access costs."
1298,How available and durable is S3 Glacier Instant Retrieval?,"S3 Glacier Instant Retrieval is designed for 99.999999999% (11 9s) of durability and 99.9% availability, the same as S3 Standard-IA, and carries a service level agreement providing service credits if availability is less than 99% in any billing cycle."
1299,What performance does S3 Glacier Instant Retrieval offer?,"S3 Glacier Instant Retrieval provides the same milliseconds latency and high throughput performance as the S3 Standard and S3 Standard-IA storage classes. Unlike the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes, which are designed for asynchronous access, you do not need to issue a Restore request before accessing an object stored in S3 Glacier Instant Retrieval."
1300,How do I get my data into S3 Glacier Instant Retrieval?,There are two ways to get data into S3 Glacier Instant Retrieval. You can directly PUT into S3 Glacier Instant retrieval by specifying GLACIER_IR in the x-amz-storage-class header or set S3 Lifecycle policies to transition objects from S3 Standard or S3 Standard-IA to S3 Glacier Instant Retrieval.
1301,Is there a minimum storage duration charge for Amazon S3 Glacier Instant Retrieval?,"S3 Glacier Instant Retrieval is designed for long-lived, rarely accessed data that is retained for months or years. Objects that are archived to S3 Glacier Instant Retrieval have a minimum of 90 days of storage, and objects deleted, overwritten, or transitioned before 90 days incur a pro-rated charge equal to the storage charge for the remaining days. View the Amazon S3 pricing page for information about Amazon S3 Glacier Instant Retrieval pricing."
1302,Is there a minimum object size charge for Amazon S3 Glacier Instant Retrieval?,"S3 Glacier Instant Retrieval is designed for larger objects and has a minimum object storage charge of 128KB. Objects smaller than 128KB in size will incur storage charges as if the object were 128KB. For example, a 6KB object in S3 Glacier Instant Retrieval will incur S3 Glacier Instant Retrieval storage charges for 6KB and an additional minimum object size charge equivalent to 122KB at the S3 Glacier Instant Retrieval storage price. View the Amazon S3 pricing page for information about Amazon S3 Glacier Instant Retrieval pricing."
1303,How am I charged for S3 Glacier Instant Retrieval?,"S3 Glacier Instant Retrieval charges you for monthly storage, requests based on the request type, and data retrievals. The volume of storage billed in a month is based on average storage used throughout the month, measured in gigabyte per month (GB-Month). You are charged for requests based on the request type such as PUTs, COPYs, and GETs. You also pay a per GB fee for every gigabyte of data returned to you."
1304,What is the S3 Glacier Flexible Retrieval storage class?,"The S3 Glacier Flexible Retrieval storage class delivers low-cost storage, up to 10% lower cost (than S3 Glacier Instant Retrieval), for archive data that is accessed 1-2 times per year and is retrieved asynchronously, with free bulk retrievals. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, S3 Glacier Flexible Retrieval is the ideal storage class. S3 Glacier Flexible Retrieval delivers the most flexible retrieval options that balance cost with access times ranging from minutes to hours and with free bulk retrievals. It is an ideal solution for backup, disaster recovery, offsite data storage needs, and for when some data needs to occasionally retrieved in minutes, and you don't want to worry about costs. S3 Glacier Flexible Retrieval is designed for 99.999999999% (11 9s) of data durability and 99.99% availability by redundantly storing data across multiple physically separated AWS Availability Zones in a given year."
1305,Why would I choose to use S3 Glacier Flexible Retrieval storage class?,"For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, S3 Glacier Flexible Retrieval is the ideal storage class. S3 Glacier Flexible Retrieval delivers the most flexible retrieval options that balance cost with access times ranging from minutes to hours and with free bulk retrievals. It is an ideal solution for backup, disaster recovery, offsite data storage needs, and for when some data needs to occasionally retrieved in minutes, and you don't want to worry about costs to retrieve the data."
1306,How do I get my into S3 Glacier Flexible Retrieval?,"There are two ways to get data into S3 Glacier Flexible Retrieval. You can directly PUT into S3 Glacier Flexible Retrieval by specifying GLACIER in the x-amz-storage-class header. You can also use S3 Lifecycle rules to transition objects from any of the S3 storage classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 One Zone-IA, and S3 Glacier Instant Retrieval) to Amazon S3 Glacier Flexible Retrieval based on object age. Use the Amazon S3 Management Console, the AWS SDKs, or the Amazon S3 APIs to directly PUT into Amazon S3 Glacier or define rules for archival.  Note: S3 Glacier Flexible Retrieval is also available through the original direct Glacier APIs and through the Amazon S3 Glacier Management Console. For an enhanced experience complete with access to the full S3 feature set including lifecycle management, S3 Replication, S3 Storage Lens, and more, we recommend using S3 APIs and the S3 Management Console to use S3 Glacier features."
1307,How can I retrieve my objects that are archived in S3 Glacier Flexible Retrieval and will I be notified when the object is restored?,"Objects that are archived in S3 Glacier Flexible Retrieval are accessed asynchronously. To retrieve data stored in S3 Glacier Flexible Retrieval, initiate a retrieval request using the Amazon S3 APIs or the Amazon S3 console. The retrieval request creates a temporary copy of your data in the S3 Standard storage class while leaving the archived data intact in S3 Glacier Flexible Retrieval. You can specify the amount of time in days for which the temporary copy is stored in Amazon S3. You can then access your temporary copy from S3 through an Amazon S3 GET request on the archived object. In AWS Regions where Reduced Redundancy Storage is a lower price than S3 Standard, temporarily available data is billed as Reduced Redundancy Storage. However, the Reduced Redundancy billing storage class doesn't reflect how the data is stored."
1308,How long will it take to restore my objects archived in Amazon S3 Glacier Flexible Retrieval?,"When processing a retrieval job, Amazon S3 first retrieves the requested data from S3 Glacier Flexible Retrieval, and then creates a temporary copy of the requested data in Amazon S3. This typically takes a few minutes. The access time of your request depends on the retrieval option you choose: Expedited, Standard, or Bulk retrievals. For all but the largest objects (250MB+), data accessed using Expedited retrievals are typically made available within 1-5 minutes. Objects retrieved using Standard retrievals typically complete between 3-5 hours. Standard retreivals typically start in minutes when initiated using S3 Batch Operations. Bulk retrievals typically complete within 5 12 hours, and are free of charge. For more information about the S3 Glacier Flexible Retrieval options, refer to restoring an archived object in the S3 user guide."
1309,How is my storage charge calculated for Amazon S3 objects archived to S3 Glacier Flexible Retrieval?,"The volume of storage billed in a month is based on average storage used throughout the month, measured in gigabyte-months (GB-Months). Amazon S3 calculates the object size as the amount of data you stored, plus an additional 32 KB of S3 Glacier data, plus an additional 8 KB of Amazon S3 Standard storage class data. S3 Glacier Flexible Retrieval requires an additional 32 KB of data per object for S3 Glacier's index and metadata so you can identify and retrieve your data. Amazon S3 requires 8 KB to store and maintain the user-defined name and metadata for objects archived to S3 Glacier Flexible Retrieval. This enables you to get a real-time list of all of your Amazon S3 objects, including those stored using S3 Glacier Flexible Retrieval, using the Amazon S3 LIST API, or the S3 inventory report."
1310,Are there minimum storage duration and minimum object storage charges for Amazon S3 Glacier Flexible Retrieval?,"Objects archived to S3 Glacier Flexible Retrieval have a minimum of 90 days of storage. If an object is deleted, overwritten, or transitioned before 90 days, a pro-rated charge equal to the storage charge for the remaining days will be incurred.   S3 Glacier Flexible Retrieval also requires 40 KB of additional metadata for each archived object. This includes 32 KB of metadata charged at the S3 Glacier Flexible Retrieval rate required to identify and retrieve your data. And, an additional 8 KB data charged at the S3 Standard rate which is required to maintain the user-defined name and metadata for objects archived to S3 Glacier Flexible Retrieval. This allows you to get a real-time list of all of your S3 objects using the S3 LIST API or the S3 Inventory report. View the Amazon S3 pricing page for information about Amazon S3 Glacier Flexible Retrieval pricing."
1311,How much does it cost to retrieve data from Amazon S3 Glacier Flexible Retrieval?,"There are three ways to retrieve data from S3 Glacier Flexible Retrieval: Expedited, Standard, and Bulk Retrievals. Expedited and Standard have a per-GB retrieval fee and per-request fee (i.e., you pay for requests made against your Amazon S3 objects). Bulk Retrievals from S3 Glacier Flexible Retrieval are free. For detailed S3 Glacier pricing by AWS Region, visit the Amazon S3 pricing page."
1312,Does Amazon S3 provide capabilities for archiving objects to lower cost storage classes?,"The Amazon S3 Glacier storage classes are purpose-built for data archiving, providing you with the highest performance, most retrieval flexibility, and the lowest cost archive storage in the cloud. You can now choose from three archive storage classes optimized for different access pattern and storage duration. For archive data that needs immediate access, such as medical images, news media assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive storage class that delivers the lowest cost storage with milliseconds retrieval. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval, with retrieval in minutes or free bulk retrievals in 5 12 hours. To save even more on long-lived archive storage such as compliance archives and digital media preservation, choose S3 Glacier Deep Archive, the lowest cost storage in the cloud with data retrieval within 12 hours."
1313,What is the backend infrastructure supporting the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage class?,"We prefer to focus on the customer outcomes of performance, durability, availability, and security. However, this question is often asked by our customers. We use a number of different technologies which allow us to offer the prices we do to our customers. Our services are built using common data storage technologies specifically assembled into purpose-built, cost-optimized systems using AWS-developed software. The S3 Glacier storage classes benefit from our ability to optimize the sequence of inputs and outputs to maximize efficiency accessing the underlying storage."
1314,What is the Amazon S3 Glacier Deep Archive storage class?,"S3 Glacier Deep Archive is an Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year. From just $0.00099 per GB-month (less than one-tenth of one cent, or about $1 per TB-month), S3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices significantly lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data off-site."
1315,What use cases are best suited for the S3 Glacier Deep Archive storage class?,"S3 Glacier Deep Archive is an ideal storage class to provide offline protection of your company's most important data assets, or when long-term data retention is required for corporate policy, contractual, or regulatory compliance requirements. Customers find S3 Glacier Deep Archive to be a compelling choice to protect core intellectual property, financial and medical records, research results, legal documents, seismic exploration studies, and long-term backups, especially in highly regulated industries, such as Financial Services, Healthcare, Oil & Gas, and Public Sectors. In addition, there are organizations, such as media and entertainment companies, that want to keep a backup copy of core intellectual property. Frequently, customers using S3 Glacier Deep Archive can reduce or discontinue the use of on-premises magnetic tape libraries and off-premises tape archival services."
1316,"How does the S3 Glacier Deep Archive storage class differ from the S3 Glacier Instant Retrieval, and S3 Glacier Flexible Retrieval storage classes?","S3 Glacier Deep Archive expands our data archiving offerings, enabling you to select the optimal storage class based on storage and retrieval costs, and retrieval times. Choose the S3 Glacier Instant Retrieval storage class when you need milliseconds access to low cost archive data. For archive data that does not require immediate access but needs the flexibility to retrieve large sets of data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible Retrieval, with retrieval in minutes or free bulk retrievals in 5-12 hours. S3 Glacier Deep Archive, in contrast, is designed for colder data that is very unlikely to be accessed, but still requires long-term, durable storage. S3 Glacier Deep Archive is up to 75% less expensive than S3 Glacier Flexible Retrieval and provides retrieval within 12 hours using the Standard retrieval tier. Standard retreivals typically start within 9 hours when initiated using S3 Batch Operations. You may also reduce retrieval costs by selecting Bulk retrieval, which will return data within 48 hours."
1317,How do I get started using S3 Glacier Deep Archive?,"The easiest way to store data in S3 Glacier Deep Archive is to use the S3 API to upload data directly. Just specify S3 Glacier Deep Archive as the storage class. You can accomplish this using the AWS Management Console, S3 REST API, AWS SDKs, or AWS Command Line Interface."
1318,How do you recommend migrating data from my existing tape archives to S3 Glacier Deep Archive?,"There are multiple ways to migrate data from existing tape archives to S3 Glacier Deep Archive. You can use the AWS Tape Gateway to integrate with existing backup applications using a virtual tape library (VTL) interface. This interface presents virtual tapes to the backup application. These can be immediately used to store data in Amazon S3, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive."
1319,How can I retrieve my objects stored in S3 Glacier Deep Archive?,"To retrieve data stored in S3 Glacier Deep Archive, initiate a Restore request using the Amazon S3 APIs or the Amazon S3 Management Console. The Restore creates a temporary copy of your data in the S3 Standard storage class while leaving the archived data intact in S3 Glacier Deep Archive. You can specify the amount of time in days for which the temporary copy is stored in S3. You can then access your temporary copy from S3 through an Amazon S3 GET request on the archived object."
1320,How am I charged for using S3 Glacier Deep Archive?,"S3 Glacier Deep Archive storage is priced based on the amount of data you store in GBs, the number of PUT/lifecycle transition requests, retrievals in GBs, and number of restore requests. This pricing model is similar to S3 Glacier Flexible Retrieval. See the Amazon S3 pricing page for information about S3 Glacier Deep Archive pricing."
1321,How will S3 Glacier Deep Archive usage show up on my AWS bill and in the AWS Cost Management tool?,"S3 Glacier Deep Archive usage and cost will show up as an independent service line item on your monthly AWS bill, separate from your Amazon S3 usage and costs. However, if you are using the AWS Cost Management tool, S3 Glacier Deep Archive usage and cost will be included under the Amazon S3 usage and cost in your detailed monthly spend reports, and not broken out as a separate service line item."
1322,Are there minimum storage duration and minimum object storage charges for S3 Glacier Deep Archive?,"Objects that are archived to S3 Glacier Deep Archive have a minimum of 180 days of storage. If an object is deleted, overwritten, or transitioned before 180 days, a pro-rated charge equal to the storage charge for the remaining days will be incurred."
1323,How does S3 Glacier Deep Archive integrate with other AWS Services?,"S3 Glacier Deep Archive is integrated with Amazon S3 features, including S3 Object Tagging, S3 Lifecycle policies, S3 Object Lock, and S3 Replication. With S3 storage management features, you can use a single Amazon S3 bucket to store a mixture of S3 Glacier Deep Archive, S3 Standard, S3 Standard-IA, S3 One Zone-IA, and S3 Glacier Flexible Retrieval data. This allows storage administrators to make decisions based on the nature of the data and data access pattern. Customers can use Amazon S3 Lifecycle policies to automatically migrate data to lower-cost storage classes as the data ages, or S3 Cross-Region Replication or Same-Region Replication policies to replicate data to the same or a different region."
1324,What is Amazon S3 on Outposts?,"Amazon S3 on Outposts delivers object storage in your on-premises environment, using the S3 APIs and capabilities that you use in AWS today. AWS Outposts is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any datacenter, co-location space, or on-premises facility. Using S3 on Outposts, you can securely process and store customer data generated on-premises before moving it to an AWS Region, access data locally for applications that run on-premises, or store data on your Outpost for companies in locations with data residency requirements, and or those in regulated industries. To learn more about S3 on Outposts, visit the overview page."
1325,What are S3 Object Tags?,"S3 Object Tags are key-value pairs applied to S3 objects which can be created, updated or deleted at any time during the lifetime of the object. With these, you have the ability to create Identity and Access Management (IAM) policies, set up S3 Lifecycle policies, and customize storage metrics. These object-level tags can then manage transitions between storage classes and expire objects in the background. You can add tags to new objects when you upload them or you can add them to existing objects. Up to ten tags can be added to each S3 object and you can use either the AWS Management Console, the REST API, the AWS CLI, or the AWS SDKs to add object tags."
1326,Why should I use object tags?,"Object tags are a tool you can use to enable simple management of your S3 storage. With the ability to create, update, and delete tags at any time during the lifetime of your object, your storage can adapt to the needs of your business. These tags allow you to control access to objects tagged with specific key-value pairs, allowing you to further secure confidential data for only a select group or user. Object tags can also be used to label objects that belong to a specific project or business unit, which could be used in conjunction with S3 Lifecycle policies to manage transitions to other storage classes (S3 Standard-IA, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive) or with S3 Replication to selectively replicate data between AWS Regions."
1327,How can I update the object tags on my objects?,"Object tags can be changed at any time during the lifetime of your S3 object, you can use either the AWS Management Console, the REST API, the AWS CLI, or the AWS SDKs to change your object tags. Note that all changes to tags outside of the AWS Management Console are made to the full tag set. If you have five tags attached to a particular object and want to add a sixth, you need to include the original five tags in that request."
1328,How much do object tags cost?,Object tags are priced based on the quantity of tags and a request cost for adding tags. The requests associated with adding and updating Object Tags are priced the same as existing request prices. See the Amazon S3 pricing page for more information.
1329,How do I get started with Storage Class Analysis?,"You can use the AWS Management Console or the S3 PUT Bucket Analytics API to configure a Storage Class Analysis policy to identify infrequently accessed storage that can be transitioned to the S3 Standard-IA or S3 One Zone-IA storage class or archived to the S3 Glacier storage classes. You can navigate to the Management tab in the S3 console to manage Storage Class Analysis, S3 Inventory, and S3 CloudWatch metrics."
1330,How do I get started with S3 Inventory?,"You can use the AWS Management Console or the PUT Bucket Inventory Configuration API to configure a daily or weekly inventory report for all the objects within your S3 bucket or a subset of the objects under a shared prefix. As part of the configuration, you can specify a destination S3 bucket for your S3 Inventory report, the output file format (CSV, ORC, or Parquet), and specific object metadata necessary for your business application, such as object name, size, last modified date, storage class, version ID, delete marker, non-current version flag, multipart upload flag, replication status, or encryption status. You can use S3 Inventory as a direct input into your application workflows or Big Data jobs. You can also query S3 Inventory using Standard SQL language with Amazon Athena, Amazon Redshift Spectrum, and other tools such as Presto, Hive, and Spark."
1331,How am I charged for using S3 Inventory?,"See the Amazon S3 pricing page for S3 Inventory pricing. Once you configure encryption using SSE-KMS, you will incur KMS charges for encryption, refer to the KMS pricing page for detail."
1332,What is S3 Batch Operations?,"S3 Batch Operations is a feature that you can use to automate the execution of a single operation (like copying an object, or executing an AWS Lambda function) across many objects. With S3 Batch Operations, you can, with a few clicks in the S3 console or a single API request, make a change to billions of objects without having to write custom application code or run compute clusters for storage management applications. Not only does S3 Batch Operations administer your storage operation across many objects, S3 Batch Operations manages retries, displays progress, delivers notifications, provides a completion report, and sends events to AWS CloudTrail for all operations performed on your target objects. S3 Batch Operations can be used from the S3 console, or through the AWS CLI and SDK."
1333,How do I get started with S3 Batch Operations?,"You can get started with S3 Batch Operations by going into the Amazon S3 console or using the AWS CLI or SDK to create your first S3 Batch Operations job. A S3 Batch Operations job consists of the list of objects to act upon and the type of operation to be performed (see the full list of available operations). Start by selecting an S3 Inventory report or providing your own custom list of objects for S3 Batch Operations to act upon. An S3 Inventory report is a file listing all objects stored in an S3 bucket or prefix. Next, you choose from a set of S3 operations supported by S3 Batch Operations, such as replacing tag sets, changing ACLs, copying storage from one bucket to another, or initiating a restore from S3 Glacier Flexible Retrieval to S3 Standard storage class. You can then customize your S3 Batch Operations jobs with specific parameters such as tag values, ACL grantees, and restoration duration. To further customize your storage actions, you can write your own Lambda function and invoke that code through S3 Batch Operations."
1334,What AWS electronic storage services have been assessed based on financial services regulations?,"For customers in the financial services industry, S3 Object Lock provides added support for broker-dealers who must retain records in a non-erasable and non-rewritable format to satisfy regulatory requirements of SEC Rule 17a-4(f), FINRA Rule 4511, or CFTC Regulation 1.31. You can easily designate the records retention time frame to retain regulatory archives in the original form for the required duration, and also place legal holds to retain data indefinitely until the hold is removed."
1335,What AWS documentation supports the SEC 17a-4(f)(2)(i) and CFTC 1.31(c) requirement for notifying my regulator?,"Provide notification to your regulator or Designated Examining Authority (DEA) of your choice to use Amazon S3 for electronic storage along with a copy of the Cohasset Assessment. For the purposes of these requirements, AWS is not a designated third party (D3P). Be sure to select a D3P and include this information in your notification to your DEA."
1336,How do I get started with S3 CloudWatch Metrics?,"You can use the AWS Management Console to enable the generation of one-minute CloudWatch request metrics for your S3 bucket or configure filters for the metrics using a prefix or object tag, or access point. Alternatively, you can call the S3 PUT Bucket Metrics API to enable and configure publication of S3 storage metrics. CloudWatch Request Metrics will be available in CloudWatch within 15 minutes after they are enabled. CloudWatch Storage Metrics are enabled by default for all buckets, and reported once per day. Learn more about CloudWatch metrics for Amazon S3."
1337,What alarms can I set on my storage metrics?,"You can use CloudWatch to set thresholds on any of the storage metrics counts, timers, or rates and trigger an action when the threshold is breached. For example, you can set a threshold on the percentage of 4xx Error Responses and when at least three data points are above the threshold trigger a CloudWatch alarm to alert a DevOps engineer."
1338,How am I charged for using  S3 CloudWatch Metrics?,CloudWatch storage metrics are provided free. Cloudwatch request metrics are priced as custom metrics for Amazon CloudWatch. See the Amazon CloudWatch pricing page for general information about S3 CloudWatch metrics pricing.
1339,What is S3 Lifecycle management?,"S3 Lifecycle management provides the ability to define the lifecycle of your object with a predefined policy and reduce your cost of storage. You can set a lifecycle transition policy to automatically migrate objects stored in the S3 Standard storage class to the S3 Standard-IA, S3 One Zone-IA, and/or S3 Glacier storage classes based on the age of the data. You can also set lifecycle expiration policies to automatically remove objects based on the age of the object. You can set a policy for multipart upload expiration, which expires incomplete multipart uploads based on the age of the upload."
1340,How do I set up an S3 Lifecycle management policy?,"You can set up and manage Lifecycle policies in the AWS Management Console, S3 REST API, AWS SDKs, or AWS Command Line Interface (CLI). You can specify the policy at the prefix or at the bucket level."
1341,How can I use Amazon S3 Lifecycle management to help lower my Amazon S3 storage costs?,"With Amazon S3 Lifecycle policies, you can configure your objects to be migrated from the S3 Standard storage class to S3 Standard-IA or S3 One Zone-IA and/or archived to S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.   You can also specify an S3 Lifecycle policy to delete objects after a specific period of time. You can use this policy-driven automation to quickly and easily reduce storage costs as well as save time. In each rule you can specify a prefix, a time period, a transition to S3 Standard-IA, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, and/or an expiration. For example, you could create a rule that archives into S3 Glacier Flexible Retrieval all objects with the common prefix logs/ 30 days from creation and expires these objects after 365 days from creation.   You can also create a separate rule that only expires all objects with the prefix backups/ 90 days from creation. S3 Lifecycle policies apply to both existing and new S3 objects, helping you optimize storage and maximize cost savings for all current data and any new data placed in S3 without time-consuming manual data review and migration.   Within a lifecycle rule, the prefix field identifies the objects subject to the rule. To apply the rule to an individual object, specify the key name. To apply the rule to a set of objects, specify their common prefix (e.g. logs/). You can specify a transition action to have your objects archived and an expiration action to have your objects removed. For time period, provide the creation date (e.g. January 31, 2015) or the number of days from creation date (e.g. 30 days) after which you want your objects to be archived or removed. You may create multiple rules for different prefixes."
1342,How much does it cost to use S3 Lifecycle management?,There is no additional cost to set up and apply Lifecycle policies. A transition request is charged per object when an object becomes eligible for transition according to the Lifecycle rule. Refer to the Amazon S3 pricing page for pricing information.
1343,Why would I use an S3 Lifecycle policy to expire incomplete multipart uploads?,"The S3 Lifecycle policy that expires incomplete multipart uploads allows you to save on costs by limiting the time non-completed multipart uploads are stored. For example, if your application uploads several multipart object parts, but never commits them, you will still be charged for that storage. This policy can lower your S3 storage bill by automatically removing incomplete multipart uploads and the associated storage after a predefined number of days."
1344,Can I set up Amazon S3 Event Notifications to send notifications when S3 Lifecycle transitions or expires objects?,"Yes, you can set up Amazon S3 Event Notifications to notify you when S3 Lifecycle transitions or expires objects. For example, you can send S3 Event Notifications to an Amazon SNS topic, Amazon SQS queue, or AWS Lambda function when S3 Lifecycle moves objects to a different S3 storage class or expires objects."
1345,What features are available to analyze my storage usage on Amazon S3?,"S3 Storage Lens delivers organization-wide visibility into object storage usage, activity trends, and makes actionable recommendations to optimize costs and apply data protection best practices. S3 Storage Class Analysis enables you to monitor access pattern across objects to help you decide when to transition data to the right storage class to optimize costs. You can then use this information to configure an S3 Lifecycle policy that makes the data transfer. Amazon S3 Inventory provides a report of your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or prefix. This report can be used to help meet business, compliance, and regulatory needs by verifying the encryption, and replication status of your objects."
1346,What is Amazon S3 Storage Lens?,"Amazon S3 Storage Lens provides organization-wide visibility into object storage usage and activity trends, as well as actionable recommendations to optimize costs and apply data protection best practices. Storage Lens offers an interactive dashboard containing a single view of your object storage usage and activity across tens or hundreds of accounts in your organization, with drill-downs to generate insights at multiple aggregation levels. This includes metrics like bytes, object counts, and requests, as well as metrics detailing S3 feature utilization, such as encrypted object counts and S3 Lifecycle rule counts. S3 Storage Lens also delivers contextual recommendations to find ways for you to reduce storage costs and apply best practices on data protection across tens or hundreds of accounts and buckets. S3 Storage Lens free metrics are enabled by default for all Amazon S3 users. If you want to get more out of S3 Storage Lens, you can activate advanced metrics and recommendations. Learn more by visiting the S3 Storage Lens user guide."
1347,How does S3 Storage Lens work?,"S3 Storage Lens aggregates your storage usage and activity metrics on a daily basis to be visualized in the S3 Storage Lens interactive dashboard, or available as a metrics export in CSV or Parquet file format. A default dashboard is created for you automatically at the account level, and you have the option to create additional custom dashboards. S3 Storage Lens dashboards can be scoped to your AWS organization or specific accounts, Regions, buckets, or even prefix level (available with S3 Storage Lens advanced metrics). You can also use S3 Storage Lens groups to aggregate metrics using custom filters based on object metadata like object tag, size, and age. While configuring your dashboard you can use the default metrics selection, or upgrade to receive 35 additional metrics and prefix-level aggregations for an additional cost. Also, S3 Storage Lens provides recommendations contextually with storage metrics in the dashboard, so you can take action to optimize your storage based on the metrics."
1348,What are the key questions that can be answered using S3 Storage Lens metrics?,"The S3 Storage Lens dashboard is organized around four main types of questions that can be answered about your storage. With the Summary filter, top-level questions related to overall storage usage and activity trends can be explored. For example, How rapidly is my overall byte count and request count increasing over time? With the Cost Optimization filter, you can explore questions related to storage cost reduction, for example, Is it possible for me to save money by retaining fewer non-current versions? With the Data Protection and Access Management filters you can answer questions about securing your data, for example, Is my storage protected from accidental or intentional deletion? Finally, with the Performance and Events filters you can explore ways to improve performance of workflows. Each of these questions represent a first layer of inquiry that would likely lead to drill-down analysis."
1349,What metrics are available in S3 Storage Lens?,"S3 Storage Lens contains more than 60 metrics, grouped into free metrics and advanced metrics (available for an additional cost). Within free metrics, you receive metrics to analyze usage (based on a daily snapshot of your objects), which are organized into the categories of cost optimization, data protection, access management, performance, and events. Within advanced metrics, you receive metrics related to activity (such as request counts), deeper cost optimization (such as S3 Lifecycle rule counts), additional data protection (such as S3 Replication rule counts), and detailed status codes (such as 403 authorization errors). In addition, derived metrics are also provided by combining any base metrics. For example, Retrieval Rate"" is a metric calculated by dividing the ""Bytes Downloaded Count"" by the ""Total Storage. To view the complete list of metrics, visit the S3 Storage Lens documentation."
1350,What are my dashboard configuration options?,"A default dashboard is configured automatically provided for your entire account, and you have the option to create additional custom dashboards that can be scoped to your AWS organization, specific regions, or buckets within an account. You can set up multiple custom dashboards, which can be useful if you require some logical separation in your storage analysis, such as segmenting on buckets to represent various internal teams. By default, your dashboard will receive the S3 Storage Lens free metrics, but you have the option to upgrade to receive S3 Storage Lens advanced metrics and recommendations (for an additional cost). S3 Storage Lens advanced metrics have 7 distinct options: Activity metrics, Advanced Cost Optimization metrics, Advanced Data Protection metrics, Detailed Status Code metrics, Prefix aggregation, CloudWatch publishing, and Storage Lens groups aggregation. Additionally, for each dashboard you can enable metrics export, with additional options to specify destination bucket and encryption type."
1351,How much historical data is available in S3 Storage Lens?,"For metrics displayed in the interactive dashboard, Storage Lens free metrics retains 14 days of historical data, and Storage Lens advanced metrics (for an additional cost) retains 15 months of historical data. For the optional metrics export, you can configure any retention period you wish, and standard S3 storage charges will apply."
1352,How will I be charged for S3 Storage Lens?,"S3 Storage Lens is available in two tiers of metrics. The free metrics are enabled by default and available at no additional charge to all S3 customers. The S3 Storage Lens advanced metrics and recommendations pricing details are available on the S3 pricing page. With S3 Storage Lens free metrics you receive 28 metrics at the bucket level, and can access 14 days of historical data in the dashboard. With S3 Storage Lens advanced metrics and recommendations you receive 35 additional metrics, prefix-level aggregation, CloudWatch metrics support, custom object metadata filtering with S3 Storage Lens groups, and can access 15 months of historical data in the dashboard."
1353,What is the difference between S3 Storage Lens and S3 Inventory?,"S3 Inventory provides a list of your objects and their corresponding metadata for an S3 bucket or a shared prefix, which can be used to perform object-level analysis of your storage. S3 Storage Lens provides metrics that can be aggregated by organization, account, region, storage class, bucket, prefix, and S3 Storage Lens group levels, which improve organization-wide visibility of your storage."
1354,What is the difference between S3 Storage Lens and S3 Storage Class Analysis (SCA)?,"S3 Storage Class Analysis provides recommendations for an optimal storage class by creating object age groups based on object-level access pattern within an individual bucket/prefix/tag for the previous 30-90 days. S3 Storage Lens provides daily organization level recommendations on ways to improve cost efficiency and apply data protection best practices, with additional granular recommendations by account, region, storage class, bucket, S3 Storage Lens group, or prefix (available with S3 Storage Lens advanced metrics). You can also use custom filters with S3 Storage Lens groups to visualize your storage based on object age and inform your storage archival strategy."
1355,What is Storage Class Analysis?,"With Storage Class Analysis, you can analyze storage access pattern to determine the optimal storage class for your storage. This S3 feature automatically identifies infrequent access pattern to help you transition storage to S3 Standard-IA. You can configure a Storage Class Analysis policy to monitor an entire bucket, prefix, or object tag. Once an infrequent access pattern is observed, you can easily create a new S3 Lifecycle age policy based on the results. Storage Class Analysis also provides daily visualizations of your storage usage on the AWS Management Console and you can also enable an export report to an S3 bucket to analyze using business intelligence tools of your choice such as Amazon QuickSight."
1356,How often is the Storage Class Analysis updated?,"Storage Class Analysis is updated on a daily basis in the S3 Management Console, but initial recommendations for storage class transitions are provided after 30 days."
1357,"What is ""Query in Place"" functionality?","Amazon S3 allows customers to run sophisticated queries against data stored without the need to move data into a separate analytics platform. The ability to query this data in place on Amazon S3 can significantly increase performance and reduce cost for analytics solutions leveraging S3 as a data lake. S3 offers multiple query in place options, including S3 Select, Amazon Athena, and Amazon Redshift Spectrum, allowing you to choose one that best fits your use case. You can even use Amazon S3 Select with AWS Lambda to build serverless apps that can take advantage of the in-place processing capabilities provided by S3 Select."
1358,What is S3 Select?,"S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster."
1359,What is Amazon Athena?,"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless, so there is no infrastructure to set up or manage, and you can start analyzing data immediately. You don't even need to load your data into Athena; it works directly with data stored in any S3 storage class. To get started, just log into the Athena Management Console, define your schema, and start querying. Amazon Athena uses Presto with full standard SQL support and works with a variety of standard data formats, including CSV, JSON, ORC, Apache Parquet and Avro. While Athena is ideal for quick, ad-hoc querying and integrates with Amazon QuickSight for easy visualization, it can also handle complex analysis, including large joins, window functions, and arrays."
1360,What is Amazon Redshift Spectrum?,"Amazon Redshift Spectrum is a feature of Amazon Redshift that lets you run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required. When you issue a query, it goes to the Amazon Redshift SQL endpoint, which generates and optimizes a query plan. Amazon Redshift determines what data is local and what is in Amazon S3, generates a plan to minimize the amount of Amazon S3 data that needs to be read, and requests Redshift Spectrum workers out of a shared resource pool to read and process data from Amazon S3."
1361,What is Amazon S3 Replication?,"Amazon S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. You can replicate new objects written to the bucket to one or more destination buckets between different AWS Regions (S3 Cross-Region Replication), or within the same AWS Region (S3 Same-Region Replication). You can also replicate existing bucket contents (S3 Batch Replication), including existing objects, objects that previously failed to replicate, and objects replicated from another source. Learn more by visiting the S3 Replication user guide."
1362,What is Amazon S3 Cross-Region Replication (CRR)?,"CRR is an Amazon S3 feature that automatically replicates data between buckets across different AWS Regions. With CRR, you can set up replication at a bucket level, a shared prefix level, or an object level using S3 object tags. You can use CRR to provide lower-latency data access in different geographic regions. CRR can also help if you have a compliance requirement to store copies of data hundreds of miles apart. You can use CRR to change account ownership for the replicated objects to protect data from accidental deletion. To learn more visit the S3 CRR user guide."
1363,What is Amazon S3 Same-Region Replication (SRR)?,"SRR is an Amazon S3 feature that automatically replicates data between buckets within the same AWS Region. With SRR, you can set up replication at a bucket level, a shared prefix level, or an object level using S3 object tags. You can use SRR to create one or more copies of your data in the same AWS Region. SRR helps you address data sovereignty and compliance requirements by keeping a copy of your data in a separate AWS account in the same region as the original. You can use SRR to change account ownership for the replicated objects to protect data from accidental deletion. You can also use SRR to easily aggregate logs from different S3 buckets for in-region processing, or to configure live replication between test and development environments. To learn more visit the S3 SRR user guide."
1364,How do I enable Amazon S3 Replication (Cross-Region Replication and Same-Region Replication)?,"Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS Region for replication."
1365,How do I use S3 Batch Replication?,"You would first need to enable S3 Replication at the bucket level. See the previous question for how you can do so. You may then initiate an S3 Batch Replication job in the S3 console after creating a new replication configuration, changing a replication destination in a replication rule from the replication configuration page, or from the S3 Batch Operations Create Job page. Alternatively, you can initiate an S3 Batch Replication jobs via the AWS CLI or SDKs. To learn more, visit S3 Replication in the Amazon S3 documentation."
1366,Can I use S3 Replication with S3 Lifecycle rules?,"With S3 Replication, you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both."
1367,Can I use replication across AWS accounts to protect against malicious or accidental deletion?,"Yes, for CRR and SRR, you can set up replication across AWS accounts to store your replicated data in a different account in the target region. You can use Ownership Overwrite in your replication configuration to maintain a distinct ownership stack between source and destination, and grant destination account ownership to the replicated storage."
1368,Will my object tags be replicated if I use Cross-Region Replication?,"Object tags can be replicated across AWS Regions using Cross-Region Replication. For customers with Cross-Region Replication already enabled, new permissions are required in order for tags to replicate. For more information about setting up Cross-Region Replication, visit How to Set Up Cross-Region Replication in the Amazon S3 documentation."
1369,Can I replicate delete markers from one bucket to another?,"Yes, you can replicate delete markers from source to destination if you have delete marker replication enabled in your replication configuration. When you replicate delete markers, Amazon S3 will behave as if the object was deleted in both buckets. You can enable delete marker replication for a new or existing replication rule. You can apply delete marker replication to the entire bucket or to Amazon S3 objects that have a specific prefix, with prefix based replication rules. Amazon S3 Replication does not support delete marker replication for object tag based replication rules. To learn more about enabling delete marker replication see Replicating delete markers from one bucket to another."
1370,Can I replicate data from other AWS Regions to China? Can a customer replicate from one China Region bucket outside of China Regions?,"No, Amazon S3 Replication is not available between AWS China Regions and AWS Regions outside of China. You are only able to replicate within the China regions."
1371,Can I replicate existing objects?,"Yes. You can use S3 Batch Replication to replicate existing objects between buckets. To learn more, visit the S3 User Guide."
1372,Can I re-try replication if object fail to replicate initially?,"Yes. You can use S3 Batch Replication to re-replicate objects that fail to replicate initially. To learn more, visit the S3 User Guide."
1373,What encryption types does S3 Replication support?,"S3 Replication supports all encryption types that S3 offers. S3 offers both server-side encryption and client-side encryption the former requests S3 to encrypt the objects for you, and the latter is for you to encrypt data on the client-side before uploading it to S3. For server-side encryption, S3 offers server-side encryption with Amazon S3-managed keys (SSE-S3), server-side encryption with KMS keys stored in AWS Key Management Service (SSE-KMS), and server-side encryption with customer-provided keys (SSE-C). For further details on these encryption types and how they work, visit the S3 documentation on using encryption."
1374,What is the pricing for cross account data replication?,"With S3 Replication, you can configure cross account replication where the source and destination buckets are owned by different AWS accounts. Excluding S3 storage and applicable retrieval charges, customers pay for replication PUT requests and inter-region Data Transfer OUT from S3 to your destination region when using S3 Replication. If you have S3 Replication Time Control (S3 RTC) enabled on your replication rules, you will see a different Data Transfer OUT and replication PUT request charges specific to S3 RTC. For cross account replication, the source account pays for all data transfer (S3 RTC and S3 CRR) and the destination account pays for the replication PUT requests. Data transfer charges only apply for S3 Cross Region Replication (S3 CRR) and S3 Replication Time Control (S3 RTC), there are no data transfer charges for S3 Same Region Replication (S3 SRR)."
1375,What is Amazon S3 Replication Time Control?,"Amazon S3 Replication Time Control provides predictable replication performance and helps you meet compliance or business requirements. S3 Replication Time Control is designed to replicate most objects in seconds, and 99.99% of objects within 15 minutes. S3 Replication Time Control is backed by a Service Level Agreement (SLA) commitment that 99.9% of objects will be replicated in 15 minutes for each replication region pair during any billing month. Replication Time works with all S3 Replication features. To learn more, visit the replication documentation."
1376,How do I enable Amazon S3 Replication Time Control?,"Amazon S3 Replication Time Control is enabled as an option for each replication rule. You can create a new S3 Replication policy with S3 Replication Time Control, or enable the feature on an existing policy. You can use either the S3 console, API, AWS CLI, AWS SDKs, or AWS CloudFormation to configure replication. To learn more, please visit overview of setting up Replication in the Amazon S3 developer guide."
1377,Can I use S3 Replication Time Control to replicate data within and between China Regions?,"Yes, you can enable Amazon S3 Replication Time Control to replicate data within and between the AWS China (Ningxia) and China (Beijing) Regions."
1378,What are Amazon S3 Replication metrics and events?,"Amazon S3 Replication provides four detailed metrics in the Amazon S3 console and in Amazon CloudWatch: operations pending, bytes pending, replication latency, and operations failed replication. You can use these metrics to monitor the total number of operations and size of objects that are pending to replicate, the replication latency between source and destination buckets, and the number of operations that did not replicate successfully for each replication rule. Additionally, you can set up Amazon S3 Event Notifications of s3:Replication type to get more information about objects that failed to replicate and the reason behind the failures. We recommend using Amazon S3 replication failure reasons to diagnose the errors quickly and fix them before re-replicating the failed objects with S3 Batch Replication. Finally, if you have S3 Replication Time Control (S3 RTC) enabled you will receive an S3 Event Notification when an object takes more than 15 minutes to replicate, and another when that object replicates successfully to the destination."
1379,How do I enable Amazon S3 Replication metrics and events?,"Amazon S3 Replication metrics and events can be enabled for each new or existing replication rules, and are enabled by default for S3 Replication Time Control enabled rules. You can access S3 Replication metrics through the Amazon S3 console and Amazon CloudWatch. Like other Amazon S3 events, S3 Replication events are available through Amazon Simple Queue Service (Amazon SQS), Amazon Simple Notification Service (Amazon SNS), or AWS Lambda. To learn more, please visit Monitoring progress with replication metrics and Amazon S3 Event Notifications in the Amazon S3 developer guide."
1380,What information does the operations failed replication metric show?,"The operations failed replication metric will show the total number of operations failing replication per minute for a specific replication rule. The metric will refresh every minute to emit +1 for each failed operation, 0 for successful operations, and nothing when there are no replication operations carried out for the minute. This metric is emitted every time an operation does not replicate successfully."
1381,Can I use Amazon S3 Replication metrics and events to track S3 Batch Replication?,"You cannot use metrics like bytes pending, operations pending, and replication latency to track S3 Batch Replication progress. However, you can use the operations failed replication metric to monitor existing objects that do not replicate successfully with S3 Batch Replication. Additionally, you can also use S3 Batch Operations completion reports to keep track of objects replicating with S3 Batch Replication."
1382,Where are Amazon S3 Replication metrics published?,"The bytes pending, operations pending, and replication latency metrics are published in the source AWS account and destination AWS Region. However, the operations failed replication metric is published in the source AWS account and source AWS Region instead of the destination AWS Region. There are two primary reasons for this. First, if the operations failed replication metric is published in the destination Region the customer will not see the metric when the destination bucket is erroneously configured. For example, if the customer has mis-typed the destination bucket name in the replication configuration and replication is unsuccessful because the destination bucket is not found, the customer will not be able to see any value for this metric because the destination Region will be unknown when the destination bucket is not found. Second, if the customer is replicating to an opt-in destination Region like Hong Kong or Bahrain, in the event of replication failures the customer will not see any metric if the source account has not opted-in for the destination Region."
1383,What is the Amazon S3 Replication Time Control Service Level Agreement (SLA)?,"Amazon S3 Replication Time Control is designed to replicate 99.99% of your objects within 15 minutes, and is backed by a service level agreement. If fewer than 99.9% of your objects are replicated in 15 minutes for each replication region pair during a monthly billing cycle, the S3 RTC SLA provides a service credit on any object that takes longer than 15 minutes to replicate. The service credit covers a percentage of all replication-related charges associated with the objects that did not meet the SLA, including the RTC charge, replication bandwidth and request charges, and the cost associated with storing your replica in the destination region in the monthly billing cycle affected. To learn more, read the S3 Replication Time Control SLA."
1384,What is the pricing for S3 Replication and S3 Replication Time Control?,"For S3 Replication (Cross-Region Replication and Same Region Replication), you pay the S3 charges for storage in the selected destination S3 storage classes, the storage charges for the primary copy, replication PUT requests, and applicable infrequent access storage retrieval charges. For CRR, you also pay for inter-region Data Transfer OUT From S3 to your destination region. S3 Replication Metrics are billed at the same rate as Amazon CloudWatch custom metrics. Additionally, when you use S3 Replication Time Control, you also pay a Replication Time Control Data Transfer charge. For more information, visit the Amazon S3 pricing page."
1385,How am I charged for S3 Replication metrics on Amazon CloudWatch?,"All S3 Replication metrics, including bytes pending, operations pending, replication latency, and operations failed replication, are billed at the same rate as Amazon CloudWatch Custom metrics: $0.30 per metric per month for the first 10K metrics, $0.10 per metric per month for the next 240K metrics, $0.05 per metric per month for the next 750K metrics, and $0.02 per metric per month for over 1M metrics."
1386,What are S3 Multi-Region Access Points?,"Amazon S3 Multi-Region Access Points accelerate performance by up to 60% when accessing data sets that are replicated across multiple AWS Regions. Based on AWS Global Accelerator, S3 Multi-Region Access Points consider factors like network congestion and the location of the requesting application to dynamically route your requests over the AWS network to the lowest latency copy of your data. This automatic routing allows you to take advantage of the global infrastructure of AWS while maintaining a simple application architecture."
1387,Why should I use S3 Multi-Region Access Points?,"S3 Multi-Region Access Points accelerate and simplify storage for your multi-region applications. By dynamically routing S3 requests made to a replicated data set, S3 Multi-Region Access Points reduce request latency, so that applications run up to 60% faster. S3 Multi-Region Access Points can also help you build resilient, multi-region and multi-account applications that are more protected against accidental or unauthorized data deletion. With S3 Multi-Region Access Points, you are able to take advantage of the global infrastructure of AWS while maintaining a simple region-agnostic architecture for your applications."
1388,How do S3 Multi-Region Access Points work?,"Multi-Region Access Points dynamically route client requests to one or more underlying S3 buckets. You can configure your Multi-Region Access Point to route across one bucket per AWS Region, in up to 20 AWS Regions. When you create a Multi-Region Access Point, S3 automatically generates a DNS-compatible name. This name is used as a global endpoint that can be used by your clients. When your clients make requests to this endpoint, S3 will dynamically route those requests to one of the underlying buckets that are specified in the configuration of your Multi-Region Access Point. Internet-based requests are onboarded to the AWS global network to avoid congested network segments on the internet, which reduces network latency and jitter while improving performance. Based on AWS Global Accelerator, applications that access S3 over the internet can see performance further improved up to 60% by S3 Multi-Region Access Points."
1389,How do S3 Multi-Region Access Points failover controls work?,"By default, S3 Multi-Region Access Points route requests to the underlying bucket closest to the client, based on network latency in an active-active configuration. For example, you can configure a Multi-Region Access Point with underlying buckets in US East (N. Virginia) and in Asia Pacific (Mumbai). With this configuration, your clients in North America route to US East (N. Virginia), while your clients in Asia route to Asia Pacific (Mumbai). This lowers latency for your requests made to S3, improving the performance of your application. If you prefer an active-passive configuration, all S3 data request traffic can be routed through the S3 Multi-Region Access Point to US East (N. Virginia) as the active Region and no traffic will be routed to Asia Pacific (Mumbai). If there is a planned or unplanned need to failover all of the S3 data request traffic to Asia Pacific (Mumbai), you can initiate a failover to switch to Asia Pacific (Mumbai) as the new active Region within minutes. Any existing uploads or downloads in progress in US East (N. Virginia) continue to completion and all new S3 data request traffic through the S3 Multi-Region Access Point is routed to Asia Pacific (Mumbai)."
1390,How is S3 Transfer Acceleration different than S3 Multi-Region Access Points?,"S3 Multi-Region Access Points and S3 Transfer Acceleration provide similar performance benefits. You can use S3 Transfer Acceleration to speed up content transfers to and from Amazon S3 using the AWS global network. S3 Transfer Accelerator can help accelerate long-distance transfers of larger objects to and from a single Amazon S3 bucket. With S3 Multi-Region Access Points, you can perform similar accelerated transfers using the AWS global network, but across many S3 buckets in multiple AWS Regions for internet-based, VPC-based, and on-premises requests to and from S3. When you combine S3 Multi-Region Access Points with S3 Cross Replication, you provide the capability for S3 Multi-Region Access Points to dynamically route your requests to the lowest latency copy of your data for applications from clients in multiple locations."
1391,How do I get started with S3 Multi-Region Access Points and failover controls?,"The S3 console provides a simple guided workflow to quickly set up everything you need to run multi-Region storage on S3 in just three simple steps. First, create an Amazon S3 Multi-Region Access Point endpoint and specify the AWS Regions you want to replicate and failover between. You can add buckets in multiple AWS accounts to a new S3 Multi-Region Access Point by entering the account IDs that own the buckets at the time of creation. Second, for each AWS Region and S3 bucket behind your S3 Multi-Region Access Point endpoint, specify whether their routing status is active or passive, where active AWS Regions accept S3 data request traffic, and passive Regions are not be routed to until you initiate a failover. Third, configure your S3 Cross-Region Replication rules to synchronize your data in S3 between the Regions and/or accounts. You can then initiate a failover at any time between the AWS Regions within minutes to shift your S3 data requests and monitor the shift of your S3 traffic to your new active AWS Region in Amazon CloudWatch. Alternatively, you can use AWS CloudFormation to automate your multi-Region storage configuration. All of the building blocks required to set up multi-Region storage on S3, including S3 Multi-Region Access Points, are supported by CloudFormation, allowing you to automate a repeatable setup process outside of the S3 console."
1392,What is S3 Object Lambda?,"S3 Object Lambda allows you to add your own code to S3 GET, LIST, and HEAD requests to modify and process data as it is returned to an application. You can use custom code to modify the data returned by S3 GET requests to filter rows, dynamically resize images, redact confidential data, and much more. You can also use S3 Object Lambda to modify the output of S3 LIST requests to create a custom view of objects in a bucket and S3 HEAD requests to modify object metadata like object name and size. S3 Object Lambda helps you to easily meet the unique data format requirements of any application without having to build and operate additional infrastructure, such as a proxy layer, or having to create and maintain multiple derivative copies of your data. S3 Object Lambda uses AWS Lambda functions to automatically process the output of a standard S3 GET, LIST, or HEAD request. AWS Lambda is a serverless compute service that runs customer-defined code without requiring management of underlying compute resources."
1393,Why should I use S3 Object Lambda?,"You should use S3 Object Lambda if you want to process data inline with an S3 GET, LIST, or HEAD request. You can use S3 Object Lambda to share a single copy of your data across many applications, avoiding the need to build and operate custom processing infrastructure or to store derivative copies of your data. For example, by using S3 Object Lambda to process S3 GET requests, you can mask sensitive data for compliance purposes, restructure raw data for the purpose of making it compatible with machine learning applications, filter data to restrict access to specific content within an S3 object, or to address a wide range of additional use cases. You can use S3 Object Lambda to enrich your object lists by querying an external index that contains additional object metadata, filter and mask your object lists to only include objects with a specific object tag, or add a file extension to all the object names in your object lists. For example, if you have an S3 bucket with multiple discrete data sets, you can use S3 Object Lambda to filter an S3 LIST response depending on the requester."
1394,How does S3 Object Lambda work?,"S3 Object Lambda uses Lambda functions specified by you to process the output of GET, LIST, and HEAD requests. Once you have defined a Lambda function to process requested data, you can attach that function to an S3 Object Lambda Access Point. GET, LIST, and HEAD requests made through an S3 Object Lambda Access Point will now invoke the specified Lambda function. Lambda will then fetch the S3 object requested by the client and process that object. Once processing has completed, Lambda will stream the processed object back to the calling client. Read the S3 Object Lambda user guide to learn more."
1395,How do I get started with S3 Object Lambda?,"S3 Object Lambda can be set up in multiple ways. You can set up S3 Object Lambda in the S3 console by navigating to the Object Lambda Access Point tab. Next, create an S3 Object Lambda Access Point, the Lambda function that you would like S3 to execute against your GET, LIST, and HEAD requests, and a supporting S3 Access Point. Grant permissions to all resources to interact with Object Lambda. Lastly, update your SDK and application to use the new S3 Object Lambda Access Point to retrieve data from S3 using the language SDK of your choice. You can use an S3 Object Lambda Access Point alias when making requests. Aliases for S3 Object Lambda Access Points are automatically generated and are interchangeable with S3 bucket names for data accessed through S3 Object Lambda. For existing S3 Object Lambda Access Points, aliases are automatically assigned and ready for use. There are example Lambda function implementations in the AWS documentation to help you get started."
1396,What kinds of operations can I perform with S3 Object Lambda?,"Any operation supported in a Lambda function is supported with S3 Object Lambda. This gives you a wide range of available options for processing your requests. You supply your own Lambda function to run custom computations against GET, LIST, and HEAD requests, giving you the flexibility to process data according to the needs of your application. Lambda processing time is limited to a maximum of 60 seconds. For more details, see the S3 Object Lambda documentation."
1397,Which S3 request types does S3 Object Lambda support?,"S3 Object Lambda supports GET, LIST and HEAD requests. Any other S3 API calls made to an S3 Object Lambda Access Point will return the standard S3 API response. Learn more about S3 Object Lambda in the user guide."
1398,What will happen when a S3 Object Lambda function fails?,"When a S3 Object Lambda function fails, you will receive a request response detailing the failure. Like other invocations of Lambda functions, AWS also automatically monitors functions on your behalf, reporting metrics through Amazon CloudWatch. To help you troubleshoot failures, Lambda logs all requests processed by your function and automatically stores logs generated by your code with Amazon CloudWatch Logs. For more information about accessing CloudWatch logs for AWS Lambda, visit CloudWatch documentation."
1399,Does S3 Object Lambda affect the S3 availability SLA or S3 durability?,"S3 Object Lambda connects Amazon S3, AWS Lambda, and optionally, other AWS services of your choosing to deliver objects relevant to requesting applications. All AWS services used in connection with S3 Object Lambda will continue to be governed by their respective Service Level Agreements (SLA). For example, in the event that any AWS Service does not meet its Service Commitment, you will be eligible to receive a Service Credit as documented in that service's SLA. Creating an S3 Object Lambda Access Point does not impact the durability of your objects. However, S3 Object Lambda invokes your specified AWS Lambda function and you must ensure your specified Lambda function is intended and correct. See the latest Amazon S3 SLA here."
1400,How much does S3 Object Lambda cost?,"When you use S3 Object Lambda, you pay a per GB charge for every gigabyte of data returned to you through S3 Object Lambda. You are also charged for requests based on the request type (GET, LIST, and HEAD requests) and AWS Lambda compute charges for the time your specified function is running to process the requested data. To see pricing details and an example, read the S3 pricing page."
1401,What is Mountpoint for Amazon S3?,"Mountpoint for Amazon S3 is an open source file client that you can use to mount an S3 bucket on your compute instance and access it as a local file system. Mountpoint for Amazon S3 translates local file system operations to REST API calls on objects stored in Amazon S3. With Mountpoint for Amazon S3, you can achieve high single-instance throughput to finish jobs faster. Mountpoint for Amazon S3 is backed by AWS Support. Customers with access to AWS Enterprise Support get 24x7 technical support from Amazon support engineers and architectural guidance delivered in the context of their use cases. Mountpoint for Amazon S3 works with the Linux operating system and AWS compute services such as Amazon Elastic Compute Cloud (EC2). Learn more on the Mountpoint for Amazon S3 page or the user guide."
1402,When should I use Mountpoint for Amazon S3?,"Mountpoint for Amazon S3 is ideal for read-heavy data lake workloads that process petabytes of data using random and sequential read operations on existing files and sequential write operations for creating new files. These workloads write from a single node and do not modify existing data in Amazon S3. Common use cases include petabyte-scale autonomous vehicle simulation, machine learning training, genomics analysis, and image rendering. These workloads scale up and down quickly, and rely on Amazon S3's elasticity to minimize underutilized capacity and avoid the cost of over-provisioning throughput. You can save on compute costs with Mountpoint for Amazon S3 by efficiently utilizing the network bandwidth use of your compute instances, and reliably scale to thousands of compute instances for petabyte-scale data lake workloads."
1403,What file system operations does Mountpoint for Amazon S3 support?,"Mountpoint for Amazon S3 supports basic file system operations such as reading files up to 5TB in size, writing new files, listing existing files, and creating and listing directories. Mountpoint for Amazon S3 does not support modifying existing files or deleting existing directories. With these operations, Mountpoint for Amazon S3 is ideal for applications that read and write data at high throughput in Amazon S3 data lakes. It is not suitable for applications that need collaboration and coordination across multiple compute instances or users. These applications typically need shared file system features like appending to existing files and file locking. You can use Amazon FSx for Lustre for data lake applications that need POSIX semantics and shared file system features."
1404,How do I get started with Mountpoint for Amazon S3?,"You can get started with Mountpoint for Amazon S3 by mounting an S3 bucket at a local directory on your compute instance using the instructions provided in the documentation. Once you mount the S3 bucket at a local directory, your applications can access S3 objects as files available locally on their compute instance. Mountpoint for Amazon S3 supports sequential and random read operations on existing Amazon S3 objects, and supports sequential writes for new objects. You should read the semantics documentation for Mountpoint for Amazon S3 for more details on supported file system operations. You can use Mountpoint for Amazon S3 to access objects in all S3 storage classes, excluding objects in S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, and objects in the Archive Access tier and Deep Archive Access tier in S3 Intelligent-Tiering."
1405,How am I charged for Mountpoint for Amazon S3?,"There is no additional charge for using Mountpoint for Amazon S3. You pay for S3 API requests such as GET, PUT, and LIST requests made by Mountpoint for Amazon S3 when you run file system operations such as file-read, file-write, and directory-listing operations. For S3 request pricing, please visit the pricing page."
1406,What performance can I expect from Mountpoint for Amazon S3?,"Mountpoint for Amazon S3 delivers the same performance as the AWS SDKs. This means data lake applications achieve high single-instance transfer rates, efficiently utilizing the available network bandwidth on their Amazon EC2 instance. To achieve even higher throughput, these applications can aggregate throughput across multiple instances to get multiple Tb/s."
1407,How can I control access to my data when using Mountpoint for Amazon S3?,"When using Mountpoint for Amazon S3, you can control access to your data using Amazon S3's existing access control mechanisms, including bucket policies and AWS Identity and Access Management (IAM) policies. Mountpoint for Amazon S3 translates file system operations like read and write into object API requests made to your S3 bucket. Afterwards, Amazon S3 evaluates all the relevant policies, such as those on the user and bucket, to decide whether to authorize the request. Mountpoint for Amazon S3 does not introduce new access control mechanisms."
1408,"Does Mountpoint for Amazon S3 support POSIX-style metadata, such as user ID, group ID, and permission fields?","Mountpoint for Amazon S3 does not support reading or writing POSIX-style metadata, such as user ID, group ID, and permission fields. You can use Amazon FSx for Lustre with Amazon S3 or AWS DataSync to store POSIX-style metadata for S3 objects."
1409,Does Mountpoint for Amazon S3 support access over AWS PrivateLink?,"Yes, Mountpoint for Amazon S3 supports access over AWS PrivateLink. AWS PrivateLink for S3 provides private connectivity between Amazon S3 and on-premises. You can provision interface VPC endpoints for S3 in your VPC to connect your on-premises applications directly to S3 over AWS Direct Connect or AWS VPN."
1410,Does Mountpoint for Amazon S3 support access over gateway VPC endpoints?,"Yes, Mountpoint for Amazon S3 supports access over gateway VPC endpoints. We recommend that you use AWS PrivateLink-based interface VPC endpoints to access S3 from on-premises or from a VPC in another AWS Region. For resources that access S3 from a VPC in the same AWS Region as your S3 bucket, we recommend using gateway VPC endpoints as they are not billed."
1411,Can I access Amazon S3 from Amazon Elastic Kubernetes Service (Amazon EKS) pods?,"Yes, you can access Amazon S3 from Amazon EKS using the AWS SDK and the AWS CLI. For applications that use a file system interface to read and write data, you can use the Mountpoint for Amazon S3 Container Storage Interface (CSI) driver. With the Mountpoint for Amazon S3 CSI driver you can achieve high levels of aggregate throughput up to terabits per second without changing a single line of application code or your permission model. Like Mountpoint for Amazon S3, the S3 CSI driver supports sequential and random read operations on existing files and sequential write operations for creating new files. For details on supported file system operations, read the Mountpoint for Amazon S3 file system behavior. You can install, configure, and update the Mountpoint for Amazon S3 CSI driver with just a few clicks in the EKS Console, AWS CLI, EKS API, or AWS CloudFormation."
1412,What is Amazon Virtual Private Cloud?,"Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways. You can also create a hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter. You can easily customize the network configuration for your Amazon VPC. For example, you can create a public-facing subnet for your web servers that have access to the Internet, and place your backend systems such as databases or application servers in a private-facing subnet with no Internet access. You can leverage multiple layers of security, including security groups and network access control lists, to help control access to Amazon EC2 instances in each subnet. "
1413,What are the components of Amazon VPC?,"Amazon VPC comprises a variety of objects that will be familiar to customers with existing networks: A Virtual Private Cloud: A logically isolated virtual network in the AWS cloud. You define a VPC's IP address space from ranges you select. Subnet: A segment of a VPC's IP address range where you can place groups of isolated resources. Internet Gateway: The Amazon VPC side of a connection to the public Internet. NAT Gateway: A highly available, managed Network Address Translation (NAT) service for your resources in a private subnet to access the Internet. Virtual private gateway: The Amazon VPC side of a VPN connection. Peering Connection: A peering connection enables you to route traffic via private IP addresses between two peered VPCs. VPC Endpoints: Enables private connectivity to services hosted in AWS, from within your VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies. Egress-only Internet Gateway: A stateful gateway to provide egress only access for IPv6 traffic from the VPC to the Internet. "
1414,Why should I use Amazon VPC?,"Amazon VPC enables you to build a virtual network in the AWS cloud - no VPNs, hardware, or physical datacenters required. You can define your own network space, and control how your network and the Amazon EC2 resources inside your network are exposed to the Internet. You can also leverage the enhanced security options in Amazon VPC to provide more granular access to and from the Amazon EC2 instances in your virtual network. "
1415,How do I get started with Amazon VPC?,"Your AWS resources are automatically provisioned in a ready-to-use default VPC. You can choose to create additional VPCs by going to the Amazon VPC page in the AWS Management Console and selecting Start VPC Wizard. You'll be presented with four basic options for network architectures. After selecting an option, you can modify the size and IP address range of the VPC and its subnets. If you select an option with Hardware VPN Access, you will need to specify the IP address of the VPN hardware on your network. You can modify the VPC to add or remove secondary IP ranges and gateways, or add more subnets to IP ranges. The four options are: Amazon VPC with a single public subnet only Amazon VPC with public and private subnets Amazon VPC with public and private subnets and AWS Site-to-Site VPN access Amazon VPC with a private subnet only and AWS Site-to-Site VPN access "
1416,What are the different types of VPC endpoints available on Amazon VPC?,"VPC endpoints enable you to privately connect your VPC to services hosted on AWS without requiring an Internet gateway, a NAT device, VPN, or firewall proxies. Endpoints are horizontally scalable and highly available virtual devices that allow communication between instances in your VPC and AWS services. Amazon VPC offers two different types of endpoints: gateway type endpoints and interface type endpoints. Gateway type endpoints are available only for AWS services including S3 and DynamoDB. These endpoints will add an entry to your route table you selected and route the traffic to the supported services through Amazon's private network. Interface type endpoints provide private connectivity to services powered by PrivateLink, being AWS services, your own services or SaaS solutions, and supports connectivity over Direct Connect. More AWS and SaaS solutions will be supported by these endpoints in the future. Please refer to VPC Pricing for the price of interface type endpoints. "
1417,How will I be charged and billed for my use of Amazon VPC?,"There are no additional charges for creating and using the VPC itself. Usage charges for other Amazon Web Services, including Amazon EC2, still apply at published rates for those resources, including data transfer charges. If you connect your VPC to your corporate datacenter using the optional hardware VPN connection, pricing is per VPN connection-hour (the amount of time you have a VPN connection in the available state.) Partial hours are billed as full hours. Data transferred over VPN connections will be charged at standard AWS Data Transfer rates. For VPC-VPN pricing information, please visit the pricing section of the Amazon VPC product page. "
1418,"What usage charges will I incur if I use other AWS services, such as Amazon S3, from Amazon EC2 instances in my VPC?","Usage charges for other Amazon Web Services, including Amazon EC2, still apply at published rates for those resources. Data transfer charges are not incurred when accessing Amazon Web Services, such as Amazon S3, via your VPC's Internet gateway. If you access AWS resources via your VPN connection, you will incur Internet data transfer charges. "
1419,What are the connectivity options for my Amazon VPC?,"You may connect your Amazon VPC to: The internet (via an internet gateway) Your corporate data center using an AWS Site-to-Site VPN connection (via the virtual private gateway) Both the internet and your corporate data center (utilizing both an internet gateway and a virtual private gateway) Other AWS services (via internet gateway, NAT, virtual private gateway, or VPC endpoints) Other Amazon VPCs (via VPC peering connections) "
1420,How do I connect my VPC to the Internet?,Amazon VPC supports the creation of an Internet gateway. This gateway enables Amazon EC2 instances in the VPC to directly access the Internet. You can also use an Egress-only internet gateway which is a stateful gateway to provide egress only access for IPv6 traffic from the VPC to the Internet. 
1421,Are there any bandwidth limitations for Internet gateways? Do I need to be concerned about its availability? Can it be a single point of failure?,"No. An Internet gateway is horizontally-scaled, redundant, and highly available. It imposes no bandwidth constraints. "
1422,How do instances in a VPC access the Internet?,"You can use public IP addresses, including Elastic IP addresses (EIPs) and IPv6 Global Unique addresses (GUA), to give instances in the VPC the ability to both directly communicate outbound to the internet and to receive unsolicited inbound traffic from the internet (e.g., web servers). You can also use the solutions in the next question. "
1423,When is an IP address considered a Public IP address?,"Any IP address that is assigned to an instance or a service hosted in a VPC that can be accessed over the internet is considered a public IP address. Only public IPv4 addresses, including Elastic IP addresses (EIPs) and IPv6 GUA can be routable on the internet. To do so, you would need to first connect the VPC to the internet and then update the route table to make them reachable to/from the internet. "
1424,How do instances without public IP addresses access the Internet?,"Instances without public IP addresses can access the Internet in one of two ways: Instances without public IP addresses can route their traffic through a NAT gateway or a NAT instance to access the Internet. These instances use the public IP address of the NAT gateway or NAT instance to traverse the Internet. The NAT gateway or NAT instance allows outbound communication but doesn't allow machines on the Internet to initiate a connection to the privately addressed instances. For VPCs with a hardware VPN connection or Direct Connect connection, instances can route their Internet traffic down the virtual private gateway to your existing datacenter. From there, it can access the Internet via your existing egress points and network security/monitoring devices. "
1425,Can I connect to my VPC using a software VPN?,Yes. You may use a third-party software VPN to create a site to site or remote access VPN connection with your VPC via the Internet gateway. 
1426,"Does traffic go over the internet when two instances communicate using public IP addresses, or when instances communicate with a public AWS service endpoint?","No. When using public IP addresses, all communication between instances and services hosted in AWS use AWS's private network. Packets that originate from the AWS network with a destination on the AWS network stay on the AWS global network, except traffic to or from AWS China Regions. In addition, all data flowing across the AWS global network that interconnects our data centers and Regions is automatically encrypted at the physical layer before it leaves our secured facilities. Additional encryption layers exist as well; for example, all VPC cross-region peering traffic, and customer or service-to-service Transport Layer Security (TLS) connections. "
1427,How does an AWS Site-to-Site VPN connection work with Amazon VPC?,An AWS Site-to-Site VPN connection connects your VPC to your datacenter. Amazon supports Internet Protocol Security (IPSec) VPN connections. Data transferred between your VPC and datacenter routes over an encrypted VPN connection to help maintain the confidentiality and integrity of data in transit. An internet gateway is not required to establish an AWS Site-to-Site VPN connection. 
1428,What IP address ranges can I use within my Amazon VPC?,"You can use any IPv4 address range, including RFC 1918 or publicly routable IP ranges, for the primary CIDR block. For the secondary CIDR blocks, certain restrictions apply. Publicly routable IP blocks are only reachable via the Virtual Private Gateway and cannot be accessed over the Internet through the Internet gateway. AWS does not advertise customer-owned IP address blocks to the Internet. You can allocate  up to 5 Amazon-provided or BYOIP IPv6 GUA CIDR blocks to a VPC by calling the relevant API or via the AWS Management Console. "
1429,How do I assign IP address ranges to Amazon VPCs?,"You assign a single Classless Internet Domain Routing (CIDR) IP address range as the primary CIDR block when you create a VPC and can add up to four (4) secondary CIDR blocks after creation of the VPC. Subnets within a VPC are addressed from these CIDR ranges by you. Please note that while you can create multiple VPCs with overlapping IP address ranges, doing so will prohibit you from connecting these VPCs to a common home network via the hardware VPN connection. . For this reason we recommend using non-overlapping IP address ranges. You can allocate up to 5 Amazon-provided or BYOIP IPv6 CIDR blocks to your VPC. "
1430,What IP address ranges are assigned to a default Amazon VPC?,Default VPCs are assigned a CIDR range of 172.31.0.0/16. Default subnets within a default VPC are assigned /20 netblocks within the VPC CIDR range. 
1431,Can I use my IP addresses in VPC and access them over the Internet?,"Yes, you can bring your public IPv4 addresses and IPv6 GUA addresses into AWS VPC and statically allocate them to subnets and EC2 instances. To access these addresses over the Internet, you will have to advertise them to the Internet from your on-premises network. You will also have to route the traffic over these addresses between your VPC and on-premises network using AWS DX or AWS VPN connection. You can route the traffic from your VPC using the Virtual Private Gateway. Similarly, you can route the traffic from your on-premises network back to your VPC using your routers. "
1432,How large of a VPC can I create?,"Currently, Amazon VPC supports five (5) IP address ranges, one (1) primary and four (4) secondary for IPv4. Each of these ranges can be between /28 (in CIDR notation) and /16 in size. The IP address ranges of your VPC should not overlap with the IP address ranges of your existing network. For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 CIDR blocks associated to it. "
1433,Can I change the size of a VPC?,"Yes. You can expand your existing VPC by adding four (4) secondary IPv4 IP ranges (CIDRs) to your VPC. You can shrink your VPC by deleting the secondary CIDR blocks you have added to your VPC.  Likewise, you can add up to five (5) additionally IPv6 IP ranges (CIDRs) to your VPC.  You can shrink your VPC by deleting these additional ranges. "
1434,How many subnets can I create per VPC?,"Currently you can create 200 subnets per VPC. If you would like to create more, please submit a case at the support center. "
1435,Is there a limit on how large or small a subnet can be?,"The minimum size of a subnet is a /28 (or 14 IP addresses.) for IPv4. Subnets cannot be larger than the VPC in which they are created. For IPv6, the subnet size is fixed to be a /64. Only one IPv6 CIDR block can be allocated to a subnet. "
1436,Can I use all the IP addresses that I assign to a subnet?,No. Amazon reserves the first four (4) IP addresses and the last one (1) IP address of every subnet for IP networking purposes. 
1437,How do I assign private IP addresses to Amazon EC2 instances within a VPC?,"When you launch an Amazon EC2 instance within a subnet that is not IPv6-only, you may optionally specify the primary private IPv4 address for the instance. If you do not specify the primary private IPv4 address, AWS automatically addresses it from the IPv4 address range you assign to that subnet. You can assign secondary private IPv4 addresses when you launch an instance, when you create an Elastic Network Interface, or any time after the instance has been launched or the interface has been created. In case you launch an Amazon EC2 instance within an IPv6-only subnet, AWS automatically addresses it from the Amazon-provided IPv6 GUA CIDR of that subnet. The instance's IPv6 GUA will remain private unless you make them reachable to/from the internet with the right security group, NACL, and route table configuration. "
1438,Can I change the private IP addresses of an Amazon EC2 instance while it is running and/or stopped within a VPC?,"For an instance launched in an IPv4 or dual-stack subnet, the primary private IPv4 address is retained for the instance's or interface's lifetime. Secondary private IPv4 addresses can be assigned, unassigned, or moved between interfaces or instances at any time. For an instance launched in an IPv6-only subnet, the assigned IPv6 GUA which is also the first IP address on the instance's primary network interface can be modified by associating a new IPv6 GUA and removing the existing IPv6 GUA at any time. "
1439,"If an Amazon EC2 instance is stopped within a VPC, can I launch another instance with the same IP address in the same VPC?","No. An IPv4 address assigned to a running instance can only be used again by another instance once that original running instance is in a terminated state. However, the IPv6 GUA assigned to a running instance can be used again by another instance after it is removed from the first instance. "
1440,Can I assign IP addresses for multiple instances simultaneously?,No. You can specify the IP address of one instance at a time when launching the instance. 
1441,Can I assign any IP address to an instance?,You can assign any IP address to your instance as long as it is: Part of the associated subnet's IP address range Not reserved by Amazon for IP networking purposes Not currently assigned to another interface 
1442,Can I assign multiple IP addresses to an instance?,Yes. You can assign one or more secondary private IP addresses to an Elastic Network Interface or an EC2 instance in Amazon VPC. The number of secondary private IP addresses you can assign depends on the instance type. See EC2 User Guide for more information on the number of secondary private IP addresses that can be assigned per instance type. 
1443,Can I assign one or more Elastic IP (EIP) addresses to VPC-based Amazon EC2 instances?,"Yes, however, the EIP addresses will only be reachable from the Internet (not over the VPN connection). Each EIP address must be associated with a unique private IP address on the instance. EIP addresses should only be used on instances in subnets configured to route their traffic directly to the Internet gateway. EIPs cannot be used on instances in subnets configured to use a NAT gateway or a NAT instance to access the Internet. This is applicable only for IPv4. Amazon VPCs do not support EIPs for IPv6 at this time. "
1444,What is the Bring Your Own IP feature?,"Bring Your Own IP (BYOIP) enables customers to move all or part of their existing publicly routable IPv4 or IPv6 address space to AWS for use with their AWS resources. Customers will continue to own the IP range. Customers can create Elastic IPs from the IPv4 space they bring to AWS and use them with EC2 instances, NAT Gateways, and Network Load Balancers. Customers can also associate up to 5 CIDRs to a VPC from the IPv6 space they bring to AWS. Customers will continue to have access to Amazon-supplied IPs and can choose to use BYOIP Elastic IPs, Amazon-supplied IPs, or both. "
1445,Why should I use BYOIP?,"You may want to bring your own IP addresses to AWS for the following reasons:IP Reputation: Many customers consider the reputation of their IP addresses to be a strategic asset and want to use those IPs on AWS with their resources. For example, customers who maintain services such as outbound e-mail MTA and have high reputation IPs, can now bring over their IP space and successfully maintain their existing sending success rate. Customer whitelisting: BYOIP also enables customers to move workloads that rely on IP address whitelisting to AWS without the need to re-establish the whitelists with new IP addresses. Hardcoded dependencies: Several customers have IPs hardcoded in devices or have taken architectural dependencies on their IPs. BYOIP enables such customers hassle free migration to AWS. Regulation and compliance: Many customers are required to use certain IPs because of regulation and compliance reasons. They too are unlocked by BYOIP. On-prem IPv6 network policy: Many customers can route only their IPv6 in their on-prem network. These customers are unlocked by BYOIP as they can assign their own IPv6 range to their VPC and choose to route to their on-prem network using internet or Direct Connect. "
1446,What happens if I release a BYOIP Elastic IP?,When you release a BYOIP Elastic IP it goes back to the BYOIP IP pool from which it was allocated. 
1447,In which AWS Regions is BYOIP available?,See our documentation for details on BYOIP availability. 
1448,Can a BYOIP prefix be shared with multiple VPCs in the same account?,Yes. You can use the BYOIP prefix with any number of VPCs in the same account. 
1449,How many IP ranges can I bring via BYOIP?,You can bring a maximum of five IP ranges to your account. 
1450,What is the most specific prefix that I can bring via BYOIP?,"Via BYOIP, the most specific IPv4 prefix you can bring is a /24 IPv4 prefix and a /56 IPv6 prefix. If you intend to advertise your Ipv6 prefix to the internet then most specific IPv6 prefix is /48. "
1451,Which RIR prefixes can I use for BYOIP?,"You can use ARIN, RIPE, and APNIC registered prefixes. "
1452,Can I bring a reassigned or reallocated prefix?,We are not accepting reassigned or reallocated prefixes at this time. IP ranges should be a net type of direct allocation or direct assignment. 
1453,Can I move a BYOIP prefix from one AWS Region to another?,Yes. You can do that by de-provisioning the BYOIP prefix from the current region and then provisioning it to the new region. 
1454,What is VPC IP Address Manager (IPAM)?,"Amazon VPC IP Address Manager (IPAM) is a managed service that makes it easier for you to plan, track, and monitor IP addresses for your AWS workloads. Using IPAM, you can easily organize IP addresses based on your routing and security needs and set simple business rules to govern IP address assignments. You can also automate IP address assignment to VPCs, eliminating the need to use spreadsheet-based or homegrown IP address planning applications, which can be hard to maintain and time-consuming. IPAM provides a unified operational view, which can be used as your single source of truth, enabling you to quickly and efficiently perform routine IP address management activities such as tracking IP utilization, troubleshooting, and auditing. "
1455,Why should you use IPAM?,"You should use IPAM to make IP address management more efficient. Existing mechanisms that leverage spreadsheets or home-grown tools require manual work, and are error-prone. With IPAM, as an example, you can roll out applications faster as your developers no longer need to wait for the central IP address administration team to allocate IP addresses. You can also detect overlapping IP addresses and fix them before there is a network outage. In addition, you can create alarms for IPAM to notify you if the address pools are nearing exhaustion or if resources fail to comply with allocations rules set on a pool. These are some of the many reasons you should use IPAM. "
1456,What are the key features offered by IPAM?,AWS IPAM provides the following features:  Allocate IP addresses for at-scale networks: IPAM can automate IP address allocations across hundreds of accounts and VPCs based on configurable business rules.  Monitor IP usage across your network: IPAM can monitor IP addresses and enables you to get alerts when IPAM detects potential issues such as depleting IP addresses that can stall network's growth or overlapping IP addresses that can result in erroneous routing.  Troubleshoot your network: IPAM can help you quickly identify if connectivity issues are due to IP address misconfigurations or other issues.  Audit IP addresses: IPAM automatically retains your IP address monitoring data (up to a maximum of three years). You can use this historical data to do retrospective analysis and audits for your network. 
1457,What are the key components of IPAM?,"The following are the key components of IPAM: A scope is the highest-level container within IPAM. An IPAM contains two default scopes. Each scope represents the IP space for a single network. The private scope is intended for all private space. The public scope is intended for all public space. Scopes enable you to reuse IP addresses across multiple unconnected networks without causing IP address overlap or conflict. Within a scope, you create IPAM pools.  A pool is a collection of contiguous IP address ranges (or CIDRs). IPAM pools enable you to organize your IP addresses according to your routing and security needs. You can have multiple pools within a top-level pool. For example, if you have separate routing and security needs for development and production applications, you can create a pool for each. Within IPAM pools, you allocate CIDRs to AWS resources. An allocation is a CIDR assignment from an IPAM pool to another resource or IPAM pool. When you create a VPC and choose an IPAM pool for the VPC's CIDR, the CIDR is allocated from the CIDR provisioned to the IPAM pool. You can monitor and manage the allocation with IPAM. "
1458,Does IPAM support Bring Your Own IPs (BYOIPs)?,"Yes. IPAM supports both BYOIPv4 and BYOIPv6 addresses. BYOIP is an EC2 feature that allows you to bring IP addresses owned by you to AWS. With IPAM, you can directly provision and share their IP address blocks across accounts and organization. Existing BYOIP customers using IPv4 can migrate their pools into IPAM to simplify IP management. "
1459,Does Amazon Provide Contiguous CIDR Blocks and how do they work with IPAM?,"Yes, Amazon Provides Contiguous IPv6 CIDR blocks for VPC allocation. Contigous CIDR blocks allow you to aggregated CIDRs in a single entry across networking and security constructs like access control lists, route tables, security groups, and firewalls. You can provision Amazon IPv6 CIDRs into a publicly scoped pool, and use all of the IPAM features to manage and monitor IP usage. Allocation of these CIDR blocks start in /52 increments, and larger blocks are available upon request. For example, you can allocate /52 CIDR from Amazon and use IPAM to share across accounts and create VPCs in those accounts. "
1460,Can you use Amazon Provided Contiguous IPv6 CIDR blocks without IPAM?,"No, Amazon Provided Contiguous IPv6 CIDR blocks are currently only supported in IPAM. "
1461,Can I share my IPAM pools with other accounts?,"You can share IPAM pools with other accounts in your AWS Organization using AWS Resource Access Manager (RAM). You can also share IPAM pools with accounts outside of your primary AWS Organization. For example, these accounts can represent another line of business in your company or a managed service hosted by a partner on your behalf, in another AWS Organization. "
1462,Can I specify which subnet will use which gateway as its default?,"Yes. You may create a default route for each subnet. The default route can direct traffic to egress the VPC via the Internet gateway, the virtual private gateway, or the NAT gateway. "
1463,How do I secure Amazon EC2 instances running within my VPC?,"Amazon EC2 security groups can be used to help secure instances within an Amazon VPC. Security groups in a VPC enable you to specify both inbound and outbound network traffic that is allowed to or from each Amazon EC2 instance. Traffic which is not explicitly allowed to or from an instance is automatically denied. In addition to security groups, network traffic entering and exiting each subnet can be allowed or denied via network Access Control Lists (ACLs). "
1464,What are the differences between security groups in a VPC and network ACLs in a VPC?,"Security groups in a VPC specify which traffic is allowed to or from an Amazon EC2 instance. Network ACLs operate at the subnet level and evaluate traffic entering and exiting a subnet. Network ACLs can be used to set both Allow and Deny rules. Network ACLs do not filter traffic between instances in the same subnet. In addition, network ACLs perform stateless filtering while security groups perform stateful filtering. "
1465,What is the difference between stateful and stateless filtering?,"Stateful filtering tracks the origin of a request and can automatically allow the reply to the request to be returned to the originating computer. For example, a stateful filter that allows inbound traffic to TCP port 80 on a webserver will allow the return traffic, usually on a high numbered port (e.g., destination TCP port 63, 912) to pass through the stateful filter between the client and the webserver. The filtering device maintains a state table that tracks the origin and destination port numbers and IP addresses. Only one rule is required on the filtering device: Allow traffic inbound to the web server on TCP port 80. Stateless filtering, on the other hand, only examines the source or destination IP address and the destination port, ignoring whether the traffic is a new request or a reply to a request. In the above example, two rules would need to be implemented on the filtering device: one rule to allow traffic inbound to the web server on TCP port 80, and another rule to allow outbound traffic from the webserver (TCP port range 49, 152 through 65, 535). "
1466,Can Amazon EC2 instances within a VPC communicate with Amazon EC2 instances not within a VPC?,"Yes. If an Internet gateway has been configured, Amazon VPC traffic bound for Amazon EC2 instances not within a VPC traverses the Internet gateway and then enters the public AWS network to reach the EC2 instance. If an Internet gateway has not been configured, or if the instance is in a subnet configured to route through the virtual private gateway, the traffic traverses the VPN connection, egresses from your datacenter, and then re-enters the public AWS network. "
1467,Can Amazon EC2 instances within a VPC in one region communicate with Amazon EC2 instances within a VPC in another region?,"Yes. Instances in one region can communicate with each other using Inter-Region VPC Peering, public IP addresses, NAT gateway, NAT instances, VPN Connections or Direct Connect connections. "
1468,Can Amazon EC2 instances within a VPC communicate with Amazon S3?,"Yes. There are multiple options for your resources within a VPC to communicate with Amazon S3. You can use VPC Endpoint for S3, which makes sure all traffic remains within Amazon's network and enables you to apply additional access policies to your Amazon S3 traffic. You can use an Internet gateway to enable Internet access from your VPC and instances in the VPC can communicate with Amazon S3. You can also make all traffic to Amazon S3 traverse the Direct Connect or VPN connection, egress from your datacenter, and then re-enter the public AWS network. "
1469,Can I monitor the network traffic in my VPC?,Yes. You can use Amazon VPC traffic mirroring and Amazon VPC flow logs features to monitor the network traffic in your Amazon VPC. 
1470,What is Amazon VPC flow logs?,"VPC flow logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow logs data can be published to either Amazon CloudWatch Logs or Amazon S3. You can monitor your VPC flow logs to gain operational visibility about your network dependencies and traffic patterns, detect anomalies and prevent data leakage, or troubleshoot network connectivity and configuration issues. The enriched metadata in flow logs help you gain additional insights about who initiated your TCP connections, and the actual packet-level source and destination for traffic flowing through intermediate layers such as the NAT Gateway. You can also archive your flow logs to meet compliance requirements. To learn more about Amazon VPC flow logs, please refer to the documentation. "
1471,How can I use VPC flow logs?,"You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. While creating a flow log subscription, you can choose the metadata fields you wish to capture, the maximum aggregation interval, and your preferred log destination. You can also choose to capture all traffic or only accepted or rejected traffic. You can use tools like CloudWatch Log Insights or CloudWatch Contributor Insights to analyze your VPC flow logs delivered to CloudWatch Logs. You can use tools like Amazon Athena or AWS QuickSight to query and visualize your VPC flow logs delivered to Amazon S3. You can also build a custom downstream application to analyze your logs or use partner solutions such as Splunk, Datadog, Sumo Logic, Cisco StealthWatch, Checkpoint CloudGuard, New Relic etc. "
1472,Do VPC flow logs support AWS Transit Gateway?,"Yes, you can create VPC flow log for a Transit Gateway or for an individual Transit Gateway attachment. With this feature Transit Gateway can export detailed information such as source/destination IPs, ports, protocol, traffic counters, timestamps and various metadata for network flows traversing via the Transit Gateway. To learn more about Amazon VPC flow logs support for Transit Gateway, please refer to the documentation. "
1473,Does using Flow Logs impact my network latency or performance?,"Flow log data is collected outside of the path of your network traffic, and therefore does not affect network throughput or latency. You can create or delete flow logs without any risk of impact to network performance. "
1474,How much VPC flow logs cost?,"Data ingestion and archival charges for vended logs apply when you publish flow logs to CloudWatch Logs or to Amazon S3. For more information and examples, see Amazon CloudWatch Pricing. You can also track charges from publishing flow logs using cost allocation tags. "
1475,What is Amazon VPC traffic mirroring?,"Amazon VPC traffic mirroring makes it easy for customers to replicate network traffic to and from an Amazon EC2 instance and forward it to out-of-band security and monitoring appliances for use-cases such as content inspection, threat monitoring, and troubleshooting. These appliances can be deployed on an individual EC2 instance or a fleet of instances behind a Network Load Balancer (NLB) with User Datagram Protocol (UDP) listener. "
1476,Which resources can be monitored with Amazon VPC traffic mirroring ?,Traffic mirroring supports network packet captures at the Elastic Network Interface (ENI) level for EC2 instances. Refer to the Traffic Mirroring documentation for the EC2 instances that support Amazon VPC Traffic Mirroring. 
1477,What type of appliances are supported with Amazon VPC traffic mirroring?,"Customers can either use open source tools or choose from a wide-range of monitoring solution available on AWS Marketplace. Traffic mirroring allows customers to stream replicated traffic to any network packet collector/broker or analytics tool, without requiring them to install vendor-specific agents. "
1478,How is Amazon VPC traffic mirroring different from Amazon VPC flow logs?,"Amazon VPC flow logs allow customers to collect, store, and analyze network flow logs. The information captured in flow logs includes information about allowed and denied traffic, source and destination IP addresses, ports, protocol number, packet and byte counts, and an action (accept or reject). You can use this feature to troubleshoot connectivity and security issues and to make sure that the network access rules are working as expected. Amazon VPC traffic mirroring, provides deeper insight into network traffic by allowing you to analyze actual traffic content, including payload, and is targeted for use-cases when you need to analyze the actual packets to determine the root cause a performance issue, reverse-engineer a sophisticated network attack, or detect and stop insider abuse or compromised workloads. "
1479,Within which Amazon EC2 region(s) is Amazon VPC available?,Amazon VPC is currently available in multiple Availability Zones in all Amazon EC2 regions. 
1480,Can a VPC span multiple Availability Zones?,Yes. 
1481,Can a subnet span Availability Zones?,No. A subnet must reside within a single Availability Zone. 
1482,How do I specify which Availability Zone my Amazon EC2 instances are launched in?,"When you launch an Amazon EC2 instance, you must specify the subnet in which to launch the instance. The instance will be launched in the Availability Zone associated with the specified subnet. "
1483,How do I determine which Availability Zone my subnets are located in?,"When you create a subnet you must specify the Availability Zone in which to place the subnet. When using the VPC Wizard, you can select the subnet's Availability Zone in the wizard confirmation screen. When using the API or the CLI you can specify the Availability Zone for the subnet as you create the subnet. If you don't specify an Availability Zone, the default No Preference option will be selected and the subnet will be created in an available Availability Zone in the region. "
1484,Am I charged for network bandwidth between instances in different subnets?,"If the instances reside in subnets in different Availability Zones, you will be charged $0.01 per GB for data transfer. "
1485,"When I call DescribeInstances(), do I see all of my Amazon EC2 instances, including those in EC2-Classic and EC2-VPC?","Yes. DescribeInstances() will return all running Amazon EC2 instances. You can differentiate EC2-Classic instances from EC2-VPC instances by an entry in the subnet field. If there is a subnet ID listed, the instance is within a VPC. "
1486,"When I call DescribeVolumes(), do I see all of my Amazon EBS volumes, including those in EC2-Classic and EC2-VPC?",Yes. DescribeVolumes() will return all your EBS volumes. 
1487,How many Amazon EC2 instances can I use within a VPC?,"For instances that require IPv4 addressing, you can run any number of Amazon EC2 instances within a VPC, so long as your VPC is appropriately sized to have an IPv4 address assigned to each instance. You are initially limited to launching 20 Amazon EC2 instances at any one time and a maximum VPC size of /16 (65,536 IPs). If you would like to increase these limits, please complete the following form. For IPv6 only instances, the VPC size of /56 provides you the ability to launch virtually unlimited number of Amazon EC2 instances. "
1488,Can I use my existing AMIs in Amazon VPC?,"You can use AMIs in Amazon VPC that are registered within the same region as your VPC. For example, you can use AMIs registered in us-east-1 with a VPC in us-east-1. More information is available in the Amazon EC2 Region and Availability Zone FAQ. "
1489,Can I use my existing Amazon EBS snapshots?,"Yes, you may use Amazon EBS snapshots if they are located in the same region as your VPC. More details are available in the Amazon EC2 Region and Availability Zone FAQ. "
1490,Can I boot an Amazon EC2 instance from an Amazon EBS volume within Amazon VPC?,"Yes, however, an instance launched in a VPC using an Amazon EBS-backed AMI maintains the same IP address when stopped and restarted. This is in contrast to similar instances launched outside a VPC, which get a new IP address. The IP addresses for any stopped instances in a subnet are considered unavailable. "
1491,Can I employ Amazon CloudWatch within Amazon VPC?,Yes. 
1492,Can I employ Auto Scaling within Amazon VPC?,Yes. 
1493,Can I launch Amazon EC2 Cluster Instances in a VPC?,"Yes. Cluster instances are supported in Amazon VPC, however, not all instance types are available in all regions and Availability Zones. "
1494,What are instance hostnames?,"When you launch an instance, it is assigned a hostname. There are two options available, an IP based name or a Resource based name, and this parameter is configurable at instance launch. The IP based name uses a form of the Private IPv4 address while the Resource based name uses a form of the instance-id. "
1495,Can I change the instance hostname of my Amazon EC2 instance?,"Yes, you can change the hostname of an instance form IP based to Resource based or vice versa by stopping the instance and then changing the resource based naming options. "
1496,Can I use the instance hostnames as DNS hostnames?,"Yes, the instance hostname can be used as DNS hostnames. For instances launched in an IPv4-only or dual-stack subnet, the IP based name always resolves to the Private IPv4 address on the primary network interface of the instance and this cannot be turned off. Additionally, the Resource based name can be configured to resolve to either the Private IPv4 address on the primary network interface, or the first IPv6 GUA on the primary network interface, or both. For instances launched in an IPv6-only subnet, the Resource based name will be configured to resolve to the first IPv6 GUA on the primary network interface. "
1497,What is a default VPC?,"A default VPC is a logically isolated virtual network in the AWS cloud that is automatically created for your AWS account the first time you provision Amazon EC2 resources. When you launch an instance without specifying a subnet-ID, your instance will be launched in your default VPC. "
1498,What are the benefits of a default VPC?,"When you launch resources in a default VPC, you can benefit from the advanced networking functionalities of Amazon VPC (EC2-VPC) with the ease of use of Amazon EC2 (EC2-Classic). You can enjoy features such as changing security group membership on the fly, security group egress filtering, multiple IP addresses, and multiple network interfaces without having to explicitly create a VPC and launch instances in the VPC. "
1499,What accounts are enabled for default VPC?,"If your AWS account was created after March 18, 2013 your account may be able to launch resources in a default VPC. See this Forum Announcement to determine which regions have been enabled for the default VPC feature set. Also, accounts created prior to the listed dates may utilize default VPCs in any default VPC enabled region in which you've not previously launched EC2 instances or provisioned Amazon Elastic Load Balancing, Amazon RDS, Amazon ElastiCache, or Amazon Redshift resources. "
1500,Will I need to know anything about Amazon VPC in order to use a default VPC?,"No. You can use the AWS Management Console, AWS EC2 CLI, or the Amazon EC2 API to launch and manage EC2 instances and other AWS resources in a default VPC. AWS will automatically create a default VPC for you and will create a default subnet in each Availability Zone in the AWS region. Your default VPC will be connected to an Internet gateway and your instances will automatically receive public IP addresses, just like EC2-Classic. "
1501,What are the differences between instances launched in EC2-Classic and EC2-VPC?,See Differences between EC2-Classic and EC2-VPC in the EC2 User Guide. 
1502,Do I need to have a VPN connection to use a default VPC?,No. Default VPCs are attached to the Internet and all instances launched in default subnets in the default VPC automatically receive public IP addresses. You can add a VPN connection to your default VPC if you choose. 
1503,Can I create other VPCs and use them in addition to my default VPC?,Yes. To launch an instance into nondefault VPCs you must specify a subnet-ID during instance launch. 
1504,"Can I create additional subnets in my default VPC, such as private subnets?","Yes. To launch into nondefault subnets, you can target your launches using the console or the --subnet option from the CLI, API, or SDK. "
1505,How many default VPCs can I have?,You can have one default VPC in each AWS region where your Supported Platforms attribute is set to EC2-VPC. 
1506,How many default subnets are in a default VPC?,One default subnet is created for each Availability Zone in your default VPC. 
1507,Can I specify which VPC is my default VPC?,Not at this time. 
1508,Can I specify which subnets are my default subnets?,Not at this time. 
1509,Can I delete a default VPC?,"Yes, you can delete a default VPC. Once deleted, you can create a new default VPC directly from the VPC Console or by using the CLI. This will create a new default VPC in the region. This does not restore the previous VPC that was deleted. "
1510,Can I delete a default subnet?,"Yes, you can delete a default subnet. Once deleted, you can create a new default subnet in the availability zone by using the CLI or SDK. This will create a new default subnet in the availability zone specified. This does not restore the previous subnet that was deleted. "
1511,I have an existing EC2-Classic account. Can I get a default VPC?,"The simplest way to get a default VPC is to create a new account in a region that is enabled for default VPCs, or use an existing account in a region you've never been to before, as long as the Supported Platforms attribute for that account in that region is set to EC2-VPC. "
1512,I really want a default VPC for my existing EC2 account. Is that possible?,"Yes, however, we can only enable an existing account for a default VPC if you have no EC2-Classic resources for that account in that region. Additionally, you must terminate all non-VPC provisioned Elastic Load Balancers, Amazon RDS, Amazon ElastiCache, and Amazon Redshift resources in that region. After your account has been configured for a default VPC, all future resource launches, including instances launched via Auto Scaling, will be placed in your default VPC. To request your existing account be setup with a default VPC, please go to Account and Billing -> Service: Account -> Category: Convert EC2 Classic to VPC and raise a request. We will review your request, your existing AWS services and EC2-Classic presence and guide you through the next steps. "
1513,How are IAM accounts impacted by default VPC?,"If your AWS account has a default VPC, any IAM accounts associated with your AWS account use the same default VPC as your AWS account. "
1514,What is EC2-Classic?,"EC2-Classic is a flat network that we launched with EC2 in the summer of 2006. With EC2-Classic, your instances run in a single, flat network that you share with other customers. Over time, inspired by our customers' evolving needs, we launched Amazon Virtual Private Cloud (VPC) in 2009 to allow you to run instances in a virtual private cloud that's logically isolated to your AWS account. Today, while majority of our customers use Amazon VPC, we have a few customers who still use EC2-Classic. "
1515,What's changing?,"We are retiring Amazon EC2-Classic on August 15, 2022 and we need you to migrate any EC2 instances and other AWS resources running on EC2-Classic to Amazon VPC before this date. The following section provides more information on the EC2-Class retirement as well as tools and resources to assist you in migration. "
1516,How is my account impacted by the retirement of EC2-Classic?,"You are affected by this change only if you have EC2-Classic enabled on your account in any of the AWS regions. You can use the console or the describe-account-attributes command to check whether you have EC2-Classic enabled for an AWS region; please refer to this document for more details.If you do not have any active AWS resources running on EC2-Classic in any region, we request you to turn off EC2-Classic from your account for that region. Turning off EC2-Classic in a region allows you to launch Default VPC there. To do so go to the AWS Support Center at console.aws.amazon.com/support, choose Create case and then Account and billing support, for Type choose Account, for Category choose Convert EC2 Classic to VPC, fill in the other details as required, and choose Submit.We will automatically turn off EC2-Classic from your account on October 30, 2021 for any AWS region where you have not had any AWS resources (EC2 Instances, Amazon Relational Database, AWS Elastic Beanstalk, Amazon Redshift, AWS Data Pipeline, Amazon EMR, AWS OpsWorks) on EC2-Classic since January 1, 2021.On the other hand, if you have AWS resources running on EC2-Classic, we request you to plan their migration to Amazon VPC as soon as possible. You will not be able to launch any instances or AWS services on EC2-Classic platform beyond August 15, 2022. Any workloads or services in running state will gradually loose access to all AWS services on EC2-Classic as we retire them beginning August 16, 2022. You can find the migration guides for your AWS resources in subsequent question. "
1517,What are the benefits of moving from EC2-Classic to Amazon VPC?,"Amazon VPC gives you complete control over your virtual network environment on AWS, logically isolated to your AWS account. In the EC2-Classic environment, your workloads are sharing a single flat network with other customers. The Amazon VPC environment offers many other advantages over the EC2-Classic environment including the ability to select your own IP address space, public and private subnet configuration, and management of route tables and network gateways. All services and instances currently available in EC2-Classic have comparable services available in the Amazon VPC environment. Amazon VPC also offers a much wider and latest generation of instances than EC2-Classic. Further information about Amazon VPC is available in this link. "
1518,How do I migrate from EC2-Classic to VPC?,"To help you migrate your resources, we have published playbooks and built solutions that you will find below. To migrate, you must recreate your EC2-Classic resources in your VPC. First, you can use this script to identify all resources provisioned in EC2-Classic across all regions in an account. You can then use the migration guide for the relevant AWS resources from below: Instances and Security Groups Classic Load Balancer Amazon Relational Database Service AWS Elastic Beanstalk Amazon Redshift for migration of DC1 Clusters and for other node types AWS Data Pipeline  Amazon EMR  AWS OpsWorks Besides the above migration guides, we are also offering a highly automated lift-and-shift (rehost) solution, AWS Application Migration Service (AWS MGN), that simplifies, expedites, and reduces the cost of migrating applications. You can find relevant resources about AWS MGN here: Getting started with AWS Application Migration Service  AWS Application Migration Service on-demand technical training Documentation to dive deep into AWS Application Migration Service Features and Functionalities Service Architecture and Network Architecture video For simple individual EC2 instance migrations from EC2-Classic to VPC, besides AWS MGN or the Instances Migration Guide, you can also use the AWSSupport-MigrateEC2 ClassicToVPC runbook from AWS Systems Manager > Automation. This runbooks automates the steps that are required to migrate an instance from EC2-Classic to VPC by creating an AMI of the instance in EC2-Classic, creating a new instance from the AMI in VPC, and optionally terminating the EC2-Classic instance. If you have any questions or concerns, you can contact the AWS Support Team via AWS Premium Support.Please Note: If you have AWS resources running on EC2-Classic in multiple AWS regions, we recommend that you turn off EC2-Classic for each of those regions as soon as you have migrated all your resources to VPC in them. "
1519,What are the important dates I should be aware of?,"We will take the following two actions ahead of the August 15, 2022 retirement date: We will stop issuing 3-year reserved instances (RI) and 1-year RI for the EC2-Classic environment on Oct 30, 2021. RIs already in place on the EC2-Classic environment will not be affected at this time. RIs that are set to expire after 8/15/2022 will need to be modified to use the Amazon VPC environment for the remaining period of the lease. For information on how to modify your RIs, please visit our document. On Aug 15, 2022, we will no longer allow the creation of new instances (Spot or on-demand) or other AWS services in the EC2-Classic environment. Any workloads or services in running state will gradually loose access to all AWS services on EC2-Classic as we retire them beginning August 16, 2022. "
1520,Can I attach or detach one or more network interfaces to an EC2 instance while it's running?,Yes. 
1521,Can I attach a network interface in one Availability Zone to an instance in another Availability Zone?,Network interfaces can only be attached to instances residing in the same Availability Zone. 
1522,Can I attach a network interface in one VPC to an instance in another VPC?,Network interfaces can only be attached to instances in VPCs in the same account. 
1523,Can I use Elastic Network Interfaces as a way to host multiple websites requiring separate IP addresses on a single instance?,"Yes, however, this is not a use case best suited for multiple interfaces. Instead, assign additional private IP addresses to the instance and then associate EIPs to the private IPs as needed. "
1524,Can I detach the primary interface (eth0) on my EC2 instance?,"No. You can attach and detach secondary interfaces (eth1-ethn) on an EC2 instance, but you can't detach the eth0 interface. "
1525,Can I create a peering connection to a VPC in a different region?,Yes. Peering connections can be created with VPCs in different regions. Inter-region VPC peering is available globally in all commercial regions (excluding China). 
1526,Can I peer my VPC with a VPC belonging to another AWS account?,"Yes, assuming the owner of the other VPC accepts your peering connection request. "
1527,How much do VPC peering connections cost?,"There is no charge for creating VPC peering connections, however, data transfer across peering connections is charged. See the Data Transfer section of the EC2 Pricing page for data transfer rates. "
1528,Do I need an Internet Gateway to use peering connections?,No. VPC peering connections do not require an Internet Gateway. 
1529,Is VPC peering traffic within the region encrypted?,No. Traffic between instances in peered VPCs remains private and isolated similar to how traffic between two instances in the same VPC is private and isolated. 
1530,"If I delete my side of a peering connection, will the other side still have access to my VPC?",No. Either side of the peering connection can terminate the peering connection at any time. Terminating a peering connection means traffic won't flow between the two VPCs. 
1531,"If I peer VPC A to VPC B and I peer VPC B to VPC C, does that mean VPCs A and C are peered?",No. Transitive peering relationships are not supported. 
1532,What if my peering connection goes down?,"AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck. Inter-Region VPC Peering operates on the same horizontally scaled, redundant, and highly available technology that powers VPC today. Inter-Region VPC Peering traffic goes over the AWS backbone that has in-built redundancy and dynamic bandwidth allocation. There is no single point of failure for communication. If an Inter-Region peering connection does go down, the traffic will not be routed over the internet. "
1533,Are there any bandwidth limitations for peering connections?,"Bandwidth between instances in peered VPCs is no different than bandwidth between instances in the same VPC. Note: A placement group can span peered VPCs; however, you will not get full-bisection bandwidth between instances in peered VPCs. Read more about Placement Groups. "
1534,Is Inter-Region VPC Peering traffic encrypted?,Traffic is encrypted using modern AEAD (Authenticated Encryption with Associated Data) algorithms. Key agreement and key management is handled by AWS. 
1535,How do DNS translations work with Inter-Region VPC Peering?,"By default, a query for a public hostname of an instance in a peered VPC in a different region will resolve to a public IP address. Route 53 private DNS can be used to resolve to a private IP address with Inter-Region VPC Peering. "
1536,Can I reference security groups across an Inter-Region VPC Peering connection?,No. Security groups cannot be referenced across an Inter-Region VPC Peering connection. 
1537,Does Inter-Region VPC Peering support IPv6?,Yes. Inter-Region VPC Peering supports IPv6. 
1538,Can Inter-Region VPC Peering be used with EC2-Classic Link?,No. Inter-Region VPC Peering cannot be used with EC2-ClassicLink. 
1539,What is ClassicLink?,"Amazon Virtual Private Cloud (VPC) ClassicLink allows EC2 instances in the EC2-Classic platform to communicate with instances in a VPC using private IP addresses. To use ClassicLink, enable it for a VPC in your account, and associate a Security Group from that VPC with an instance in EC2-Classic. All the rules of your VPC Security Group will apply to communications between instances in EC2-Classic and instances in the VPC. "
1540,What does ClassicLink cost?,"There is no additional charge for using ClassicLink; however, existing cross Availability Zone data transfer charges will apply. For more information, consult the EC2 pricing page. "
1541,How do I use ClassicLink?,"In order to use ClassicLink, you first need to enable at least one VPC in your account for ClassicLink. Then you associate a Security Group from the VPC with the desired EC2-Classic instance. The EC2-Classic instance is now linked to the VPC and is a member of the selected Security Group in the VPC. Your EC2-Classic instance cannot be linked to more than one VPC at the same time. "
1542,Does the EC2-Classic instance become a member of the VPC?,The EC2-Classic instance does not become a member of the VPC. It becomes a member of the VPC Security Group that was associated with the instance. All the rules and references to the VPC Security Group apply to communication between instances in EC2-Classic instance and resources within the VPC. 
1543,"Can I use EC2 public DNS hostnames from my EC2-Classic and EC2-VPC instances to address each other, in order to communicate using private IP?","No. The EC2 public DNS hostname will not resolve to the private IP address of the EC2-VPC instance when queried from an EC2-Classic instance, and vice-versa. "
1544,Are there any VPCs for which I cannot enable ClassicLink?,"Yes. ClassicLink cannot be enabled for a VPC that has a Classless Inter-Domain Routing (CIDR) that is within the 10.0.0.0/8 range, with the exception of 10.0.0.0/16 and 10.1.0.0/16. In addition, ClassicLink cannot be enabled for any VPC that has a route table entry pointing to the 10.0.0.0/8 CIDR space to a target other than local. "
1545,"Can traffic from an EC2-Classic instance travel through the Amazon VPC and egress through the Internet gateway, virtual private gateway, or to peered VPCs?","Traffic from an EC2-Classic instance can only be routed to private IP addresses within the VPC. They will not be routed to any destinations outside the VPC, including Internet gateway, virtual private gateway, or peered VPC destinations. "
1546,"Does ClassicLink affect the access control between the EC2-Classic instance, and other instances that are in the EC2-Classic platform?",ClassicLink does not change the access control defined for an EC2-Classic instance through its existing Security Groups from the EC2-Classic platform. 
1547,Will ClassicLink settings on my EC2-Classic instance persist through stop/start cycles?,"The ClassicLink connection will not persist through stop/start cycles of the EC2-Classic instance. The EC2-Classic instance will need to be linked back to a VPC after it is stopped and started. However, the ClassicLink connection will persist through instance reboot cycles. "
1548,"Does ClassicLink allow EC2-Classic Security Group rules to reference VPC Security Groups, or vice versa?","ClassicLink does not allow EC2-Classic Security Group rules to reference VPC Security Groups, or vice versa. "
1549,What is AWS PrivateLink?,"AWS PrivateLink enables customers to access services hosted on AWS in a highly available and scalable manner, while keeping all the network traffic within the AWS network. Service users can use this to privately access services powered by PrivateLink from their Amazon Virtual Private Cloud (VPC) or their on-premises, without using public IPs, and without requiring the traffic to traverse across the Internet. Service owners can register their Network Load Balancers to PrivateLink services and provide the services to other AWS customers. "
1550,How can I use AWS PrivateLink?,"As a service user, you will need to create interface type VPC endpoints for services that are powered by PrivateLink. These service endpoints will appear as Elastic Network Interfaces (ENIs) with private IPs in your VPCs. Once these endpoints are created, any traffic destined to these IPs will get privately routed to the corresponding AWS services. As a service owner, you can onboard your service to AWS PrivateLink by establishing a Network Load Balancer (NLB) to front your service and create a PrivateLink service to register with the NLB. Your customers will be able to establish endpoints within their VPC to connect to your service after you whitelisted their accounts and IAM roles. "
1551,Which services are currently available on AWS PrivateLink?,"The following AWS services support this feature: Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Kinesis Streams, Service Catalog, EC2 Systems Manager, Amazon SNS, and AWS DataSync. Many SaaS solutions support this feature as well. Please visit AWS Marketplace for more SaaS products powered by AWS PrivateLink. "
1552,Can I privately access services powered by AWS PrivateLink over AWS Direct Connect?,Yes. The application in your on-premises can connect to the service endpoints in Amazon VPC over AWS Direct Connect. The service endpoints will automatically direct the traffic to AWS services powered by AWS PrivateLink. 
1553,Can I use the AWS Management Console to control and manage Amazon VPC?,"Yes. You can use the AWS Management Console to manage Amazon VPC objects such as VPCs, subnets, route tables, Internet gateways, and IPSec VPN connections. Additionally, you can use a simple wizard to create a VPC. "
1554,"How many VPCs, subnets, Elastic IP addresses, and internet gateways can I create?",See the Amazon VPC user guide for information on VPC limits. 
1555,Can I obtain AWS support with Amazon VPC?,Yes. Click here for more information on AWS support. 
1556,Can I use ElasticFox with Amazon VPC?,"ElasticFox is no longer officially supported for managing your Amazon VPC. Amazon VPC support is available via the AWS APIs, command line tools, and the AWS Management Console, as well as a variety of third-party utilities. "
